{"id": "2506.02013", "pdf": "https://arxiv.org/pdf/2506.02013", "abs": "https://arxiv.org/abs/2506.02013", "authors": ["Sun Ye", "Zuo Cuiming", "Zhang Rui", "Shi Bin", "Pang Yajing", "Gao Lingyun", "Zhao Bowei", "Wang Jing", "Yao Dezhong", "Liu Gang"], "title": "Non-invasive two-step strategy BCI: brain-muscle-hand interface", "categories": ["q-bio.NC"], "comment": null, "summary": "Brain-computer interface enables direct interaction between brain and device.\nHowever, common brain-computer interfaces often employ one-step strategy that\nrely on non-natural paradigms, such as SSVEP-BCI and MI-BCI, are limited to\nspecific scenarios, restricting their broader application. This paper first\nproposes a two-step strategic brain-muscular-hand interface (BMHI) based on\nbiological evolutionary selection mechanism, by integrating the brain-muscle\n(BM) interface with the muscle-hand (MH) interface through crosstalk (\"BMHI =\nBM + MH\"). To verify the effectiveness of BMHI and the advantages of a two-step\nstrategy inspired by natural evolution, we conducted offline, comparison\n(comparing BMHI (two-step) and brain-hand interface (one-step)), and online\nexperiments (using BMHI to control a virtual/machine hand for daily tasks). The\nresults show that: (1) BMHI is feasible and the prediction accuracy is 0.79;\n(2) Unlike traditional multi-layer neural networks that attempt to establish a\ndirect brain-signal-to-action mapping through a single end-to-end process\n(brain-hand interface), BMHI incorporates the neuro-muscular transmission\nmechanisms evolved in biological systems as an intermediate constraint layer.\nThis phased decoding strategy can reduce training time by approximately 18-fold\nand improve decoding accuracy; (3) In the online control experiment, both the\nvirtual hand and the manipulator were able to successfully complete tasks, like\nmoving objects such as boxes or plates and holding water glasses. The results\nshow that BMHI adopts a two-step decoding strategy that mimics natural human\nneural motor pathways, improves training efficiency and prediction accuracy,\nand promotes the development of BCI technology to a more natural interaction\nmode."}
{"id": "2506.02044", "pdf": "https://arxiv.org/pdf/2506.02044", "abs": "https://arxiv.org/abs/2506.02044", "authors": ["Xinxu Wei", "Kanhao Zhao", "Yong Jiao", "Lifang He", "Yu Zhang"], "title": "A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder", "categories": ["q-bio.NC", "cs.LG"], "comment": "34pages", "summary": "As large language models (LLMs) continue to revolutionize AI research, there\nis a growing interest in building large-scale brain foundation models to\nadvance neuroscience. While most existing brain foundation models are\npre-trained on time-series signals or region-of-interest (ROI) features, we\npropose a novel graph-based pre-training paradigm for constructing a brain\ngraph foundation model. In this paper, we introduce the Brain Graph Foundation\nModel, termed BrainGFM, a unified framework that leverages graph contrastive\nlearning and graph masked autoencoders for large-scale fMRI-based pre-training.\nBrainGFM is pre-trained on a diverse mixture of brain atlases with varying\nparcellations, significantly expanding the pre-training corpus and enhancing\nthe model's ability to generalize across heterogeneous fMRI-derived brain\nrepresentations. To support efficient and versatile downstream transfer, we\nintegrate both graph prompts and language prompts into the model design,\nenabling BrainGFM to flexibly adapt to a wide range of atlases, neurological\nand psychiatric disorders, and task settings. Furthermore, we employ\nmeta-learning to optimize the graph prompts, facilitating strong generalization\nto previously unseen disorders under both few-shot and zero-shot learning\nconditions via language-guided prompting. BrainGFM is pre-trained on 27\nneuroimaging datasets spanning 25 common neurological and psychiatric\ndisorders, encompassing 2 types of brain atlases (functional and anatomical)\nacross 8 widely-used parcellations, and covering over 25,000 subjects, 60,000\nfMRI scans, and a total of 400,000 graph samples aggregated across all atlases\nand parcellations. The code is available at:\nhttps://github.com/weixinxu666/BrainGFM"}
{"id": "2506.02263", "pdf": "https://arxiv.org/pdf/2506.02263", "abs": "https://arxiv.org/abs/2506.02263", "authors": ["Qi Xin", "Robert E. Kass"], "title": "Identifying interactions across brain areas while accounting for individual-neuron dynamics with a Transformer-based variational autoencoder", "categories": ["q-bio.NC"], "comment": null, "summary": "Advances in large-scale recording technologies now enable simultaneous\nmeasurements from multiple brain areas, offering new opportunities to study\nsignal transmission across interacting components of neural circuits. However,\nneural responses exhibit substantial trial-to-trial variability, often driven\nby unobserved factors such as subtle changes in animal behavior or internal\nstates. To prevent evolving background dynamics from contaminating\nidentification of functional coupling, we developed a hybrid neural spike train\nmodel, GLM-Transformer, that incorporates flexible, deep latent variable models\ninto a point process generalized linear model (GLM) having an interpretable\ncomponent for cross-population interactions. A Transformer-based variational\nautoencoder captures nonstationary individual-neuron dynamics that vary across\ntrials, while standard nonparametric regression GLM coupling terms provide\nestimates of directed interactions between neural populations. We incorporate a\nlow-rank structure on population-to-population coupling effects to improve\nscalability. Across synthetic datasets and mechanistic simulations,\nGLM-Transformer recovers known coupling structure and remains robust to shared\nbackground fluctuations. When applied to the Allen Institute Visual Coding\ndataset, it identifies feedforward pathways consistent with established visual\nhierarchies. This work offers a step toward improved identification of neural\npopulation interactions, and contributes to ongoing efforts aimed at achieving\ninterpretable results while harvesting the benefits of deep learning."}
{"id": "2506.02813", "pdf": "https://arxiv.org/pdf/2506.02813", "abs": "https://arxiv.org/abs/2506.02813", "authors": ["Jack Cook", "Danyal Akarca", "Rui Ponte Costa", "Jascha Achterberg"], "title": "Brain-Like Processing Pathways Form in Models With Heterogeneous Experts", "categories": ["q-bio.NC", "cs.NE"], "comment": "25 pages, 14 figures", "summary": "The brain is made up of a vast set of heterogeneous regions that dynamically\norganize into pathways as a function of task demands. Examples of such pathways\ncan be seen in the interactions between cortical and subcortical networks\nduring learning. This raises the question of how exactly brain regions organize\ninto these dynamic groups. In this work, we use an extension of the\nHeterogeneous Mixture-of-Experts architecture, to show that heterogeneous\nregions do not form processing pathways by themselves, implying that the brain\nlikely implements specific constraints which result in reliable formation of\npathways. We identify three biologically relevant inductive biases that\nencourage pathway formation: a routing cost imposed on the use of more complex\nregions, a scaling factor that reduces this cost when task performance is low,\nand randomized expert dropout. When comparing our resulting Mixture-of-Pathways\nmodel with the brain, we observe that the artificial pathways match how the\nbrain uses cortical and subcortical systems to learn and solve tasks of varying\ndifficulty. In summary, we introduce a novel framework for investigating how\nthe brain forms task-specific pathways through inductive biases which may make\nMixture-of-Experts architectures in general more adaptive."}
{"id": "2506.03088", "pdf": "https://arxiv.org/pdf/2506.03088", "abs": "https://arxiv.org/abs/2506.03088", "authors": ["Lloyd Pellatt", "Fotios Drakopoulos", "Shievanie Sabesan", "Nicholas A. Lesica"], "title": "Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": "12 pages, 3 figures", "summary": "The mapping from sound to neural activity that underlies hearing is highly\nnon-linear. The first few stages of this mapping in the cochlea have been\nmodelled successfully, with biophysical models built by hand and, more\nrecently, with DNN models trained on datasets simulated by biophysical models.\nModelling the auditory brain has been a challenge because central auditory\nprocessing is too complex for models to be built by hand, and datasets for\ntraining DNN models directly have not been available. Recent work has taken\nadvantage of large-scale high resolution neural recordings from the auditory\nmidbrain to build a DNN model of normal hearing with great success. But this\nmodel assumes that auditory processing is the same in all brains, and therefore\nit cannot capture the widely varying effects of hearing loss.\n  We propose a novel variational-conditional model to learn to encode the space\nof hearing loss directly from recordings of neural activity in the auditory\nmidbrain of healthy and noise exposed animals. With hearing loss parametrised\nby only 6 free parameters per animal, our model accurately predicts 62\\% of the\nexplainable variance in neural responses from normal hearing animals and 68%\nfor hearing impaired animals, within a few percentage points of state of the\nart animal specific models. We demonstrate that the model can be used to\nsimulate realistic activity from out of sample animals by fitting only the\nlearned conditioning parameters with Bayesian optimisation, achieving\ncrossentropy loss within 2% of the optimum in 15-30 iterations. Including more\nanimals in the training data slightly improved the performance on unseen\nanimals. This model will enable future development of parametrised hearing loss\ncompensation models trained to directly restore normal neural coding in hearing\nimpaired brains, which can be quickly fitted for a new user by human in the\nloop optimisation."}
{"id": "2505.21777", "pdf": "https://arxiv.org/pdf/2505.21777", "abs": "https://arxiv.org/abs/2505.21777", "authors": ["Bao Pham", "Gabriel Raya", "Matteo Negri", "Mohammed J. Zaki", "Luca Ambrogioni", "Dmitry Krotov"], "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.CV", "q-bio.NC", "stat.ML"], "comment": null, "summary": "Hopfield networks are associative memory (AM) systems, designed for storing\nand retrieving patterns as local minima of an energy landscape. In the\nclassical Hopfield model, an interesting phenomenon occurs when the amount of\ntraining data reaches its critical memory load $- spurious\\,\\,states$, or\nunintended stable points, emerge at the end of the retrieval dynamics, leading\nto incorrect recall. In this work, we examine diffusion models, commonly used\nin generative modeling, from the perspective of AMs. The training phase of\ndiffusion model is conceptualized as memory encoding (training data is stored\nin the memory). The generation phase is viewed as an attempt of memory\nretrieval. In the small data regime the diffusion model exhibits a strong\nmemorization phase, where the network creates distinct basins of attraction\naround each sample in the training set, akin to the Hopfield model below the\ncritical memory load. In the large data regime, a different phase appears where\nan increase in the size of the training set fosters the creation of new\nattractor states that correspond to manifolds of the generated samples.\nSpurious states appear at the boundary of this transition and correspond to\nemergent attractor states, which are absent in the training set, but, at the\nsame time, have distinct basins of attraction around them. Our findings\nprovide: a novel perspective on the memorization-generalization phenomenon in\ndiffusion models via the lens of AMs, theoretical prediction of existence of\nspurious states, empirical validation of this prediction in commonly-used\ndiffusion models."}
{"id": "2506.02164", "pdf": "https://arxiv.org/pdf/2506.02164", "abs": "https://arxiv.org/abs/2506.02164", "authors": ["Yu", "Qian", "Wilson S. Geisler", "Xue-Xin Wei"], "title": "Quantifying task-relevant representational similarity using decision variable correlation", "categories": ["cs.CV", "cs.LG", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Previous studies have compared the brain and deep neural networks trained on\nimage classification. Intriguingly, while some suggest that their\nrepresentations are highly similar, others argued the opposite. Here, we\npropose a new approach to characterize the similarity of the decision\nstrategies of two observers (models or brains) using decision variable\ncorrelation (DVC). DVC quantifies the correlation between decoded decisions on\nindividual samples in a classification task and thus can capture task-relevant\ninformation rather than general representational alignment. We evaluate this\nmethod using monkey V4/IT recordings and models trained on image classification\ntasks.\n  We find that model--model similarity is comparable to monkey--monkey\nsimilarity, whereas model--monkey similarity is consistently lower and,\nsurprisingly, decreases with increasing ImageNet-1k performance. While\nadversarial training enhances robustness, it does not improve model--monkey\nsimilarity in task-relevant dimensions; however, it markedly increases\nmodel--model similarity. Similarly, pre-training on larger datasets does not\nimprove model--monkey similarity. These results suggest a fundamental\ndivergence between the task-relevant representations in monkey V4/IT and those\nlearned by models trained on image classification tasks."}
{"id": "2506.03089", "pdf": "https://arxiv.org/pdf/2506.03089", "abs": "https://arxiv.org/abs/2506.03089", "authors": ["Lucas Piper", "Arlindo L. Oliveira", "Tiago Marques"], "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches."}
{"id": "2506.02013", "pdf": "https://arxiv.org/pdf/2506.02013", "abs": "https://arxiv.org/abs/2506.02013", "authors": ["Sun Ye", "Zuo Cuiming", "Zhang Rui", "Shi Bin", "Pang Yajing", "Gao Lingyun", "Zhao Bowei", "Wang Jing", "Yao Dezhong", "Liu Gang"], "title": "Non-invasive two-step strategy BCI: brain-muscle-hand interface", "categories": ["q-bio.NC"], "comment": null, "summary": "Brain-computer interface enables direct interaction between brain and device.\nHowever, common brain-computer interfaces often employ one-step strategy that\nrely on non-natural paradigms, such as SSVEP-BCI and MI-BCI, are limited to\nspecific scenarios, restricting their broader application. This paper first\nproposes a two-step strategic brain-muscular-hand interface (BMHI) based on\nbiological evolutionary selection mechanism, by integrating the brain-muscle\n(BM) interface with the muscle-hand (MH) interface through crosstalk (\"BMHI =\nBM + MH\"). To verify the effectiveness of BMHI and the advantages of a two-step\nstrategy inspired by natural evolution, we conducted offline, comparison\n(comparing BMHI (two-step) and brain-hand interface (one-step)), and online\nexperiments (using BMHI to control a virtual/machine hand for daily tasks). The\nresults show that: (1) BMHI is feasible and the prediction accuracy is 0.79;\n(2) Unlike traditional multi-layer neural networks that attempt to establish a\ndirect brain-signal-to-action mapping through a single end-to-end process\n(brain-hand interface), BMHI incorporates the neuro-muscular transmission\nmechanisms evolved in biological systems as an intermediate constraint layer.\nThis phased decoding strategy can reduce training time by approximately 18-fold\nand improve decoding accuracy; (3) In the online control experiment, both the\nvirtual hand and the manipulator were able to successfully complete tasks, like\nmoving objects such as boxes or plates and holding water glasses. The results\nshow that BMHI adopts a two-step decoding strategy that mimics natural human\nneural motor pathways, improves training efficiency and prediction accuracy,\nand promotes the development of BCI technology to a more natural interaction\nmode."}
{"id": "2506.02044", "pdf": "https://arxiv.org/pdf/2506.02044", "abs": "https://arxiv.org/abs/2506.02044", "authors": ["Xinxu Wei", "Kanhao Zhao", "Yong Jiao", "Lifang He", "Yu Zhang"], "title": "A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder", "categories": ["q-bio.NC", "cs.LG"], "comment": "34pages", "summary": "As large language models (LLMs) continue to revolutionize AI research, there\nis a growing interest in building large-scale brain foundation models to\nadvance neuroscience. While most existing brain foundation models are\npre-trained on time-series signals or region-of-interest (ROI) features, we\npropose a novel graph-based pre-training paradigm for constructing a brain\ngraph foundation model. In this paper, we introduce the Brain Graph Foundation\nModel, termed BrainGFM, a unified framework that leverages graph contrastive\nlearning and graph masked autoencoders for large-scale fMRI-based pre-training.\nBrainGFM is pre-trained on a diverse mixture of brain atlases with varying\nparcellations, significantly expanding the pre-training corpus and enhancing\nthe model's ability to generalize across heterogeneous fMRI-derived brain\nrepresentations. To support efficient and versatile downstream transfer, we\nintegrate both graph prompts and language prompts into the model design,\nenabling BrainGFM to flexibly adapt to a wide range of atlases, neurological\nand psychiatric disorders, and task settings. Furthermore, we employ\nmeta-learning to optimize the graph prompts, facilitating strong generalization\nto previously unseen disorders under both few-shot and zero-shot learning\nconditions via language-guided prompting. BrainGFM is pre-trained on 27\nneuroimaging datasets spanning 25 common neurological and psychiatric\ndisorders, encompassing 2 types of brain atlases (functional and anatomical)\nacross 8 widely-used parcellations, and covering over 25,000 subjects, 60,000\nfMRI scans, and a total of 400,000 graph samples aggregated across all atlases\nand parcellations. The code is available at:\nhttps://github.com/weixinxu666/BrainGFM"}
{"id": "2506.02263", "pdf": "https://arxiv.org/pdf/2506.02263", "abs": "https://arxiv.org/abs/2506.02263", "authors": ["Qi Xin", "Robert E. Kass"], "title": "Identifying interactions across brain areas while accounting for individual-neuron dynamics with a Transformer-based variational autoencoder", "categories": ["q-bio.NC"], "comment": null, "summary": "Advances in large-scale recording technologies now enable simultaneous\nmeasurements from multiple brain areas, offering new opportunities to study\nsignal transmission across interacting components of neural circuits. However,\nneural responses exhibit substantial trial-to-trial variability, often driven\nby unobserved factors such as subtle changes in animal behavior or internal\nstates. To prevent evolving background dynamics from contaminating\nidentification of functional coupling, we developed a hybrid neural spike train\nmodel, GLM-Transformer, that incorporates flexible, deep latent variable models\ninto a point process generalized linear model (GLM) having an interpretable\ncomponent for cross-population interactions. A Transformer-based variational\nautoencoder captures nonstationary individual-neuron dynamics that vary across\ntrials, while standard nonparametric regression GLM coupling terms provide\nestimates of directed interactions between neural populations. We incorporate a\nlow-rank structure on population-to-population coupling effects to improve\nscalability. Across synthetic datasets and mechanistic simulations,\nGLM-Transformer recovers known coupling structure and remains robust to shared\nbackground fluctuations. When applied to the Allen Institute Visual Coding\ndataset, it identifies feedforward pathways consistent with established visual\nhierarchies. This work offers a step toward improved identification of neural\npopulation interactions, and contributes to ongoing efforts aimed at achieving\ninterpretable results while harvesting the benefits of deep learning."}
{"id": "2506.02813", "pdf": "https://arxiv.org/pdf/2506.02813", "abs": "https://arxiv.org/abs/2506.02813", "authors": ["Jack Cook", "Danyal Akarca", "Rui Ponte Costa", "Jascha Achterberg"], "title": "Brain-Like Processing Pathways Form in Models With Heterogeneous Experts", "categories": ["q-bio.NC", "cs.NE"], "comment": "25 pages, 14 figures", "summary": "The brain is made up of a vast set of heterogeneous regions that dynamically\norganize into pathways as a function of task demands. Examples of such pathways\ncan be seen in the interactions between cortical and subcortical networks\nduring learning. This raises the question of how exactly brain regions organize\ninto these dynamic groups. In this work, we use an extension of the\nHeterogeneous Mixture-of-Experts architecture, to show that heterogeneous\nregions do not form processing pathways by themselves, implying that the brain\nlikely implements specific constraints which result in reliable formation of\npathways. We identify three biologically relevant inductive biases that\nencourage pathway formation: a routing cost imposed on the use of more complex\nregions, a scaling factor that reduces this cost when task performance is low,\nand randomized expert dropout. When comparing our resulting Mixture-of-Pathways\nmodel with the brain, we observe that the artificial pathways match how the\nbrain uses cortical and subcortical systems to learn and solve tasks of varying\ndifficulty. In summary, we introduce a novel framework for investigating how\nthe brain forms task-specific pathways through inductive biases which may make\nMixture-of-Experts architectures in general more adaptive."}
{"id": "2506.03088", "pdf": "https://arxiv.org/pdf/2506.03088", "abs": "https://arxiv.org/abs/2506.03088", "authors": ["Lloyd Pellatt", "Fotios Drakopoulos", "Shievanie Sabesan", "Nicholas A. Lesica"], "title": "Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": "12 pages, 3 figures", "summary": "The mapping from sound to neural activity that underlies hearing is highly\nnon-linear. The first few stages of this mapping in the cochlea have been\nmodelled successfully, with biophysical models built by hand and, more\nrecently, with DNN models trained on datasets simulated by biophysical models.\nModelling the auditory brain has been a challenge because central auditory\nprocessing is too complex for models to be built by hand, and datasets for\ntraining DNN models directly have not been available. Recent work has taken\nadvantage of large-scale high resolution neural recordings from the auditory\nmidbrain to build a DNN model of normal hearing with great success. But this\nmodel assumes that auditory processing is the same in all brains, and therefore\nit cannot capture the widely varying effects of hearing loss.\n  We propose a novel variational-conditional model to learn to encode the space\nof hearing loss directly from recordings of neural activity in the auditory\nmidbrain of healthy and noise exposed animals. With hearing loss parametrised\nby only 6 free parameters per animal, our model accurately predicts 62\\% of the\nexplainable variance in neural responses from normal hearing animals and 68%\nfor hearing impaired animals, within a few percentage points of state of the\nart animal specific models. We demonstrate that the model can be used to\nsimulate realistic activity from out of sample animals by fitting only the\nlearned conditioning parameters with Bayesian optimisation, achieving\ncrossentropy loss within 2% of the optimum in 15-30 iterations. Including more\nanimals in the training data slightly improved the performance on unseen\nanimals. This model will enable future development of parametrised hearing loss\ncompensation models trained to directly restore normal neural coding in hearing\nimpaired brains, which can be quickly fitted for a new user by human in the\nloop optimisation."}
{"id": "2505.21777", "pdf": "https://arxiv.org/pdf/2505.21777", "abs": "https://arxiv.org/abs/2505.21777", "authors": ["Bao Pham", "Gabriel Raya", "Matteo Negri", "Mohammed J. Zaki", "Luca Ambrogioni", "Dmitry Krotov"], "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.CV", "q-bio.NC", "stat.ML"], "comment": null, "summary": "Hopfield networks are associative memory (AM) systems, designed for storing\nand retrieving patterns as local minima of an energy landscape. In the\nclassical Hopfield model, an interesting phenomenon occurs when the amount of\ntraining data reaches its critical memory load $- spurious\\,\\,states$, or\nunintended stable points, emerge at the end of the retrieval dynamics, leading\nto incorrect recall. In this work, we examine diffusion models, commonly used\nin generative modeling, from the perspective of AMs. The training phase of\ndiffusion model is conceptualized as memory encoding (training data is stored\nin the memory). The generation phase is viewed as an attempt of memory\nretrieval. In the small data regime the diffusion model exhibits a strong\nmemorization phase, where the network creates distinct basins of attraction\naround each sample in the training set, akin to the Hopfield model below the\ncritical memory load. In the large data regime, a different phase appears where\nan increase in the size of the training set fosters the creation of new\nattractor states that correspond to manifolds of the generated samples.\nSpurious states appear at the boundary of this transition and correspond to\nemergent attractor states, which are absent in the training set, but, at the\nsame time, have distinct basins of attraction around them. Our findings\nprovide: a novel perspective on the memorization-generalization phenomenon in\ndiffusion models via the lens of AMs, theoretical prediction of existence of\nspurious states, empirical validation of this prediction in commonly-used\ndiffusion models."}
{"id": "2506.02164", "pdf": "https://arxiv.org/pdf/2506.02164", "abs": "https://arxiv.org/abs/2506.02164", "authors": ["Yu", "Qian", "Wilson S. Geisler", "Xue-Xin Wei"], "title": "Quantifying task-relevant representational similarity using decision variable correlation", "categories": ["cs.CV", "cs.LG", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Previous studies have compared the brain and deep neural networks trained on\nimage classification. Intriguingly, while some suggest that their\nrepresentations are highly similar, others argued the opposite. Here, we\npropose a new approach to characterize the similarity of the decision\nstrategies of two observers (models or brains) using decision variable\ncorrelation (DVC). DVC quantifies the correlation between decoded decisions on\nindividual samples in a classification task and thus can capture task-relevant\ninformation rather than general representational alignment. We evaluate this\nmethod using monkey V4/IT recordings and models trained on image classification\ntasks.\n  We find that model--model similarity is comparable to monkey--monkey\nsimilarity, whereas model--monkey similarity is consistently lower and,\nsurprisingly, decreases with increasing ImageNet-1k performance. While\nadversarial training enhances robustness, it does not improve model--monkey\nsimilarity in task-relevant dimensions; however, it markedly increases\nmodel--model similarity. Similarly, pre-training on larger datasets does not\nimprove model--monkey similarity. These results suggest a fundamental\ndivergence between the task-relevant representations in monkey V4/IT and those\nlearned by models trained on image classification tasks."}
{"id": "2506.03089", "pdf": "https://arxiv.org/pdf/2506.03089", "abs": "https://arxiv.org/abs/2506.03089", "authors": ["Lucas Piper", "Arlindo L. Oliveira", "Tiago Marques"], "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches."}
{"id": "2506.02013", "pdf": "https://arxiv.org/pdf/2506.02013", "abs": "https://arxiv.org/abs/2506.02013", "authors": ["Sun Ye", "Zuo Cuiming", "Zhang Rui", "Shi Bin", "Pang Yajing", "Gao Lingyun", "Zhao Bowei", "Wang Jing", "Yao Dezhong", "Liu Gang"], "title": "Non-invasive two-step strategy BCI: brain-muscle-hand interface", "categories": ["q-bio.NC"], "comment": null, "summary": "Brain-computer interface enables direct interaction between brain and device.\nHowever, common brain-computer interfaces often employ one-step strategy that\nrely on non-natural paradigms, such as SSVEP-BCI and MI-BCI, are limited to\nspecific scenarios, restricting their broader application. This paper first\nproposes a two-step strategic brain-muscular-hand interface (BMHI) based on\nbiological evolutionary selection mechanism, by integrating the brain-muscle\n(BM) interface with the muscle-hand (MH) interface through crosstalk (\"BMHI =\nBM + MH\"). To verify the effectiveness of BMHI and the advantages of a two-step\nstrategy inspired by natural evolution, we conducted offline, comparison\n(comparing BMHI (two-step) and brain-hand interface (one-step)), and online\nexperiments (using BMHI to control a virtual/machine hand for daily tasks). The\nresults show that: (1) BMHI is feasible and the prediction accuracy is 0.79;\n(2) Unlike traditional multi-layer neural networks that attempt to establish a\ndirect brain-signal-to-action mapping through a single end-to-end process\n(brain-hand interface), BMHI incorporates the neuro-muscular transmission\nmechanisms evolved in biological systems as an intermediate constraint layer.\nThis phased decoding strategy can reduce training time by approximately 18-fold\nand improve decoding accuracy; (3) In the online control experiment, both the\nvirtual hand and the manipulator were able to successfully complete tasks, like\nmoving objects such as boxes or plates and holding water glasses. The results\nshow that BMHI adopts a two-step decoding strategy that mimics natural human\nneural motor pathways, improves training efficiency and prediction accuracy,\nand promotes the development of BCI technology to a more natural interaction\nmode."}
{"id": "2506.02044", "pdf": "https://arxiv.org/pdf/2506.02044", "abs": "https://arxiv.org/abs/2506.02044", "authors": ["Xinxu Wei", "Kanhao Zhao", "Yong Jiao", "Lifang He", "Yu Zhang"], "title": "A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder", "categories": ["q-bio.NC", "cs.LG"], "comment": "34pages", "summary": "As large language models (LLMs) continue to revolutionize AI research, there\nis a growing interest in building large-scale brain foundation models to\nadvance neuroscience. While most existing brain foundation models are\npre-trained on time-series signals or region-of-interest (ROI) features, we\npropose a novel graph-based pre-training paradigm for constructing a brain\ngraph foundation model. In this paper, we introduce the Brain Graph Foundation\nModel, termed BrainGFM, a unified framework that leverages graph contrastive\nlearning and graph masked autoencoders for large-scale fMRI-based pre-training.\nBrainGFM is pre-trained on a diverse mixture of brain atlases with varying\nparcellations, significantly expanding the pre-training corpus and enhancing\nthe model's ability to generalize across heterogeneous fMRI-derived brain\nrepresentations. To support efficient and versatile downstream transfer, we\nintegrate both graph prompts and language prompts into the model design,\nenabling BrainGFM to flexibly adapt to a wide range of atlases, neurological\nand psychiatric disorders, and task settings. Furthermore, we employ\nmeta-learning to optimize the graph prompts, facilitating strong generalization\nto previously unseen disorders under both few-shot and zero-shot learning\nconditions via language-guided prompting. BrainGFM is pre-trained on 27\nneuroimaging datasets spanning 25 common neurological and psychiatric\ndisorders, encompassing 2 types of brain atlases (functional and anatomical)\nacross 8 widely-used parcellations, and covering over 25,000 subjects, 60,000\nfMRI scans, and a total of 400,000 graph samples aggregated across all atlases\nand parcellations. The code is available at:\nhttps://github.com/weixinxu666/BrainGFM"}
{"id": "2506.02263", "pdf": "https://arxiv.org/pdf/2506.02263", "abs": "https://arxiv.org/abs/2506.02263", "authors": ["Qi Xin", "Robert E. Kass"], "title": "Identifying interactions across brain areas while accounting for individual-neuron dynamics with a Transformer-based variational autoencoder", "categories": ["q-bio.NC"], "comment": null, "summary": "Advances in large-scale recording technologies now enable simultaneous\nmeasurements from multiple brain areas, offering new opportunities to study\nsignal transmission across interacting components of neural circuits. However,\nneural responses exhibit substantial trial-to-trial variability, often driven\nby unobserved factors such as subtle changes in animal behavior or internal\nstates. To prevent evolving background dynamics from contaminating\nidentification of functional coupling, we developed a hybrid neural spike train\nmodel, GLM-Transformer, that incorporates flexible, deep latent variable models\ninto a point process generalized linear model (GLM) having an interpretable\ncomponent for cross-population interactions. A Transformer-based variational\nautoencoder captures nonstationary individual-neuron dynamics that vary across\ntrials, while standard nonparametric regression GLM coupling terms provide\nestimates of directed interactions between neural populations. We incorporate a\nlow-rank structure on population-to-population coupling effects to improve\nscalability. Across synthetic datasets and mechanistic simulations,\nGLM-Transformer recovers known coupling structure and remains robust to shared\nbackground fluctuations. When applied to the Allen Institute Visual Coding\ndataset, it identifies feedforward pathways consistent with established visual\nhierarchies. This work offers a step toward improved identification of neural\npopulation interactions, and contributes to ongoing efforts aimed at achieving\ninterpretable results while harvesting the benefits of deep learning."}
{"id": "2506.02813", "pdf": "https://arxiv.org/pdf/2506.02813", "abs": "https://arxiv.org/abs/2506.02813", "authors": ["Jack Cook", "Danyal Akarca", "Rui Ponte Costa", "Jascha Achterberg"], "title": "Brain-Like Processing Pathways Form in Models With Heterogeneous Experts", "categories": ["q-bio.NC", "cs.NE"], "comment": "25 pages, 14 figures", "summary": "The brain is made up of a vast set of heterogeneous regions that dynamically\norganize into pathways as a function of task demands. Examples of such pathways\ncan be seen in the interactions between cortical and subcortical networks\nduring learning. This raises the question of how exactly brain regions organize\ninto these dynamic groups. In this work, we use an extension of the\nHeterogeneous Mixture-of-Experts architecture, to show that heterogeneous\nregions do not form processing pathways by themselves, implying that the brain\nlikely implements specific constraints which result in reliable formation of\npathways. We identify three biologically relevant inductive biases that\nencourage pathway formation: a routing cost imposed on the use of more complex\nregions, a scaling factor that reduces this cost when task performance is low,\nand randomized expert dropout. When comparing our resulting Mixture-of-Pathways\nmodel with the brain, we observe that the artificial pathways match how the\nbrain uses cortical and subcortical systems to learn and solve tasks of varying\ndifficulty. In summary, we introduce a novel framework for investigating how\nthe brain forms task-specific pathways through inductive biases which may make\nMixture-of-Experts architectures in general more adaptive."}
{"id": "2506.03088", "pdf": "https://arxiv.org/pdf/2506.03088", "abs": "https://arxiv.org/abs/2506.03088", "authors": ["Lloyd Pellatt", "Fotios Drakopoulos", "Shievanie Sabesan", "Nicholas A. Lesica"], "title": "Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": "12 pages, 3 figures", "summary": "The mapping from sound to neural activity that underlies hearing is highly\nnon-linear. The first few stages of this mapping in the cochlea have been\nmodelled successfully, with biophysical models built by hand and, more\nrecently, with DNN models trained on datasets simulated by biophysical models.\nModelling the auditory brain has been a challenge because central auditory\nprocessing is too complex for models to be built by hand, and datasets for\ntraining DNN models directly have not been available. Recent work has taken\nadvantage of large-scale high resolution neural recordings from the auditory\nmidbrain to build a DNN model of normal hearing with great success. But this\nmodel assumes that auditory processing is the same in all brains, and therefore\nit cannot capture the widely varying effects of hearing loss.\n  We propose a novel variational-conditional model to learn to encode the space\nof hearing loss directly from recordings of neural activity in the auditory\nmidbrain of healthy and noise exposed animals. With hearing loss parametrised\nby only 6 free parameters per animal, our model accurately predicts 62\\% of the\nexplainable variance in neural responses from normal hearing animals and 68%\nfor hearing impaired animals, within a few percentage points of state of the\nart animal specific models. We demonstrate that the model can be used to\nsimulate realistic activity from out of sample animals by fitting only the\nlearned conditioning parameters with Bayesian optimisation, achieving\ncrossentropy loss within 2% of the optimum in 15-30 iterations. Including more\nanimals in the training data slightly improved the performance on unseen\nanimals. This model will enable future development of parametrised hearing loss\ncompensation models trained to directly restore normal neural coding in hearing\nimpaired brains, which can be quickly fitted for a new user by human in the\nloop optimisation."}
{"id": "2505.21777", "pdf": "https://arxiv.org/pdf/2505.21777", "abs": "https://arxiv.org/abs/2505.21777", "authors": ["Bao Pham", "Gabriel Raya", "Matteo Negri", "Mohammed J. Zaki", "Luca Ambrogioni", "Dmitry Krotov"], "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.CV", "q-bio.NC", "stat.ML"], "comment": null, "summary": "Hopfield networks are associative memory (AM) systems, designed for storing\nand retrieving patterns as local minima of an energy landscape. In the\nclassical Hopfield model, an interesting phenomenon occurs when the amount of\ntraining data reaches its critical memory load $- spurious\\,\\,states$, or\nunintended stable points, emerge at the end of the retrieval dynamics, leading\nto incorrect recall. In this work, we examine diffusion models, commonly used\nin generative modeling, from the perspective of AMs. The training phase of\ndiffusion model is conceptualized as memory encoding (training data is stored\nin the memory). The generation phase is viewed as an attempt of memory\nretrieval. In the small data regime the diffusion model exhibits a strong\nmemorization phase, where the network creates distinct basins of attraction\naround each sample in the training set, akin to the Hopfield model below the\ncritical memory load. In the large data regime, a different phase appears where\nan increase in the size of the training set fosters the creation of new\nattractor states that correspond to manifolds of the generated samples.\nSpurious states appear at the boundary of this transition and correspond to\nemergent attractor states, which are absent in the training set, but, at the\nsame time, have distinct basins of attraction around them. Our findings\nprovide: a novel perspective on the memorization-generalization phenomenon in\ndiffusion models via the lens of AMs, theoretical prediction of existence of\nspurious states, empirical validation of this prediction in commonly-used\ndiffusion models."}
{"id": "2506.02164", "pdf": "https://arxiv.org/pdf/2506.02164", "abs": "https://arxiv.org/abs/2506.02164", "authors": ["Yu", "Qian", "Wilson S. Geisler", "Xue-Xin Wei"], "title": "Quantifying task-relevant representational similarity using decision variable correlation", "categories": ["cs.CV", "cs.LG", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Previous studies have compared the brain and deep neural networks trained on\nimage classification. Intriguingly, while some suggest that their\nrepresentations are highly similar, others argued the opposite. Here, we\npropose a new approach to characterize the similarity of the decision\nstrategies of two observers (models or brains) using decision variable\ncorrelation (DVC). DVC quantifies the correlation between decoded decisions on\nindividual samples in a classification task and thus can capture task-relevant\ninformation rather than general representational alignment. We evaluate this\nmethod using monkey V4/IT recordings and models trained on image classification\ntasks.\n  We find that model--model similarity is comparable to monkey--monkey\nsimilarity, whereas model--monkey similarity is consistently lower and,\nsurprisingly, decreases with increasing ImageNet-1k performance. While\nadversarial training enhances robustness, it does not improve model--monkey\nsimilarity in task-relevant dimensions; however, it markedly increases\nmodel--model similarity. Similarly, pre-training on larger datasets does not\nimprove model--monkey similarity. These results suggest a fundamental\ndivergence between the task-relevant representations in monkey V4/IT and those\nlearned by models trained on image classification tasks."}
{"id": "2506.03089", "pdf": "https://arxiv.org/pdf/2506.03089", "abs": "https://arxiv.org/abs/2506.03089", "authors": ["Lucas Piper", "Arlindo L. Oliveira", "Tiago Marques"], "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches."}
{"id": "2506.02013", "pdf": "https://arxiv.org/pdf/2506.02013", "abs": "https://arxiv.org/abs/2506.02013", "authors": ["Sun Ye", "Zuo Cuiming", "Zhang Rui", "Shi Bin", "Pang Yajing", "Gao Lingyun", "Zhao Bowei", "Wang Jing", "Yao Dezhong", "Liu Gang"], "title": "Non-invasive two-step strategy BCI: brain-muscle-hand interface", "categories": ["q-bio.NC"], "comment": null, "summary": "Brain-computer interface enables direct interaction between brain and device.\nHowever, common brain-computer interfaces often employ one-step strategy that\nrely on non-natural paradigms, such as SSVEP-BCI and MI-BCI, are limited to\nspecific scenarios, restricting their broader application. This paper first\nproposes a two-step strategic brain-muscular-hand interface (BMHI) based on\nbiological evolutionary selection mechanism, by integrating the brain-muscle\n(BM) interface with the muscle-hand (MH) interface through crosstalk (\"BMHI =\nBM + MH\"). To verify the effectiveness of BMHI and the advantages of a two-step\nstrategy inspired by natural evolution, we conducted offline, comparison\n(comparing BMHI (two-step) and brain-hand interface (one-step)), and online\nexperiments (using BMHI to control a virtual/machine hand for daily tasks). The\nresults show that: (1) BMHI is feasible and the prediction accuracy is 0.79;\n(2) Unlike traditional multi-layer neural networks that attempt to establish a\ndirect brain-signal-to-action mapping through a single end-to-end process\n(brain-hand interface), BMHI incorporates the neuro-muscular transmission\nmechanisms evolved in biological systems as an intermediate constraint layer.\nThis phased decoding strategy can reduce training time by approximately 18-fold\nand improve decoding accuracy; (3) In the online control experiment, both the\nvirtual hand and the manipulator were able to successfully complete tasks, like\nmoving objects such as boxes or plates and holding water glasses. The results\nshow that BMHI adopts a two-step decoding strategy that mimics natural human\nneural motor pathways, improves training efficiency and prediction accuracy,\nand promotes the development of BCI technology to a more natural interaction\nmode."}
{"id": "2506.02044", "pdf": "https://arxiv.org/pdf/2506.02044", "abs": "https://arxiv.org/abs/2506.02044", "authors": ["Xinxu Wei", "Kanhao Zhao", "Yong Jiao", "Lifang He", "Yu Zhang"], "title": "A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder", "categories": ["q-bio.NC", "cs.LG"], "comment": "34pages", "summary": "As large language models (LLMs) continue to revolutionize AI research, there\nis a growing interest in building large-scale brain foundation models to\nadvance neuroscience. While most existing brain foundation models are\npre-trained on time-series signals or region-of-interest (ROI) features, we\npropose a novel graph-based pre-training paradigm for constructing a brain\ngraph foundation model. In this paper, we introduce the Brain Graph Foundation\nModel, termed BrainGFM, a unified framework that leverages graph contrastive\nlearning and graph masked autoencoders for large-scale fMRI-based pre-training.\nBrainGFM is pre-trained on a diverse mixture of brain atlases with varying\nparcellations, significantly expanding the pre-training corpus and enhancing\nthe model's ability to generalize across heterogeneous fMRI-derived brain\nrepresentations. To support efficient and versatile downstream transfer, we\nintegrate both graph prompts and language prompts into the model design,\nenabling BrainGFM to flexibly adapt to a wide range of atlases, neurological\nand psychiatric disorders, and task settings. Furthermore, we employ\nmeta-learning to optimize the graph prompts, facilitating strong generalization\nto previously unseen disorders under both few-shot and zero-shot learning\nconditions via language-guided prompting. BrainGFM is pre-trained on 27\nneuroimaging datasets spanning 25 common neurological and psychiatric\ndisorders, encompassing 2 types of brain atlases (functional and anatomical)\nacross 8 widely-used parcellations, and covering over 25,000 subjects, 60,000\nfMRI scans, and a total of 400,000 graph samples aggregated across all atlases\nand parcellations. The code is available at:\nhttps://github.com/weixinxu666/BrainGFM"}
{"id": "2506.02263", "pdf": "https://arxiv.org/pdf/2506.02263", "abs": "https://arxiv.org/abs/2506.02263", "authors": ["Qi Xin", "Robert E. Kass"], "title": "Identifying interactions across brain areas while accounting for individual-neuron dynamics with a Transformer-based variational autoencoder", "categories": ["q-bio.NC"], "comment": null, "summary": "Advances in large-scale recording technologies now enable simultaneous\nmeasurements from multiple brain areas, offering new opportunities to study\nsignal transmission across interacting components of neural circuits. However,\nneural responses exhibit substantial trial-to-trial variability, often driven\nby unobserved factors such as subtle changes in animal behavior or internal\nstates. To prevent evolving background dynamics from contaminating\nidentification of functional coupling, we developed a hybrid neural spike train\nmodel, GLM-Transformer, that incorporates flexible, deep latent variable models\ninto a point process generalized linear model (GLM) having an interpretable\ncomponent for cross-population interactions. A Transformer-based variational\nautoencoder captures nonstationary individual-neuron dynamics that vary across\ntrials, while standard nonparametric regression GLM coupling terms provide\nestimates of directed interactions between neural populations. We incorporate a\nlow-rank structure on population-to-population coupling effects to improve\nscalability. Across synthetic datasets and mechanistic simulations,\nGLM-Transformer recovers known coupling structure and remains robust to shared\nbackground fluctuations. When applied to the Allen Institute Visual Coding\ndataset, it identifies feedforward pathways consistent with established visual\nhierarchies. This work offers a step toward improved identification of neural\npopulation interactions, and contributes to ongoing efforts aimed at achieving\ninterpretable results while harvesting the benefits of deep learning."}
{"id": "2506.02813", "pdf": "https://arxiv.org/pdf/2506.02813", "abs": "https://arxiv.org/abs/2506.02813", "authors": ["Jack Cook", "Danyal Akarca", "Rui Ponte Costa", "Jascha Achterberg"], "title": "Brain-Like Processing Pathways Form in Models With Heterogeneous Experts", "categories": ["q-bio.NC", "cs.NE"], "comment": "25 pages, 14 figures", "summary": "The brain is made up of a vast set of heterogeneous regions that dynamically\norganize into pathways as a function of task demands. Examples of such pathways\ncan be seen in the interactions between cortical and subcortical networks\nduring learning. This raises the question of how exactly brain regions organize\ninto these dynamic groups. In this work, we use an extension of the\nHeterogeneous Mixture-of-Experts architecture, to show that heterogeneous\nregions do not form processing pathways by themselves, implying that the brain\nlikely implements specific constraints which result in reliable formation of\npathways. We identify three biologically relevant inductive biases that\nencourage pathway formation: a routing cost imposed on the use of more complex\nregions, a scaling factor that reduces this cost when task performance is low,\nand randomized expert dropout. When comparing our resulting Mixture-of-Pathways\nmodel with the brain, we observe that the artificial pathways match how the\nbrain uses cortical and subcortical systems to learn and solve tasks of varying\ndifficulty. In summary, we introduce a novel framework for investigating how\nthe brain forms task-specific pathways through inductive biases which may make\nMixture-of-Experts architectures in general more adaptive."}
{"id": "2506.03088", "pdf": "https://arxiv.org/pdf/2506.03088", "abs": "https://arxiv.org/abs/2506.03088", "authors": ["Lloyd Pellatt", "Fotios Drakopoulos", "Shievanie Sabesan", "Nicholas A. Lesica"], "title": "Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning", "categories": ["q-bio.NC", "cs.AI", "cs.LG"], "comment": "12 pages, 3 figures", "summary": "The mapping from sound to neural activity that underlies hearing is highly\nnon-linear. The first few stages of this mapping in the cochlea have been\nmodelled successfully, with biophysical models built by hand and, more\nrecently, with DNN models trained on datasets simulated by biophysical models.\nModelling the auditory brain has been a challenge because central auditory\nprocessing is too complex for models to be built by hand, and datasets for\ntraining DNN models directly have not been available. Recent work has taken\nadvantage of large-scale high resolution neural recordings from the auditory\nmidbrain to build a DNN model of normal hearing with great success. But this\nmodel assumes that auditory processing is the same in all brains, and therefore\nit cannot capture the widely varying effects of hearing loss.\n  We propose a novel variational-conditional model to learn to encode the space\nof hearing loss directly from recordings of neural activity in the auditory\nmidbrain of healthy and noise exposed animals. With hearing loss parametrised\nby only 6 free parameters per animal, our model accurately predicts 62\\% of the\nexplainable variance in neural responses from normal hearing animals and 68%\nfor hearing impaired animals, within a few percentage points of state of the\nart animal specific models. We demonstrate that the model can be used to\nsimulate realistic activity from out of sample animals by fitting only the\nlearned conditioning parameters with Bayesian optimisation, achieving\ncrossentropy loss within 2% of the optimum in 15-30 iterations. Including more\nanimals in the training data slightly improved the performance on unseen\nanimals. This model will enable future development of parametrised hearing loss\ncompensation models trained to directly restore normal neural coding in hearing\nimpaired brains, which can be quickly fitted for a new user by human in the\nloop optimisation."}
{"id": "2505.21777", "pdf": "https://arxiv.org/pdf/2505.21777", "abs": "https://arxiv.org/abs/2505.21777", "authors": ["Bao Pham", "Gabriel Raya", "Matteo Negri", "Mohammed J. Zaki", "Luca Ambrogioni", "Dmitry Krotov"], "title": "Memorization to Generalization: Emergence of Diffusion Models from Associative Memory", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.CV", "q-bio.NC", "stat.ML"], "comment": null, "summary": "Hopfield networks are associative memory (AM) systems, designed for storing\nand retrieving patterns as local minima of an energy landscape. In the\nclassical Hopfield model, an interesting phenomenon occurs when the amount of\ntraining data reaches its critical memory load $- spurious\\,\\,states$, or\nunintended stable points, emerge at the end of the retrieval dynamics, leading\nto incorrect recall. In this work, we examine diffusion models, commonly used\nin generative modeling, from the perspective of AMs. The training phase of\ndiffusion model is conceptualized as memory encoding (training data is stored\nin the memory). The generation phase is viewed as an attempt of memory\nretrieval. In the small data regime the diffusion model exhibits a strong\nmemorization phase, where the network creates distinct basins of attraction\naround each sample in the training set, akin to the Hopfield model below the\ncritical memory load. In the large data regime, a different phase appears where\nan increase in the size of the training set fosters the creation of new\nattractor states that correspond to manifolds of the generated samples.\nSpurious states appear at the boundary of this transition and correspond to\nemergent attractor states, which are absent in the training set, but, at the\nsame time, have distinct basins of attraction around them. Our findings\nprovide: a novel perspective on the memorization-generalization phenomenon in\ndiffusion models via the lens of AMs, theoretical prediction of existence of\nspurious states, empirical validation of this prediction in commonly-used\ndiffusion models."}
{"id": "2506.02164", "pdf": "https://arxiv.org/pdf/2506.02164", "abs": "https://arxiv.org/abs/2506.02164", "authors": ["Yu", "Qian", "Wilson S. Geisler", "Xue-Xin Wei"], "title": "Quantifying task-relevant representational similarity using decision variable correlation", "categories": ["cs.CV", "cs.LG", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Previous studies have compared the brain and deep neural networks trained on\nimage classification. Intriguingly, while some suggest that their\nrepresentations are highly similar, others argued the opposite. Here, we\npropose a new approach to characterize the similarity of the decision\nstrategies of two observers (models or brains) using decision variable\ncorrelation (DVC). DVC quantifies the correlation between decoded decisions on\nindividual samples in a classification task and thus can capture task-relevant\ninformation rather than general representational alignment. We evaluate this\nmethod using monkey V4/IT recordings and models trained on image classification\ntasks.\n  We find that model--model similarity is comparable to monkey--monkey\nsimilarity, whereas model--monkey similarity is consistently lower and,\nsurprisingly, decreases with increasing ImageNet-1k performance. While\nadversarial training enhances robustness, it does not improve model--monkey\nsimilarity in task-relevant dimensions; however, it markedly increases\nmodel--model similarity. Similarly, pre-training on larger datasets does not\nimprove model--monkey similarity. These results suggest a fundamental\ndivergence between the task-relevant representations in monkey V4/IT and those\nlearned by models trained on image classification tasks."}
{"id": "2506.03089", "pdf": "https://arxiv.org/pdf/2506.03089", "abs": "https://arxiv.org/abs/2506.03089", "authors": ["Lucas Piper", "Arlindo L. Oliveira", "Tiago Marques"], "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness", "categories": ["cs.CV", "q-bio.NC"], "comment": null, "summary": "Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches."}
