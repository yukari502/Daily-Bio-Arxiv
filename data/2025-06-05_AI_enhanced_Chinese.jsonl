{"id": "2506.03214", "pdf": "https://arxiv.org/pdf/2506.03214", "abs": "https://arxiv.org/abs/2506.03214", "authors": ["Yi Guo", "Yihang Dong", "Michael Kwok-Po Ng", "Shuqiang Wang"], "title": "A Pre-trained Framework for Multilingual Brain Decoding Using Non-invasive Recordings", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Brain-computer interfaces (BCIs) with speech decoding from brain recordings\nhave broad application potential in fields such as clinical rehabilitation and\ncognitive neuroscience. However, current decoding methods remain limited to\nsingle-language, single-subject, and single neuroimaging modality settings,\nrestricting their clinical applicability and generalizability. Here we propose\na joint multilingual, multi-subject and multimodal decoding framework. It maps\ndiverse brain recordings into a unified semantic space defined by a pre-trained\nmultilingual model (PMM), enabling decoding across multiple languages, multiple\nsubjects and multiple neuroimaging modalities. The proposed framework is\nvalidated using non-invasive brain recordings from 159 participants across four\nlanguages. Experimental results show that it exhibits strong generalization\nacross multilingual, multi-subject, and multimodal settings. More importantly,\nthe proposed framework can promote linguistic fairness, which is vital for\nunderrepresented languages in BCI applications. The unified semantic space\nenables cross-lingual mapping enhancement, allowing the framework to boost the\ndecoding performance of underrepresented languages, thereby promoting\nlinguistic fairness. Overall, the proposed framework establishes a new\npotential paradigm for brain decoding, opening new paths for broader\napplications of BCI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u3001\u591a\u88ab\u8bd5\u548c\u591a\u6a21\u6001\u7684\u8054\u5408\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u63d0\u5347\u4e86\u8111\u673a\u63a5\u53e3\u7684\u89e3\u7801\u6027\u80fd\u548c\u8bed\u8a00\u516c\u5e73\u6027\u3002", "motivation": "\u5f53\u524d\u8111\u673a\u63a5\u53e3\u7684\u89e3\u7801\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u8bed\u8a00\u3001\u5355\u88ab\u8bd5\u548c\u5355\u6a21\u6001\uff0c\u9650\u5236\u4e86\u5176\u4e34\u5e8a\u5e94\u7528\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u591a\u6837\u5316\u7684\u8111\u8bb0\u5f55\u6620\u5c04\u5230\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\u5b9a\u4e49\u7684\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u5b9e\u73b0\u8de8\u8bed\u8a00\u3001\u8de8\u88ab\u8bd5\u548c\u591a\u6a21\u6001\u7684\u89e3\u7801\u3002", "result": "\u5728159\u540d\u53c2\u4e0e\u8005\u7684\u975e\u4fb5\u5165\u6027\u8111\u8bb0\u5f55\u4e0a\u9a8c\u8bc1\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u8bed\u8a00\u3001\u8de8\u88ab\u8bd5\u548c\u591a\u6a21\u6001\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u5347\u4e86\u5c11\u6570\u8bed\u8a00\u7684\u89e3\u7801\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8111\u89e3\u7801\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u62d3\u5bbd\u4e86\u8111\u673a\u63a5\u53e3\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5e76\u4fc3\u8fdb\u4e86\u8bed\u8a00\u516c\u5e73\u6027\u3002"}}
{"id": "2506.03293", "pdf": "https://arxiv.org/pdf/2506.03293", "abs": "https://arxiv.org/abs/2506.03293", "authors": ["Nina S. Nellen", "Polina Turishcheva", "Michaela Vystr\u010dilov\u00e1", "Shashwat Sridhar", "Tim Gollisch", "Andreas S. Tolias", "Alexander S. Ecker"], "title": "Learning to cluster neuronal function", "categories": ["q-bio.NC"], "comment": null, "summary": "Deep neural networks trained to predict neural activity from visual input and\nbehaviour have shown great potential to serve as digital twins of the visual\ncortex. Per-neuron embeddings derived from these models could potentially be\nused to map the functional landscape or identify cell types. However,\nstate-of-the-art predictive models of mouse V1 do not generate functional\nembeddings that exhibit clear clustering patterns which would correspond to\ncell types. This raises the question whether the lack of clustered structure is\ndue to limitations of current models or a true feature of the functional\norganization of mouse V1. In this work, we introduce DECEMber -- Deep Embedding\nClustering via Expectation Maximization-based refinement -- an explicit\ninductive bias into predictive models that enhances clustering by adding an\nauxiliary $t$-distribution-inspired loss function that enforces structured\norganization among per-neuron embeddings. We jointly optimize both neuronal\nfeature embeddings and clustering parameters, updating cluster centers and\nscale matrices using the EM-algorithm. We demonstrate that these modifications\nimprove cluster consistency while preserving high predictive performance and\nsurpassing standard clustering methods in terms of stability. Moreover,\nDECEMber generalizes well across species (mice, primates) and visual areas\n(retina, V1, V4). The code is available at\nhttps://github.com/Nisone2000/sensorium/tree/neuroips_version.", "AI": {"tldr": "DECEMber\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u635f\u5931\u51fd\u6570\u548c\u6539\u8fdb\u805a\u7c7b\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u795e\u7ecf\u5143\u5d4c\u5165\u7684\u805a\u7c7b\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u751f\u6210\u7684\u795e\u7ecf\u5143\u5d4c\u5165\u7f3a\u4e4f\u6e05\u6670\u7684\u805a\u7c7b\u6a21\u5f0f\uff0c\u65e0\u6cd5\u6709\u6548\u6620\u5c04\u529f\u80fd\u666f\u89c2\u6216\u8bc6\u522b\u7ec6\u80de\u7c7b\u578b\uff0cDECEMber\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "DECEMber\u901a\u8fc7\u8054\u5408\u4f18\u5316\u795e\u7ecf\u5143\u7279\u5f81\u5d4c\u5165\u548c\u805a\u7c7b\u53c2\u6570\uff0c\u5229\u7528EM\u7b97\u6cd5\u66f4\u65b0\u805a\u7c7b\u4e2d\u5fc3\u548c\u5c3a\u5ea6\u77e9\u9635\uff0c\u589e\u5f3a\u805a\u7c7b\u7ed3\u6784\u3002", "result": "DECEMber\u63d0\u9ad8\u4e86\u805a\u7c7b\u4e00\u81f4\u6027\uff0c\u4fdd\u6301\u4e86\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u6807\u51c6\u805a\u7c7b\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u7269\u79cd\u548c\u89c6\u89c9\u533a\u57df\u3002", "conclusion": "DECEMber\u4e3a\u795e\u7ecf\u5143\u529f\u80fd\u7ec4\u7ec7\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u573a\u666f\u3002"}}
{"id": "2506.03640", "pdf": "https://arxiv.org/pdf/2506.03640", "abs": "https://arxiv.org/abs/2506.03640", "authors": ["Rub\u00e9n Calvo", "Carles Martorell", "Adri\u00e1n Roig", "Miguel A. Mu\u00f1oz"], "title": "Robust Scaling in Human Brain Dynamics Despite Latent Variables and Limited Sampling Distortions", "categories": ["q-bio.NC", "cond-mat.dis-nn", "cond-mat.stat-mech"], "comment": null, "summary": "The idea that information-processing systems operate near criticality to\nenhance computational performance is supported by scaling signatures in brain\nactivity. However, external signals raise the question of whether this behavior\nis intrinsic or input-driven. We show that autocorrelated inputs and temporal\nresolution influence observed scaling exponents in simple neural models. We\nalso demonstrate analytically that under subsampling, non-critical systems\ndriven by independent autocorrelated signals can exhibit strong signatures of\napparent criticality. To address these pitfalls, we develop a robust framework\nand apply it to pooled neural data, revealing resting-state brain activity at\nthe population level is slightly sub-critical yet near-critical. Notably, the\nextracted critical exponents closely match predictions from a simple recurrent\nfiring-rate model, supporting the emergence of near-critical dynamics from\nreverberant network activity, with potential implications for information\nprocessing and artificial intelligence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4fe1\u606f\u5904\u7406\u7cfb\u7edf\u662f\u5426\u56e0\u4e34\u754c\u6027\u800c\u63d0\u5347\u8ba1\u7b97\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u5916\u90e8\u4fe1\u53f7\u5bf9\u4e34\u754c\u6027\u7279\u5f81\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u53d1\u73b0\u975e\u4e34\u754c\u7cfb\u7edf\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4f1a\u8868\u73b0\u51fa\u4e34\u754c\u6027\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\u6765\u533a\u5206\u771f\u5b9e\u4e34\u754c\u6027\u3002\u6700\u7ec8\u63ed\u793a\u9759\u606f\u6001\u5927\u8111\u6d3b\u52a8\u63a5\u8fd1\u4e34\u754c\u72b6\u6001\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u4fe1\u606f\u5904\u7406\u7cfb\u7edf\u662f\u5426\u56e0\u4e34\u754c\u6027\u800c\u4f18\u5316\u6027\u80fd\u7684\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u5916\u90e8\u4fe1\u53f7\u5bf9\u4e34\u754c\u6027\u7279\u5f81\u7684\u5f71\u54cd\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u7406\u8bba\u5206\u6790\u7b80\u5355\u795e\u7ecf\u6a21\u578b\u4e2d\u7684\u81ea\u76f8\u5173\u8f93\u5165\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u5bf9\u4e34\u754c\u6027\u7279\u5f81\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u5f00\u53d1\u4e00\u4e2a\u6846\u67b6\u6765\u533a\u5206\u771f\u5b9e\u4e34\u754c\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\u975e\u4e34\u754c\u7cfb\u7edf\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4f1a\u8868\u73b0\u51fa\u4e34\u754c\u6027\u7279\u5f81\uff0c\u800c\u9759\u606f\u6001\u5927\u8111\u6d3b\u52a8\u5728\u7fa4\u4f53\u6c34\u5e73\u4e0a\u63a5\u8fd1\u4e34\u754c\u72b6\u6001\u3002", "conclusion": "\u7ed3\u8bba\u662f\u5927\u8111\u6d3b\u52a8\u63a5\u8fd1\u4e34\u754c\u72b6\u6001\uff0c\u4e14\u8fd9\u79cd\u72b6\u6001\u53ef\u80fd\u7531\u7f51\u7edc\u6d3b\u52a8\u9a71\u52a8\uff0c\u5bf9\u4fe1\u606f\u5904\u7406\u548c\u4eba\u5de5\u667a\u80fd\u6709\u6f5c\u5728\u610f\u4e49\u3002"}}
{"id": "2506.03423", "pdf": "https://arxiv.org/pdf/2506.03423", "abs": "https://arxiv.org/abs/2506.03423", "authors": ["Timothy B Mahoney", "David B Grayden", "Sam E John"], "title": "Sub-Scalp EEG for Sensorimotor Brain-Computer Interface", "categories": ["eess.SP", "q-bio.NC"], "comment": "43 Pages, 9 Figures, 3 Tables", "summary": "Objective: To establish sub-scalp electroencephalography (EEG) as a viable\noption for brain-computer interface (BCI) applications, particularly for\nchronic use, by demonstrating its effectiveness in recording and classifying\nsensorimotor neural activity. Approach: Two experiments were conducted in this\nstudy. The first aim was to demonstrate the high spatial resolution of\nsub-scalp EEG through analysis of somatosensory evoked potentials in sheep\nmodels. The second focused on the practical application of sub-scalp EEG,\nclassifying motor execution using data collected during a sheep behavioural\nexperiment. Main Results: We successfully demonstrated the recording of\nsensorimotor rhythms using sub-scalp EEG in sheep models. Important spatial,\ntemporal, and spectral features of these signals were identified, and we were\nable to classify motor execution with above-chance performance. These results\nare comparable to previous work that investigated signal quality and motor\nexecution classification using ECoG and endovascular arrays in sheep models.\nSignificance: These results suggest that sub-scalp EEG may provide signal\nquality that approaches that of more invasive neural recording methods such as\nECoG and endovascular arrays, and support the use of sub-scalp EEG for chronic\nBCI applications.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u660e\u76ae\u4e0b\u8111\u7535\u56fe\uff08EEG\uff09\u5728\u8bb0\u5f55\u548c\u5206\u7c7b\u611f\u89c9\u8fd0\u52a8\u795e\u7ecf\u6d3b\u52a8\u65b9\u9762\u6709\u6548\uff0c\u652f\u6301\u5176\u4f5c\u4e3a\u6162\u6027\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u5e94\u7528\u7684\u53ef\u884c\u9009\u62e9\u3002", "motivation": "\u63a2\u8ba8\u76ae\u4e0bEEG\u5728BCI\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u957f\u671f\u4f7f\u7528\u573a\u666f\uff0c\u4ee5\u66ff\u4ee3\u66f4\u5177\u4fb5\u5165\u6027\u7684\u8bb0\u5f55\u65b9\u6cd5\uff08\u5982ECoG\u548c\u8840\u7ba1\u5185\u9635\u5217\uff09\u3002", "method": "\u901a\u8fc7\u4e24\u9879\u5b9e\u9a8c\uff1a1\uff09\u5728\u7f8a\u6a21\u578b\u4e2d\u5206\u6790\u4f53\u611f\u8bf1\u53d1\u7535\u4f4d\uff0c\u9a8c\u8bc1\u76ae\u4e0bEEG\u7684\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\uff1b2\uff09\u5728\u884c\u4e3a\u5b9e\u9a8c\u4e2d\u5206\u7c7b\u8fd0\u52a8\u6267\u884c\u6570\u636e\u3002", "result": "\u6210\u529f\u8bb0\u5f55\u7f8a\u6a21\u578b\u7684\u611f\u89c9\u8fd0\u52a8\u8282\u5f8b\uff0c\u8bc6\u522b\u4fe1\u53f7\u7684\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u9891\u8c31\u7279\u5f81\uff0c\u8fd0\u52a8\u6267\u884c\u5206\u7c7b\u6027\u80fd\u4f18\u4e8e\u968f\u673a\u6c34\u5e73\uff0c\u7ed3\u679c\u4e0eECoG\u548c\u8840\u7ba1\u5185\u9635\u5217\u76f8\u5f53\u3002", "conclusion": "\u76ae\u4e0bEEG\u7684\u4fe1\u53f7\u8d28\u91cf\u63a5\u8fd1\u4fb5\u5165\u6027\u65b9\u6cd5\uff0c\u9002\u5408\u6162\u6027BCI\u5e94\u7528\u3002"}}
{"id": "2506.03832", "pdf": "https://arxiv.org/pdf/2506.03832", "abs": "https://arxiv.org/abs/2506.03832", "authors": ["Omer Moussa", "Mariya Toneva"], "title": "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Proceedings of Interspeech 2025", "summary": "Pretrained self-supervised speech models excel in speech tasks but do not\nreflect the hierarchy of human speech processing, as they encode rich semantics\nin middle layers and poor semantics in late layers. Recent work showed that\nbrain-tuning (fine-tuning models using human brain recordings) improves speech\nmodels' semantic understanding. Here, we examine how well brain-tuned models\nfurther reflect the brain's intermediate stages of speech processing. We find\nthat late layers of brain-tuned models substantially improve over pretrained\nmodels in their alignment with semantic language regions. Further layer-wise\nprobing reveals that early layers remain dedicated to low-level acoustic\nfeatures, while late layers become the best at complex high-level tasks. These\nfindings show that brain-tuned models not only perform better but also exhibit\na well-defined hierarchical processing going from acoustic to semantic\nrepresentations, making them better model organisms for human speech\nprocessing.", "AI": {"tldr": "\u8111\u8c03\u4f18\u7684\u8bed\u97f3\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u4e0a\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e14\u80fd\u66f4\u597d\u5730\u53cd\u6620\u4eba\u8111\u8bed\u97f3\u5904\u7406\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "motivation": "\u7814\u7a76\u8111\u8c03\u4f18\u6a21\u578b\u662f\u5426\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u4eba\u8111\u8bed\u97f3\u5904\u7406\u7684\u4e2d\u95f4\u9636\u6bb5\u3002", "method": "\u901a\u8fc7\u8111\u8c03\u4f18\uff08\u4f7f\u7528\u4eba\u8111\u8bb0\u5f55\u5fae\u8c03\u6a21\u578b\uff09\u6539\u8fdb\u8bed\u97f3\u6a21\u578b\uff0c\u5e76\u5206\u6790\u5176\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u8111\u8c03\u4f18\u6a21\u578b\u7684\u540e\u671f\u5c42\u5728\u8bed\u4e49\u533a\u57df\u5bf9\u9f50\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5c42\u6b21\u7ed3\u6784\u66f4\u6e05\u6670\uff08\u65e9\u671f\u5c42\u5904\u7406\u58f0\u5b66\u7279\u5f81\uff0c\u540e\u671f\u5c42\u5904\u7406\u9ad8\u7ea7\u8bed\u4e49\u4efb\u52a1\uff09\u3002", "conclusion": "\u8111\u8c03\u4f18\u6a21\u578b\u4e0d\u4ec5\u6027\u80fd\u66f4\u597d\uff0c\u8fd8\u80fd\u66f4\u51c6\u786e\u5730\u6a21\u62df\u4eba\u8111\u8bed\u97f3\u5904\u7406\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u6210\u4e3a\u66f4\u597d\u7684\u7814\u7a76\u5de5\u5177\u3002"}}
{"id": "2506.03959", "pdf": "https://arxiv.org/pdf/2506.03959", "abs": "https://arxiv.org/abs/2506.03959", "authors": ["Jacob de Nobel", "Jeroen J. Briaire", "Thomas H. W. Baeck", "Anna V. Kononova", "Johan H. M. Frijns"], "title": "From Spikes to Speech: NeuroVoc -- A Biologically Plausible Vocoder Framework for Auditory Perception and Cochlear Implant Simulation", "categories": ["cs.SD", "q-bio.NC"], "comment": "43 Pages, 11 Figures, 2 Tables", "summary": "We present NeuroVoc, a flexible model-agnostic vocoder framework that\nreconstructs acoustic waveforms from simulated neural activity patterns using\nan inverse Fourier transform. The system applies straightforward signal\nprocessing to neurogram representations, time-frequency binned outputs from\nauditory nerve fiber models. Crucially, the model architecture is modular,\nallowing for easy substitution or modification of the underlying auditory\nmodels. This flexibility eliminates the need for\nspeech-coding-strategy-specific vocoder implementations when simulating\nauditory perception in cochlear implant (CI) users. It also allows direct\ncomparisons between normal hearing (NH) and electrical hearing (EH) models, as\ndemonstrated in this study. The vocoder preserves distinctive features of each\nmodel; for example, the NH model retains harmonic structure more faithfully\nthan the EH model. We evaluated perceptual intelligibility in noise using an\nonline Digits-in-Noise (DIN) test, where participants completed three test\nconditions: one with standard speech, and two with vocoded speech using the NH\nand EH models. Both the standard DIN test and the EH-vocoded groups were\nstatistically equivalent to clinically reported data for NH and CI listeners.\nOn average, the NH and EH vocoded groups increased SRT compared to the standard\ntest by 2.4 dB and 7.1 dB, respectively. These findings show that, although\nsome degradation occurs, the vocoder can reconstruct intelligible speech under\nboth hearing models and accurately reflects the reduced speech-in-noise\nperformance experienced by CI users.", "AI": {"tldr": "NeuroVoc\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u3001\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u58f0\u7801\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u5085\u91cc\u53f6\u53d8\u6362\u4ece\u6a21\u62df\u7684\u795e\u7ecf\u6d3b\u52a8\u6a21\u5f0f\u91cd\u5efa\u58f0\u6ce2\u3002\u5b83\u652f\u6301\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4fbf\u4e8e\u6bd4\u8f83\u6b63\u5e38\u542c\u529b\u4e0e\u7535\u542c\u529b\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u566a\u58f0\u4e2d\u7684\u8bed\u97f3\u53ef\u61c2\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u58f0\u7801\u5668\u5728\u6a21\u62df\u4eba\u5de5\u8033\u8717\u7528\u6237\u542c\u89c9\u611f\u77e5\u65f6\u9700\u9488\u5bf9\u7279\u5b9a\u8bed\u97f3\u7f16\u7801\u7b56\u7565\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u4e14\u7075\u6d3b\u7684\u6846\u67b6\u3002", "method": "\u5229\u7528\u9006\u5085\u91cc\u53f6\u53d8\u6362\u5904\u7406\u795e\u7ecf\u56fe\u8c31\u8868\u793a\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u5141\u8bb8\u66ff\u6362\u6216\u4fee\u6539\u5e95\u5c42\u542c\u89c9\u6a21\u578b\u3002", "result": "NH\u548cEH\u6a21\u578b\u7684\u58f0\u7801\u5668\u5728\u566a\u58f0\u6d4b\u8bd5\u4e2d\u5206\u522b\u589e\u52a0\u4e862.4 dB\u548c7.1 dB\u7684\u4fe1\u566a\u6bd4\u9608\u503c\uff0c\u4e0e\u4e34\u5e8a\u6570\u636e\u4e00\u81f4\u3002", "conclusion": "NeuroVoc\u80fd\u6709\u6548\u91cd\u5efa\u53ef\u61c2\u8bed\u97f3\uff0c\u51c6\u786e\u53cd\u6620\u4eba\u5de5\u8033\u8717\u7528\u6237\u5728\u566a\u58f0\u4e2d\u7684\u8bed\u97f3\u611f\u77e5\u6027\u80fd\u4e0b\u964d\u3002"}}
{"id": "2506.03214", "pdf": "https://arxiv.org/pdf/2506.03214", "abs": "https://arxiv.org/abs/2506.03214", "authors": ["Yi Guo", "Yihang Dong", "Michael Kwok-Po Ng", "Shuqiang Wang"], "title": "A Pre-trained Framework for Multilingual Brain Decoding Using Non-invasive Recordings", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Brain-computer interfaces (BCIs) with speech decoding from brain recordings\nhave broad application potential in fields such as clinical rehabilitation and\ncognitive neuroscience. However, current decoding methods remain limited to\nsingle-language, single-subject, and single neuroimaging modality settings,\nrestricting their clinical applicability and generalizability. Here we propose\na joint multilingual, multi-subject and multimodal decoding framework. It maps\ndiverse brain recordings into a unified semantic space defined by a pre-trained\nmultilingual model (PMM), enabling decoding across multiple languages, multiple\nsubjects and multiple neuroimaging modalities. The proposed framework is\nvalidated using non-invasive brain recordings from 159 participants across four\nlanguages. Experimental results show that it exhibits strong generalization\nacross multilingual, multi-subject, and multimodal settings. More importantly,\nthe proposed framework can promote linguistic fairness, which is vital for\nunderrepresented languages in BCI applications. The unified semantic space\nenables cross-lingual mapping enhancement, allowing the framework to boost the\ndecoding performance of underrepresented languages, thereby promoting\nlinguistic fairness. Overall, the proposed framework establishes a new\npotential paradigm for brain decoding, opening new paths for broader\napplications of BCI.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u591a\u8bed\u8a00\u3001\u591a\u88ab\u8bd5\u3001\u591a\u6a21\u6001\u7684\u8054\u5408\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u63d0\u5347\u8111\u673a\u63a5\u53e3\u7684\u6cdb\u5316\u80fd\u529b\u548c\u8bed\u8a00\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709\u8111\u673a\u63a5\u53e3\u89e3\u7801\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u8bed\u8a00\u3001\u5355\u88ab\u8bd5\u548c\u5355\u6a21\u6001\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5c06\u591a\u6837\u5316\u8111\u8bb0\u5f55\u6620\u5c04\u5230\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\u5b9a\u4e49\u7684\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u652f\u6301\u8de8\u8bed\u8a00\u3001\u8de8\u88ab\u8bd5\u548c\u8de8\u6a21\u6001\u89e3\u7801\u3002", "result": "\u5728159\u540d\u88ab\u8bd5\u7684\u56db\u79cd\u8bed\u8a00\u975e\u4fb5\u5165\u6027\u8111\u8bb0\u5f55\u4e0a\u9a8c\u8bc1\uff0c\u6846\u67b6\u5728\u591a\u8bed\u8a00\u3001\u591a\u88ab\u8bd5\u548c\u591a\u6a21\u6001\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u5347\u5f31\u52bf\u8bed\u8a00\u7684\u89e3\u7801\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8111\u89e3\u7801\u5f00\u8f9f\u4e86\u65b0\u8303\u5f0f\uff0c\u62d3\u5bbd\u4e86\u8111\u673a\u63a5\u53e3\u7684\u5e94\u7528\u8303\u56f4\uff0c\u5e76\u4fc3\u8fdb\u8bed\u8a00\u516c\u5e73\u6027\u3002"}}
{"id": "2506.03293", "pdf": "https://arxiv.org/pdf/2506.03293", "abs": "https://arxiv.org/abs/2506.03293", "authors": ["Nina S. Nellen", "Polina Turishcheva", "Michaela Vystr\u010dilov\u00e1", "Shashwat Sridhar", "Tim Gollisch", "Andreas S. Tolias", "Alexander S. Ecker"], "title": "Learning to cluster neuronal function", "categories": ["q-bio.NC"], "comment": null, "summary": "Deep neural networks trained to predict neural activity from visual input and\nbehaviour have shown great potential to serve as digital twins of the visual\ncortex. Per-neuron embeddings derived from these models could potentially be\nused to map the functional landscape or identify cell types. However,\nstate-of-the-art predictive models of mouse V1 do not generate functional\nembeddings that exhibit clear clustering patterns which would correspond to\ncell types. This raises the question whether the lack of clustered structure is\ndue to limitations of current models or a true feature of the functional\norganization of mouse V1. In this work, we introduce DECEMber -- Deep Embedding\nClustering via Expectation Maximization-based refinement -- an explicit\ninductive bias into predictive models that enhances clustering by adding an\nauxiliary $t$-distribution-inspired loss function that enforces structured\norganization among per-neuron embeddings. We jointly optimize both neuronal\nfeature embeddings and clustering parameters, updating cluster centers and\nscale matrices using the EM-algorithm. We demonstrate that these modifications\nimprove cluster consistency while preserving high predictive performance and\nsurpassing standard clustering methods in terms of stability. Moreover,\nDECEMber generalizes well across species (mice, primates) and visual areas\n(retina, V1, V4). The code is available at\nhttps://github.com/Nisone2000/sensorium/tree/neuroips_version.", "AI": {"tldr": "DECEMber\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u635f\u5931\u51fd\u6570\u548cEM\u7b97\u6cd5\u4f18\u5316\u795e\u7ecf\u5143\u5d4c\u5165\u548c\u805a\u7c7b\u53c2\u6570\uff0c\u63d0\u5347\u4e86\u805a\u7c7b\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u751f\u6210\u7684\u795e\u7ecf\u5143\u5d4c\u5165\u662f\u5426\u7f3a\u4e4f\u805a\u7c7b\u7ed3\u6784\u662f\u7531\u4e8e\u6a21\u578b\u9650\u5236\u8fd8\u662f\u5c0f\u9f20V1\u529f\u80fd\u7ec4\u7ec7\u7684\u771f\u5b9e\u7279\u5f81\u3002", "method": "\u63d0\u51faDECEMber\u65b9\u6cd5\uff0c\u7ed3\u5408t\u5206\u5e03\u542f\u53d1\u7684\u635f\u5931\u51fd\u6570\u548cEM\u7b97\u6cd5\uff0c\u4f18\u5316\u795e\u7ecf\u5143\u5d4c\u5165\u548c\u805a\u7c7b\u53c2\u6570\u3002", "result": "DECEMber\u63d0\u9ad8\u4e86\u805a\u7c7b\u4e00\u81f4\u6027\uff0c\u4fdd\u6301\u4e86\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u6807\u51c6\u805a\u7c7b\u65b9\u6cd5\uff0c\u4e14\u9002\u7528\u4e8e\u591a\u79cd\u7269\u79cd\u548c\u89c6\u89c9\u533a\u57df\u3002", "conclusion": "DECEMber\u4e3a\u795e\u7ecf\u5143\u529f\u80fd\u5d4c\u5165\u7684\u805a\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u8de8\u7269\u79cd\u548c\u533a\u57df\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2506.03640", "pdf": "https://arxiv.org/pdf/2506.03640", "abs": "https://arxiv.org/abs/2506.03640", "authors": ["Rub\u00e9n Calvo", "Carles Martorell", "Adri\u00e1n Roig", "Miguel A. Mu\u00f1oz"], "title": "Robust Scaling in Human Brain Dynamics Despite Latent Variables and Limited Sampling Distortions", "categories": ["q-bio.NC", "cond-mat.dis-nn", "cond-mat.stat-mech"], "comment": null, "summary": "The idea that information-processing systems operate near criticality to\nenhance computational performance is supported by scaling signatures in brain\nactivity. However, external signals raise the question of whether this behavior\nis intrinsic or input-driven. We show that autocorrelated inputs and temporal\nresolution influence observed scaling exponents in simple neural models. We\nalso demonstrate analytically that under subsampling, non-critical systems\ndriven by independent autocorrelated signals can exhibit strong signatures of\napparent criticality. To address these pitfalls, we develop a robust framework\nand apply it to pooled neural data, revealing resting-state brain activity at\nthe population level is slightly sub-critical yet near-critical. Notably, the\nextracted critical exponents closely match predictions from a simple recurrent\nfiring-rate model, supporting the emergence of near-critical dynamics from\nreverberant network activity, with potential implications for information\nprocessing and artificial intelligence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4fe1\u606f\u5904\u7406\u7cfb\u7edf\u5728\u4e34\u754c\u70b9\u9644\u8fd1\u8fd0\u884c\u4ee5\u63d0\u5347\u8ba1\u7b97\u6027\u80fd\u7684\u89c2\u70b9\uff0c\u5e76\u5206\u6790\u4e86\u5916\u90e8\u4fe1\u53f7\u5bf9\u89c2\u5bdf\u5230\u7684\u4e34\u754c\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u533a\u5206\u5927\u8111\u6d3b\u52a8\u4e2d\u7684\u4e34\u754c\u884c\u4e3a\u662f\u5185\u5728\u7684\u8fd8\u662f\u7531\u5916\u90e8\u8f93\u5165\u9a71\u52a8\u7684\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u6f5c\u5728\u8bef\u533a\u3002", "method": "\u901a\u8fc7\u5206\u6790\u7b80\u5355\u795e\u7ecf\u6a21\u578b\u4e2d\u7684\u81ea\u76f8\u5173\u8f93\u5165\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u5bf9\u4e34\u754c\u6307\u6570\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e00\u4e2a\u9c81\u68d2\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u7fa4\u4f53\u795e\u7ecf\u6570\u636e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u9759\u606f\u72b6\u6001\u4e0b\u7684\u5927\u8111\u6d3b\u52a8\u5728\u7fa4\u4f53\u6c34\u5e73\u4e0a\u63a5\u8fd1\u4e34\u754c\u70b9\u4f46\u7565\u4f4e\u4e8e\u4e34\u754c\u70b9\uff0c\u4e14\u4e34\u754c\u6307\u6570\u4e0e\u7b80\u5355\u5faa\u73af\u53d1\u653e\u7387\u6a21\u578b\u7684\u9884\u6d4b\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u4e86\u5faa\u73af\u7f51\u7edc\u6d3b\u52a8\u4ea7\u751f\u63a5\u8fd1\u4e34\u754c\u52a8\u529b\u5b66\u7684\u89c2\u70b9\uff0c\u5bf9\u4fe1\u606f\u5904\u7406\u548c\u4eba\u5de5\u667a\u80fd\u5177\u6709\u6f5c\u5728\u610f\u4e49\u3002"}}
{"id": "2506.03423", "pdf": "https://arxiv.org/pdf/2506.03423", "abs": "https://arxiv.org/abs/2506.03423", "authors": ["Timothy B Mahoney", "David B Grayden", "Sam E John"], "title": "Sub-Scalp EEG for Sensorimotor Brain-Computer Interface", "categories": ["eess.SP", "q-bio.NC"], "comment": "43 Pages, 9 Figures, 3 Tables", "summary": "Objective: To establish sub-scalp electroencephalography (EEG) as a viable\noption for brain-computer interface (BCI) applications, particularly for\nchronic use, by demonstrating its effectiveness in recording and classifying\nsensorimotor neural activity. Approach: Two experiments were conducted in this\nstudy. The first aim was to demonstrate the high spatial resolution of\nsub-scalp EEG through analysis of somatosensory evoked potentials in sheep\nmodels. The second focused on the practical application of sub-scalp EEG,\nclassifying motor execution using data collected during a sheep behavioural\nexperiment. Main Results: We successfully demonstrated the recording of\nsensorimotor rhythms using sub-scalp EEG in sheep models. Important spatial,\ntemporal, and spectral features of these signals were identified, and we were\nable to classify motor execution with above-chance performance. These results\nare comparable to previous work that investigated signal quality and motor\nexecution classification using ECoG and endovascular arrays in sheep models.\nSignificance: These results suggest that sub-scalp EEG may provide signal\nquality that approaches that of more invasive neural recording methods such as\nECoG and endovascular arrays, and support the use of sub-scalp EEG for chronic\nBCI applications.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u660e\u76ae\u4e0b\u8111\u7535\u56fe\uff08EEG\uff09\u5728\u8bb0\u5f55\u548c\u5206\u7c7b\u611f\u89c9\u8fd0\u52a8\u795e\u7ecf\u6d3b\u52a8\u65b9\u9762\u6709\u6548\uff0c\u652f\u6301\u5176\u4f5c\u4e3a\u6162\u6027\u8111\u673a\u63a5\u53e3\uff08BCI\uff09\u5e94\u7528\u7684\u53ef\u884c\u9009\u62e9\u3002", "motivation": "\u9a8c\u8bc1\u76ae\u4e0bEEG\u5728BCI\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u957f\u671f\u4f7f\u7528\uff0c\u4ee5\u66ff\u4ee3\u66f4\u5177\u4fb5\u5165\u6027\u7684\u65b9\u6cd5\u5982ECoG\u548c\u8840\u7ba1\u5185\u9635\u5217\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\uff1a1\uff09\u5728\u7ef5\u7f8a\u6a21\u578b\u4e2d\u5206\u6790\u4f53\u611f\u8bf1\u53d1\u7535\u4f4d\u4ee5\u5c55\u793a\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\uff1b2\uff09\u5728\u884c\u4e3a\u5b9e\u9a8c\u4e2d\u5206\u7c7b\u8fd0\u52a8\u6267\u884c\u6570\u636e\u3002", "result": "\u6210\u529f\u8bb0\u5f55\u611f\u89c9\u8fd0\u52a8\u8282\u5f8b\uff0c\u8bc6\u522b\u4fe1\u53f7\u7279\u5f81\uff0c\u5e76\u4ee5\u9ad8\u4e8e\u968f\u673a\u6c34\u5e73\u7684\u6027\u80fd\u5206\u7c7b\u8fd0\u52a8\u6267\u884c\uff0c\u7ed3\u679c\u4e0eECoG\u548c\u8840\u7ba1\u5185\u9635\u5217\u76f8\u5f53\u3002", "conclusion": "\u76ae\u4e0bEEG\u4fe1\u53f7\u8d28\u91cf\u63a5\u8fd1\u4fb5\u5165\u6027\u65b9\u6cd5\uff0c\u9002\u5408\u6162\u6027BCI\u5e94\u7528\u3002"}}
{"id": "2506.03832", "pdf": "https://arxiv.org/pdf/2506.03832", "abs": "https://arxiv.org/abs/2506.03832", "authors": ["Omer Moussa", "Mariya Toneva"], "title": "Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain", "categories": ["cs.CL", "cs.SD", "eess.AS", "q-bio.NC"], "comment": "Proceedings of Interspeech 2025", "summary": "Pretrained self-supervised speech models excel in speech tasks but do not\nreflect the hierarchy of human speech processing, as they encode rich semantics\nin middle layers and poor semantics in late layers. Recent work showed that\nbrain-tuning (fine-tuning models using human brain recordings) improves speech\nmodels' semantic understanding. Here, we examine how well brain-tuned models\nfurther reflect the brain's intermediate stages of speech processing. We find\nthat late layers of brain-tuned models substantially improve over pretrained\nmodels in their alignment with semantic language regions. Further layer-wise\nprobing reveals that early layers remain dedicated to low-level acoustic\nfeatures, while late layers become the best at complex high-level tasks. These\nfindings show that brain-tuned models not only perform better but also exhibit\na well-defined hierarchical processing going from acoustic to semantic\nrepresentations, making them better model organisms for human speech\nprocessing.", "AI": {"tldr": "\u8111\u8c03\u4f18\u7684\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u5728\u8bed\u4e49\u7406\u89e3\u548c\u5c42\u6b21\u5904\u7406\u4e0a\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u66f4\u63a5\u8fd1\u4eba\u8111\u7684\u8bed\u97f3\u5904\u7406\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u8111\u8c03\u4f18\u6a21\u578b\u662f\u5426\u80fd\u66f4\u597d\u5730\u53cd\u6620\u4eba\u8111\u8bed\u97f3\u5904\u7406\u7684\u4e2d\u95f4\u9636\u6bb5\uff0c\u4ee5\u6539\u8fdb\u8bed\u97f3\u6a21\u578b\u7684\u5c42\u6b21\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u8111\u8c03\u4f18\uff08\u4f7f\u7528\u4eba\u8111\u8bb0\u5f55\u5fae\u8c03\u6a21\u578b\uff09\u6539\u8fdb\u8bed\u97f3\u6a21\u578b\uff0c\u5e76\u5206\u6790\u5176\u5404\u5c42\u4e0e\u8bed\u4e49\u8bed\u8a00\u533a\u57df\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u8111\u8c03\u4f18\u6a21\u578b\u7684\u540e\u671f\u5c42\u663e\u8457\u63d0\u5347\u4e86\u4e0e\u8bed\u4e49\u8bed\u8a00\u533a\u57df\u7684\u5bf9\u9f50\u6027\uff0c\u65e9\u671f\u5c42\u4e13\u6ce8\u4e8e\u4f4e\u5c42\u6b21\u58f0\u5b66\u7279\u5f81\uff0c\u540e\u671f\u5c42\u64c5\u957f\u9ad8\u5c42\u6b21\u4efb\u52a1\u3002", "conclusion": "\u8111\u8c03\u4f18\u6a21\u578b\u4e0d\u4ec5\u6027\u80fd\u66f4\u597d\uff0c\u8fd8\u8868\u73b0\u51fa\u4ece\u58f0\u5b66\u5230\u8bed\u4e49\u7684\u5c42\u6b21\u5904\u7406\uff0c\u66f4\u63a5\u8fd1\u4eba\u8111\u8bed\u97f3\u5904\u7406\u673a\u5236\u3002"}}
{"id": "2506.03959", "pdf": "https://arxiv.org/pdf/2506.03959", "abs": "https://arxiv.org/abs/2506.03959", "authors": ["Jacob de Nobel", "Jeroen J. Briaire", "Thomas H. W. Baeck", "Anna V. Kononova", "Johan H. M. Frijns"], "title": "From Spikes to Speech: NeuroVoc -- A Biologically Plausible Vocoder Framework for Auditory Perception and Cochlear Implant Simulation", "categories": ["cs.SD", "q-bio.NC"], "comment": "43 Pages, 11 Figures, 2 Tables", "summary": "We present NeuroVoc, a flexible model-agnostic vocoder framework that\nreconstructs acoustic waveforms from simulated neural activity patterns using\nan inverse Fourier transform. The system applies straightforward signal\nprocessing to neurogram representations, time-frequency binned outputs from\nauditory nerve fiber models. Crucially, the model architecture is modular,\nallowing for easy substitution or modification of the underlying auditory\nmodels. This flexibility eliminates the need for\nspeech-coding-strategy-specific vocoder implementations when simulating\nauditory perception in cochlear implant (CI) users. It also allows direct\ncomparisons between normal hearing (NH) and electrical hearing (EH) models, as\ndemonstrated in this study. The vocoder preserves distinctive features of each\nmodel; for example, the NH model retains harmonic structure more faithfully\nthan the EH model. We evaluated perceptual intelligibility in noise using an\nonline Digits-in-Noise (DIN) test, where participants completed three test\nconditions: one with standard speech, and two with vocoded speech using the NH\nand EH models. Both the standard DIN test and the EH-vocoded groups were\nstatistically equivalent to clinically reported data for NH and CI listeners.\nOn average, the NH and EH vocoded groups increased SRT compared to the standard\ntest by 2.4 dB and 7.1 dB, respectively. These findings show that, although\nsome degradation occurs, the vocoder can reconstruct intelligible speech under\nboth hearing models and accurately reflects the reduced speech-in-noise\nperformance experienced by CI users.", "AI": {"tldr": "NeuroVoc\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u3001\u6a21\u578b\u65e0\u5173\u7684\u58f0\u7801\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u5085\u91cc\u53f6\u53d8\u6362\u4ece\u6a21\u62df\u795e\u7ecf\u6d3b\u52a8\u6a21\u5f0f\u91cd\u5efa\u58f0\u6ce2\u3002\u5b83\u652f\u6301\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4fbf\u4e8e\u6bd4\u8f83\u6b63\u5e38\u542c\u529b\u4e0e\u7535\u542c\u529b\u6a21\u578b\uff0c\u5e76\u5728\u566a\u58f0\u4e2d\u6d4b\u8bd5\u4e86\u8bed\u97f3\u53ef\u61c2\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u58f0\u7801\u5668\u5728\u6a21\u62df\u4eba\u5de5\u8033\u8717\u7528\u6237\u542c\u89c9\u611f\u77e5\u65f6\u9700\u8981\u7279\u5b9a\u7f16\u7801\u7b56\u7565\u7684\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u76f4\u63a5\u6bd4\u8f83\u6b63\u5e38\u542c\u529b\u4e0e\u7535\u542c\u529b\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u9006\u5085\u91cc\u53f6\u53d8\u6362\u5904\u7406\u795e\u7ecf\u56fe\u8c31\u8868\u793a\uff0c\u6a21\u5757\u5316\u8bbe\u8ba1\u5141\u8bb8\u66ff\u6362\u6216\u4fee\u6539\u5e95\u5c42\u542c\u89c9\u6a21\u578b\u3002\u901a\u8fc7\u5728\u7ebf\u566a\u58f0\u6570\u5b57\u6d4b\u8bd5\u8bc4\u4f30\u53ef\u61c2\u5ea6\u3002", "result": "NH\u548cEH\u6a21\u578b\u7684\u58f0\u7801\u5668\u5728\u566a\u58f0\u4e2d\u5206\u522b\u589e\u52a0\u4e862.4 dB\u548c7.1 dB\u7684SRT\uff0c\u4e0e\u4e34\u5e8a\u6570\u636e\u4e00\u81f4\uff0c\u8868\u660e\u5176\u80fd\u51c6\u786e\u53cd\u6620CI\u7528\u6237\u7684\u8bed\u97f3\u611f\u77e5\u9000\u5316\u3002", "conclusion": "NeuroVoc\u80fd\u591f\u91cd\u5efa\u53ef\u61c2\u8bed\u97f3\uff0c\u5e76\u51c6\u786e\u6a21\u62dfCI\u7528\u6237\u5728\u566a\u58f0\u4e2d\u7684\u8bed\u97f3\u611f\u77e5\u6027\u80fd\u4e0b\u964d\u3002"}}
