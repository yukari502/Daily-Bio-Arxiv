{"id": "2506.04235", "pdf": "https://arxiv.org/pdf/2506.04235", "abs": "https://arxiv.org/abs/2506.04235", "authors": ["Xinyan Zhao", "Yi-Ching Tang", "Akshita Singh", "Victor J Cantu", "KwanHo An", "Junseok Lee", "Adam E Stogsdill", "Ashwin Kumar Ramesh", "Zhiqiang An", "Xiaoqian Jiang", "Yejin Kim"], "title": "Benchmark for Antibody Binding Affinity Maturation and Design", "categories": ["q-bio.QM", "cs.AI", "cs.CE", "cs.LG", "q-bio.BM"], "comment": null, "summary": "We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking\nframework for antibody binding affinity maturation and design. Unlike existing\nantibody evaluation strategies that rely on antibody alone and its similarity\nto natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench\nconsiders an antibody-antigen (Ab-Ag) complex as a functional unit and\nevaluates the potential of an antibody design binding to given antigen by\nmeasuring protein model's likelihood on the Ab-Ag complex. We first curate,\nstandardize, and share 9 datasets containing 9 antigens (involving influenza,\nanti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain\nmutated antibodies. Using these datasets, we systematically compare 14 protein\nmodels including masked language models, autoregressive language models,\ninverse folding models, diffusion-based generative models, and geometric graph\nmodels. The correlation between model likelihood and experimental affinity\nvalues is used to evaluate model performance. Additionally, in a case study to\nincrease binding affinity of antibody F045-092 to antigen influenza H1N1, we\nevaluate the generative power of the top-performing models by sampling a set of\nnew antibodies binding to the antigen and ranking them based on structural\nintegrity and biophysical properties of the Ab-Ag complex. As a result,\nstructure-conditioned inverse folding models outperform others in both affinity\ncorrelation and generation tasks. Overall, AbBiBench provides a unified,\nbiologically grounded evaluation framework to facilitate the development of\nmore effective, function-aware antibody design models."}
{"id": "2506.04247", "pdf": "https://arxiv.org/pdf/2506.04247", "abs": "https://arxiv.org/abs/2506.04247", "authors": ["Gage K. R. Hooper"], "title": "The GAIN Model: A Nature-Inspired Neural Network Framework Based on an Adaptation of the Izhikevich Model", "categories": ["q-bio.NC", "cs.AI", "cs.NE", "92B20, 37N25, 68T05", "I.2.6; I.5.1; I.6.3"], "comment": "31 pages, 16 figures", "summary": "While many neural networks focus on layers to process information, the GAIN\nmodel uses a grid-based structure to improve biological plausibility and the\ndynamics of the model. The grid structure helps neurons to interact with their\nclosest neighbors and improve their connections with one another, which is seen\nin biological neurons. While also being implemented with the Izhikevich model\nthis approach allows for a computationally efficient and biologically accurate\nsimulation that can aid in the development of neural networks, large scale\nsimulations, and the development in the neuroscience field. This adaptation of\nthe Izhikevich model can improve the dynamics and accuracy of the model,\nallowing for its uses to be specialized but efficient."}
{"id": "2506.04284", "pdf": "https://arxiv.org/pdf/2506.04284", "abs": "https://arxiv.org/abs/2506.04284", "authors": ["Diepreye Ayabina", "Hasan Sevil", "Adam Kleczkowski", "M. Gabriela M. Gomes"], "title": "A note on metapopulation models", "categories": ["q-bio.PE"], "comment": "23 pages, 11 figures", "summary": "Metapopulation models are commonly used in ecology, evolution, and\nepidemiology. These models usually entail homogeneity assumptions within\npatches and study networks of migration between patches to generate insights\ninto conservation of species, differentiation of populations, and persistence\nof infectious diseases. Here, focusing on infectious disease epidemiology, we\ntake a complementary approach and study the effects of individual variation\nwithin patches while neglecting any form of disease transmission between\npatches. Consistently with previous work on single populations, we show how\nmetapopulation models that neglect in-patch heterogeneity also underestimate\nbasic reproduction numbers ($\\mathcal{R}_{0}$) and the effort required to\ncontrol or eliminate infectious diseases by uniform interventions. We then go\nbeyond this confirmatory result and introduce a scheme to infer distributions\nof individual susceptibility or exposure to infection based on suitable\nstratifications of a population into patches. We apply the resulting\nmetapopulation models to a simple case study of the COVID-19 pandemic."}
{"id": "2506.04303", "pdf": "https://arxiv.org/pdf/2506.04303", "abs": "https://arxiv.org/abs/2506.04303", "authors": ["Zhizheng Wang", "Chi-Ping Day", "Chih-Hsuan Wei", "Qiao Jin", "Robert Leaman", "Yifan Yang", "Shubo Tian", "Aodong Qiu", "Yin Fang", "Qingqing Zhu", "Xinghua Lu", "Zhiyong Lu"], "title": "Knowledge-guided Contextual Gene Set Analysis Using Large Language Models", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "comment": "56 pages, 9 figures, 1 table", "summary": "Gene set analysis (GSA) is a foundational approach for interpreting genomic\ndata of diseases by linking genes to biological processes. However,\nconventional GSA methods overlook clinical context of the analyses, often\ngenerating long lists of enriched pathways with redundant, nonspecific, or\nirrelevant results. Interpreting these requires extensive, ad-hoc manual\neffort, reducing both reliability and reproducibility. To address this\nlimitation, we introduce cGSA, a novel AI-driven framework that enhances GSA by\nincorporating context-aware pathway prioritization. cGSA integrates gene\ncluster detection, enrichment analysis, and large language models to identify\npathways that are not only statistically significant but also biologically\nmeaningful. Benchmarking on 102 manually curated gene sets across 19 diseases\nand ten disease-related biological mechanisms shows that cGSA outperforms\nbaseline methods by over 30%, with expert validation confirming its increased\nprecision and interpretability. Two independent case studies in melanoma and\nbreast cancer further demonstrate its potential to uncover context-specific\ninsights and support targeted hypothesis generation."}
{"id": "2506.04511", "pdf": "https://arxiv.org/pdf/2506.04511", "abs": "https://arxiv.org/abs/2506.04511", "authors": ["Mario E. Lozano", "Marcela G. Pilloff"], "title": "Viral Hitchhikers and Macroevolution: A Novel Hypothesis on Explosive Speciation", "categories": ["q-bio.PE"], "comment": "19 pages, 1 Table, 2 figures. Includes 3 boxes: Highlights,\n  Outstanding Questions box, Glossary", "summary": "Mobile genetic elements (e.g., endogenous viruses, LINEs, SINEs) can transfer\nbetween genomes, even between species, triggering dramatic genetic changes.\nEndogenous viral elements (EVEs) arise when infectious viruses integrate into\nthe host germline. EVEs integrate at specific sites; their genes or regulatory\nregions can be exapted and could induce chromosomal rearrangement. We propose\nthat EVEs participate in adaptive radiations and that their parent viruses,\nthrough interspecific transfer, could initiate new species formation. By\nsynchronously affecting multiple individuals, viral outbreaks generate shared\ngenomic changes that both facilitate reproductive isolation and provide the\nsimultaneous modifications needed for groups to emerge as founders of new\nspecies. We suggest horizontal viral transfer during the K-Pg accelerated\nmammalian radiation linking viral epidemics to macroevolutionary\ndiversification. This theoretical work proposes endogenous viruses as catalysts\nfor explosive speciation."}
{"id": "2506.04515", "pdf": "https://arxiv.org/pdf/2506.04515", "abs": "https://arxiv.org/abs/2506.04515", "authors": ["Salil Patel"], "title": "The Latent Space Hypothesis: Toward Universal Medical Representation Learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "51 pages, 12 figures. A position paper examining the latent space\n  hypothesis - the proposition that diverse medical data can be represented in\n  shared latent spaces reflecting fundamental biological processes. The paper\n  discusses theoretical foundations, reviews supporting evidence, and considers\n  potential implications for medical AI and representation learning", "summary": "Medical data range from genomic sequences and retinal photographs to\nstructured laboratory results and unstructured clinical narratives. Although\nthese modalities appear disparate, many encode convergent information about a\nsingle underlying physiological state. The Latent Space Hypothesis frames each\nobservation as a projection of a unified, hierarchically organized manifold --\nmuch like shadows cast by the same three-dimensional object. Within this\nlearned geometric representation, an individual's health status occupies a\npoint, disease progression traces a trajectory, and therapeutic intervention\ncorresponds to a directed vector. Interpreting heterogeneous evidence in a\nshared space provides a principled way to re-examine eponymous conditions --\nsuch as Parkinson's or Crohn's -- that often mask multiple pathophysiological\nentities and involve broader anatomical domains than once believed. By\nrevealing sub-trajectories and patient-specific directions of change, the\nframework supplies a quantitative rationale for personalised diagnosis,\nlongitudinal monitoring, and tailored treatment, moving clinical practice away\nfrom grouping by potentially misleading labels toward navigation of each\nperson's unique trajectory. Challenges remain -- bias amplification, data\nscarcity for rare disorders, privacy, and the correlation-causation divide --\nbut scale-aware encoders, continual learning on longitudinal data streams, and\nperturbation-based validation offer plausible paths forward."}
{"id": "2506.04538", "pdf": "https://arxiv.org/pdf/2506.04538", "abs": "https://arxiv.org/abs/2506.04538", "authors": ["Mitchel J. Colebank"], "title": "Assessing parameter identifiability of a hemodynamics PDE model using spectral surrogates and dimension reduction", "categories": ["q-bio.TO", "stat.AP", "stat.CO"], "comment": "30 pages, 12 figures", "summary": "Computational inverse problems for biomedical simulators suffer from limited\ndata and relatively high parameter dimensionality. This often requires\nsensitivity analysis, where parameters of the model are ranked based on their\ninfluence on the specific quantities of interest. This is especially important\nfor simulators used to build medical digital twins, as the amount of data is\ntypically limited. For expensive models, such as blood flow models, emulation\nis employed to expedite the simulation time. Parameter ranking and fixing using\nsensitivity analysis are often heuristic, though, and vary with the specific\napplication or simulator used. The present study provides an innovative\nsolution to this problem by leveraging polynomial chaos expansions (PCEs) for\nboth multioutput global sensitivity analysis and formal parameter\nidentifiability. For the former, we use dimension reduction to efficiently\nquantify time-series sensitivity of a one-dimensional pulmonary hemodynamics\nmodel. We consider both Windkessel and structured tree boundary conditions. We\nthen use PCEs to construct profile-likelihood confidence intervals to formally\nassess parameter identifiability, and show how changes in experimental design\nimprove identifiability. Our work presents a novel approach to determining\nparameter identifiability and leverages a common emulation strategy for\nenabling profile-likelihood analysis in problems governed by partial\ndifferential equations."}
{"id": "2506.04549", "pdf": "https://arxiv.org/pdf/2506.04549", "abs": "https://arxiv.org/abs/2506.04549", "authors": ["Vardhan Palod", "Pranav Mahajan", "Veeky Baths", "Boris S. Gutkin"], "title": "Discounting and Drug Seeking in Biological Hierarchical Reinforcement Learning", "categories": ["q-bio.NC"], "comment": "Accepted in the Proceedings of Cognitive Computational Neuroscience\n  (CCN) 2025", "summary": "Despite a strong desire to quit, individuals with long-term substance use\ndisorder (SUD) often struggle to resist drug use, even when aware of its\nharmful consequences. This disconnect between knowledge and compulsive behavior\nreflects a fundamental cognitive-behavioral conflict in addiction.\nNeurobiologically, differential cue-induced activity within striatal\nsubregions, along with dopamine-mediated connectivity from the ventral to the\ndorsal striatum, contributes to compulsive drug-seeking. However, the\nfunctional mechanism linking these findings to behavioral conflict remains\nunclear. Another hallmark of addiction is temporal discounting: individuals\nwith drug dependence exhibit steeper discount rates than non-users. Assuming\nthe ventral-dorsal striatal organization reflects a gradient from cognitive to\nmotor representations, addiction can be modeled within a hierarchical\nreinforcement learning (HRL) framework. However, integrating discounting into\nbiologically grounded HRL remains an open challenge. In this work, we build on\na model showing how action choices reinforced with drug rewards become\ninsensitive to the negative consequences that follow. We address the\nintegration of discounting by ensuring natural reward values converge across\nall levels in the HRL hierarchy, while drug rewards diverge due to their\ndopaminergic effects. Our results show that high discounting amplifies\ndrug-seeking across the hierarchy, linking faster discounting with increased\naddiction severity and impulsivity. We demonstrate alignment with empirical\nfindings on temporal discounting and propose testable predictions, establishing\naddiction as a disorder of hierarchical decision-making."}
{"id": "2506.04702", "pdf": "https://arxiv.org/pdf/2506.04702", "abs": "https://arxiv.org/abs/2506.04702", "authors": ["Shobhan Dev Mandal", "Sakuntala Chatterjee"], "title": "Bacterial Chemotaxis in a Traveling Wave Attractant Environment", "categories": ["q-bio.CB", "cond-mat.stat-mech", "physics.bio-ph"], "comment": null, "summary": "We study single cell E.coli chemotaxis in a spatio-temporally varying\nattractant environment. Modeling the attractant concentration in the form of a\ntraveling sine wave, we measure in our simulations, the chemotactic drift\nvelocity of the cell for different propagation speed of the attractant wave. We\nfind a highly non-trivial dependence where the chemotactic drift velocity\nchanges sign, and also shows multiple peaks. For slowly moving attractant wave,\ndrift velocity is negative, i.e. the drift motion is directed opposite to wave\npropagation. As the wave speed increases, drift velocity shows a negative peak,\nthen changes sign, reaches a positive peak and finally becomes zero when the\nwave moves too fast for the cell to respond. We explain this rich behavior from\nthe difference in attractant gradient perceived by the cell during its run\nalong the propagation direction and opposite to it. In particular, when the\ncell moves in the same direction as the wave, the relative velocity of the cell\nwith respect to the wave becomes zero when the wave speed matches the run\nspeed. In this limit, the cell is able to ride the wave and experiences no\nconcentration gradient during these runs. On the contrary, for runs in the\nopposite direction, no such effect is present and the effective gradient\nincreases monotonically with the wave speed. We show, using detailed\nquantitative measurements, how this difference gives rise to the\ncounter-intuitive behavior of chemotactic drift velocity described above."}
{"id": "2506.04875", "pdf": "https://arxiv.org/pdf/2506.04875", "abs": "https://arxiv.org/abs/2506.04875", "authors": ["Alexander Gorsky"], "title": "Consciousness via MIPT?", "categories": ["q-bio.NC"], "comment": "24 pages", "summary": "The measurement-induced phase transition (MIPT) is a recently formulated\nphenomenon in out-of-equilibrium systems. The competition between unitary\nevolutions and measurement-induced non-unitaries leads to the transition\nbetween the entangled and disentangled phases at some critical measurement\nrate. We conjecture that self-organized MIPT plays a key role in the generative\nmodel of cognitive networks and the formation of the state of consciousness in\nthe \"newborn-adult\" transition. To this aim, we formulate the probe-target\npicture for the brain and suggest that MIPT interpreted as learnability\ntransition takes place in the mental part of the target where the sites in the\ncognitive networks of semantic memory are concepts. Comparison with the\nsynchronization phase transitions in the probe is made."}
{"id": "2506.04906", "pdf": "https://arxiv.org/pdf/2506.04906", "abs": "https://arxiv.org/abs/2506.04906", "authors": ["Lisa Schmors", "Dominic Gonschorek", "Jan Niklas Böhm", "Yongrong Qiu", "Na Zhou", "Dmitry Kobak", "Andreas Tolias", "Fabian Sinz", "Jacob Reimer", "Katrin Franke", "Sebastian Damrich", "Philipp Berens"], "title": "TRACE: Contrastive learning for multi-trial time-series data in neuroscience", "categories": ["q-bio.NC"], "comment": null, "summary": "Modern neural recording techniques such as two-photon imaging allow to\nacquire vast time-series datasets with responses of hundreds or thousands of\nneurons. Contrastive learning is a powerful self-supervised framework for\nlearning representations of complex datasets. Existing applications for neural\ntime series rely on generic data augmentations and do not exploit the\nmulti-trial data structure inherent in many neural datasets. Here we present\nTRACE, a new contrastive learning framework that averages across different\nsubsets of trials to generate positive pairs. TRACE allows to directly learn a\ntwo-dimensional embedding, combining ideas from contrastive learning and\nneighbor embeddings. We show that TRACE outperforms other methods, resolving\nfine response differences in simulated data. Further, using in vivo recordings,\nwe show that the representations learned by TRACE capture both biologically\nrelevant continuous variation, cell-type-related cluster structure, and can\nassist data quality control."}
{"id": "2506.04943", "pdf": "https://arxiv.org/pdf/2506.04943", "abs": "https://arxiv.org/abs/2506.04943", "authors": ["Shujun Zhou", "Guozhang Chen"], "title": "The Hippocampal Place Field Gradient: An Eigenmode Theory Linking Grid Cell Projections to Multiscale Learning", "categories": ["q-bio.NC"], "comment": null, "summary": "The hippocampus encodes space through a striking gradient of place field\nsizes along its dorsal-ventral axis, yet the principles generating this\ncontinuous gradient from discrete grid cell inputs remain debated. We propose a\nunified theoretical framework establishing that hippocampal place fields arise\nnaturally as linear projections of grid cell population activity, interpretable\nas eigenmodes. Critically, we demonstrate that a frequency-dependent decay of\nthese grid-to-place connection weights naturally transforms inputs from\ndiscrete grid modules into a continuous spectrum of place field sizes. This\nmultiscale organization is functionally significant: we reveal it shapes the\ninductive bias of the population code, balancing a fundamental trade-off\nbetween precision and generalization. Mathematical analysis and simulations\ndemonstrate an optimal place field size for few-shot learning, which scales\nwith environment structure. Our results offer a principled explanation for the\nplace field gradient and generate testable predictions, bridging anatomical\nconnectivity with adaptive learning in both biological and artificial\nintelligence."}
{"id": "2506.05320", "pdf": "https://arxiv.org/pdf/2506.05320", "abs": "https://arxiv.org/abs/2506.05320", "authors": ["Avery Hee-Woon Ryoo", "Nanda H. Krishna", "Ximeng Mao", "Mehdi Azabou", "Eva L. Dyer", "Matthew G. Perich", "Guillaume Lajoie"], "title": "Generalizable, real-time neural decoding with hybrid state-space models", "categories": ["q-bio.NC", "cs.LG"], "comment": "Preprint. Under review", "summary": "Real-time decoding of neural activity is central to neuroscience and\nneurotechnology applications, from closed-loop experiments to brain-computer\ninterfaces, where models are subject to strict latency constraints. Traditional\nmethods, including simple recurrent neural networks, are fast and lightweight\nbut often struggle to generalize to unseen data. In contrast, recent\nTransformer-based approaches leverage large-scale pretraining for strong\ngeneralization performance, but typically have much larger computational\nrequirements and are not always suitable for low-resource or real-time\nsettings. To address these shortcomings, we present POSSM, a novel hybrid\narchitecture that combines individual spike tokenization via a cross-attention\nmodule with a recurrent state-space model (SSM) backbone to enable (1) fast and\ncausal online prediction on neural activity and (2) efficient generalization to\nnew sessions, individuals, and tasks through multi-dataset pretraining. We\nevaluate POSSM's decoding performance and inference speed on intracortical\ndecoding of monkey motor tasks, and show that it extends to clinical\napplications, namely handwriting and speech decoding in human subjects.\nNotably, we demonstrate that pretraining on monkey motor-cortical recordings\nimproves decoding performance on the human handwriting task, highlighting the\nexciting potential for cross-species transfer. In all of these tasks, we find\nthat POSSM achieves decoding accuracy comparable to state-of-the-art\nTransformers, at a fraction of the inference cost (up to 9x faster on GPU).\nThese results suggest that hybrid SSMs are a promising approach to bridging the\ngap between accuracy, inference speed, and generalization when training neural\ndecoders for real-time, closed-loop applications."}
{"id": "2506.04264", "pdf": "https://arxiv.org/pdf/2506.04264", "abs": "https://arxiv.org/abs/2506.04264", "authors": ["Ketian Sun", "Qi Su", "Long Wang"], "title": "Direct reciprocity in asynchronous interactions", "categories": ["physics.soc-ph", "q-bio.PE"], "comment": null, "summary": "Cooperation is vital for the survival of living systems but is challenging\ndue to the costs borne by altruistic individuals. Direct reciprocity, where\nactions are based on past encounters, is a key mechanism fostering cooperation.\nHowever, most studies assume synchronous decision-making, whereas real-world\ninteractions are often asynchronous, with individuals acting in sequence. This\nasynchrony can undermine standard cooperative strategies like Tit-for-Tat and\nWin-Stay Lose-Shift. To better understand cooperation in real-world contexts,\nit is crucial to explore the theory of direct reciprocity in asynchronous\ninteractions. To address this, we introduce a framework based on asynchronous\nstochastic games, incorporating asynchronous decisions and dynamic\nenvironmental feedback. We analytically derive the conditions under which\nstrategies form cooperative Nash equilibria. Our results demonstrate that the\norder of interactions can significantly alter outcomes: interaction asynchrony\ngenerally inhibits cooperation, except under specific conditions where\nenvironmental feedback effectively mitigates its negative impact. When\nenvironmental feedback is incorporated, a variety of stable reciprocal\nstrategies can be sustained. Notably, above a critical environmental threshold,\nany cooperative strategy can form a Nash equilibrium. Overall, our work\nunderscores the importance of interaction order in long-term evolutionary\nprocesses and highlights the pivotal role of environmental feedback in\nstabilizing cooperation in asynchronous interactions."}
{"id": "2506.04289", "pdf": "https://arxiv.org/pdf/2506.04289", "abs": "https://arxiv.org/abs/2506.04289", "authors": ["Jesse Geerts", "Stephanie Chan", "Claudia Clopath", "Kimberly Stachenfeld"], "title": "Relational reasoning and inductive bias in transformers trained on a transitive inference task", "categories": ["cs.LG", "q-bio.NC"], "comment": "13 pages, 6 figures", "summary": "Transformer-based models have demonstrated remarkable reasoning abilities,\nbut the mechanisms underlying relational reasoning in different learning\nregimes remain poorly understood. In this work, we investigate how transformers\nperform a classic relational reasoning task from the Psychology literature,\n\\textit{transitive inference}, which requires inference about indirectly\nrelated items by integrating information across observed adjacent item pairs\n(e.g., if A>B and B>C, then A>C). We compare transitive inference behavior\nacross two distinct learning regimes: in-weights learning (IWL), where models\nstore information in network parameters, and in-context learning (ICL), where\nmodels flexibly utilize information presented within the input sequence. Our\nfindings reveal that IWL naturally induces a generalization bias towards\ntransitive inference, despite being trained only on adjacent items, whereas ICL\nmodels trained solely on adjacent items do not generalize transitively.\nMechanistic analysis shows that ICL models develop induction circuits that\nimplement a simple match-and-copy strategy that performs well at relating\nadjacent pairs, but does not encoding hierarchical relationships among\nindirectly related items. Interestingly, when pre-trained on in-context linear\nregression tasks, transformers successfully exhibit in-context generalizable\ntransitive inference. Moreover, like IWL, they display both \\textit{symbolic\ndistance} and \\textit{terminal item effects} characteristic of human and animal\nperformance, without forming induction circuits. These results suggest that\npre-training on tasks with underlying structure promotes the development of\nrepresentations that can scaffold in-context relational reasoning."}
{"id": "2506.04379", "pdf": "https://arxiv.org/pdf/2506.04379", "abs": "https://arxiv.org/abs/2506.04379", "authors": ["Matthew W. Shinkle", "Mark D. Lescroart"], "title": "Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "comment": "Accepted to the Mechanistic Interpretability for Vision (MIV)\n  Workshop at the 2025 Conference on Computer Vision and Pattern Recognition\n  (CVPR) conference", "summary": "Deep neural networks (DNNs) trained on visual tasks develop feature\nrepresentations that resemble those in the human visual system. Although\nDNN-based encoding models can accurately predict brain responses to visual\nstimuli, they offer limited insight into the specific features driving these\nresponses. Here, we demonstrate that activation maximization -- a technique\ndesigned to interpret vision DNNs -- can be applied to DNN-based encoding\nmodels of the human brain. We extract and adaptively downsample activations\nfrom multiple layers of a pretrained Inception V3 network, then use linear\nregression to predict fMRI responses. This yields a full image-computable model\nof brain responses. Next, we apply activation maximization to generate images\noptimized for predicted responses in individual cortical voxels. We find that\nthese images contain visual characteristics that qualitatively correspond with\nknown selectivity and enable exploration of selectivity across the visual\ncortex. We further extend our method to whole regions of interest (ROIs) of the\nbrain and validate its efficacy by presenting these images to human\nparticipants in an fMRI study. We find that the generated images reliably drive\nactivity in targeted regions across both low- and high-level visual areas and\nacross subjects. These results demonstrate that activation maximization can be\nsuccessfully applied to DNN-based encoding models. By addressing key\nlimitations of alternative approaches that require natively generative models,\nour approach enables flexible characterization and modulation of responses\nacross the human visual system."}
{"id": "2506.04490", "pdf": "https://arxiv.org/pdf/2506.04490", "abs": "https://arxiv.org/abs/2506.04490", "authors": ["Rishwanth Raghu", "Axel Levy", "Gordon Wetzstein", "Ellen D. Zhong"], "title": "Multiscale guidance of AlphaFold3 with heterogeneous cryo-EM data", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Protein structure prediction models are now capable of generating accurate 3D\nstructural hypotheses from sequence alone. However, they routinely fail to\ncapture the conformational diversity of dynamic biomolecular complexes, often\nrequiring heuristic MSA subsampling approaches for generating alternative\nstates. In parallel, cryo-electron microscopy (cryo-EM) has emerged as a\npowerful tool for imaging near-native structural heterogeneity, but is\nchallenged by arduous pipelines to go from raw experimental data to atomic\nmodels. Here, we bridge the gap between these modalities, combining cryo-EM\ndensity maps with the rich sequence and biophysical priors learned by protein\nstructure prediction models. Our method, CryoBoltz, guides the sampling\ntrajectory of a pretrained protein structure prediction model using both global\nand local structural constraints derived from density maps, driving predictions\ntowards conformational states consistent with the experimental data. We\ndemonstrate that this flexible yet powerful inference-time approach allows us\nto build atomic models into heterogeneous cryo-EM maps across a variety of\ndynamic biomolecular systems including transporters and antibodies."}
{"id": "2506.04536", "pdf": "https://arxiv.org/pdf/2506.04536", "abs": "https://arxiv.org/abs/2506.04536", "authors": ["Luca Ghafourpour", "Valentin Duruisseaux", "Bahareh Tolooshams", "Philip H. Wong", "Costas A. Anastassiou", "Anima Anandkumar"], "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Characterizing the diverse computational properties of human neurons via\nmultimodal electrophysiological, transcriptomic, and morphological data\nprovides the foundation for constructing and validating bio-realistic neuron\nmodels that can advance our understanding of fundamental mechanisms underlying\nbrain function. However, current modeling approaches remain constrained by the\nlimited availability and intrinsic variability of experimental neuronal data.\nTo capture variability, ensembles of deterministic models are often used, but\nare difficult to scale as model generation requires repeating computationally\nexpensive optimization for each neuron. While deep learning is becoming\nincreasingly relevant in this space, it fails to capture the full biophysical\ncomplexity of neurons, their nonlinear voltage dynamics, and variability. To\naddress these shortcomings, we introduce NOBLE, a neural operator framework\nthat learns a mapping from a continuous frequency-modulated embedding of\ninterpretable neuron features to the somatic voltage response induced by\ncurrent injection. Trained on data generated from biophysically realistic\nneuron models, NOBLE predicts distributions of neural dynamics accounting for\nthe intrinsic experimental variability. Unlike conventional bio-realistic\nneuron models, interpolating within the embedding space offers models whose\ndynamics are consistent with experimentally observed responses. NOBLE is the\nfirst scaled-up deep learning framework validated on real experimental data,\nenabling efficient generation of synthetic neurons that exhibit trial-to-trial\nvariability and achieve a $4200\\times$ speedup over numerical solvers. To this\nend, NOBLE captures fundamental neural properties, opening the door to a better\nunderstanding of cellular composition and computations, neuromorphic\narchitectures, large-scale brain circuits, and general neuroAI applications."}
{"id": "2506.05127", "pdf": "https://arxiv.org/pdf/2506.05127", "abs": "https://arxiv.org/abs/2506.05127", "authors": ["Srikar Yellapragada", "Alexandros Graikos", "Zilinghan Li", "Kostas Triaridis", "Varun Belagali", "Saarthak Kapse", "Tarak Nath Nandi", "Ravi K Madduri", "Prateek Prasanna", "Tahsin Kurc", "Rajarsi R. Gupta", "Joel Saltz", "Dimitris Samaras"], "title": "PixCell: A generative foundation model for digital histopathology images", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "comment": null, "summary": "The digitization of histology slides has revolutionized pathology, providing\nmassive datasets for cancer diagnosis and research. Contrastive self-supervised\nand vision-language models have been shown to effectively mine large pathology\ndatasets to learn discriminative representations. On the other hand, generative\nmodels, capable of synthesizing realistic and diverse images, present a\ncompelling solution to address unique problems in pathology that involve\nsynthesizing images; overcoming annotated data scarcity, enabling\nprivacy-preserving data sharing, and performing inherently generative tasks,\nsuch as virtual staining. We introduce PixCell, the first diffusion-based\ngenerative foundation model for histopathology. We train PixCell on PanCan-30M,\na vast, diverse dataset derived from 69,184 H\\&E-stained whole slide images\ncovering various cancer types. We employ a progressive training strategy and a\nself-supervision-based conditioning that allows us to scale up training without\nany annotated data. PixCell generates diverse and high-quality images across\nmultiple cancer types, which we find can be used in place of real data to train\na self-supervised discriminative model. Synthetic images shared between\ninstitutions are subject to fewer regulatory barriers than would be the case\nwith real clinical images. Furthermore, we showcase the ability to precisely\ncontrol image generation using a small set of annotated images, which can be\nused for both data augmentation and educational purposes. Testing on a cell\nsegmentation task, a mask-guided PixCell enables targeted data augmentation,\nimproving downstream performance. Finally, we demonstrate PixCell's ability to\nuse H\\&E structural staining to infer results from molecular marker studies; we\nuse this capability to infer IHC staining from H\\&E images. Our trained models\nare publicly released to accelerate research in computational pathology."}
{"id": "2506.05178", "pdf": "https://arxiv.org/pdf/2506.05178", "abs": "https://arxiv.org/abs/2506.05178", "authors": ["Joshua Hess", "Quaid Morris"], "title": "Associative Memory and Generative Diffusion in the Zero-noise Limit", "categories": ["cs.LG", "cond-mat.dis-nn", "math.DS", "nlin.AO", "q-bio.NC"], "comment": null, "summary": "Connections between generative diffusion and continuous-state associative\nmemory models are studied. Morse-Smale dynamical systems are emphasized as\nuniversal approximators of gradient-based associative memory models and\ndiffusion models as white-noise perturbed systems thereof. Universal properties\nof associative memory that follow from this description are described and used\nto characterize a generic transition from generation to memory as noise levels\ndiminish. Structural stability inherited by Morse-Smale flows is shown to imply\na notion of stability for diffusions at vanishing noise levels. Applied to one-\nand two-parameter families of gradients, this indicates stability at all but\nisolated points of associative memory learning landscapes and the learning and\ngeneration landscapes of diffusion models with gradient drift in the zero-noise\nlimit, at which small sets of generic bifurcations characterize qualitative\ntransitions between stable systems. Examples illustrating the characterization\nof these landscapes by sequences of these bifurcations are given, along with\nstructural stability criterion for classic and modern Hopfield networks\n(equivalently, the attention mechanism)."}
{"id": "2506.05303", "pdf": "https://arxiv.org/pdf/2506.05303", "abs": "https://arxiv.org/abs/2506.05303", "authors": ["David G. Clark"], "title": "Transient dynamics of associative memory models", "categories": ["cond-mat.dis-nn", "q-bio.NC"], "comment": "18 pages, 7 figures", "summary": "Associative memory models such as the Hopfield network and its dense\ngeneralizations with higher-order interactions exhibit a \"blackout\ncatastrophe\"--a discontinuous transition where stable memory states abruptly\nvanish when the number of stored patterns exceeds a critical capacity. This\ntransition is often interpreted as rendering networks unusable beyond capacity\nlimits. We argue that this interpretation is largely an artifact of the\nequilibrium perspective. We derive dynamical mean-field equations using a\nbipartite cavity approach for graded-activity dense associative memory models,\nwith the Hopfield model as a special case, and solve them using a numerical\nscheme. We show that patterns can be transiently retrieved with high accuracy\nabove capacity despite the absence of stable attractors. This occurs because\nslow regions persist in the above-capacity energy landscape as shallow,\nunstable remnants of below-capacity stable basins. The same transient-retrieval\neffect occurs in below-capacity networks initialized outside basins of\nattraction. \"Transient-recovery curves\" provide a concise visual summary of\nthese effects, revealing graceful, non-catastrophic changes in retrieval\nbehavior above capacity and allowing us to compare the behavior across\ninteraction orders. This dynamical perspective reveals rich energy landscape\nstructure obscured by equilibrium analysis and suggests biological neural\ncircuits may exploit transient dynamics for memory retrieval. Furthermore, our\napproach suggests ways of understanding computational properties of neural\ncircuits without reference to fixed points, advances the technical repertoire\nof numerical mean-field solution methods for recurrent neural networks, and\nyields new theoretical results on generalizations of the Hopfield model."}
{"id": "2506.04235", "pdf": "https://arxiv.org/pdf/2506.04235", "abs": "https://arxiv.org/abs/2506.04235", "authors": ["Xinyan Zhao", "Yi-Ching Tang", "Akshita Singh", "Victor J Cantu", "KwanHo An", "Junseok Lee", "Adam E Stogsdill", "Ashwin Kumar Ramesh", "Zhiqiang An", "Xiaoqian Jiang", "Yejin Kim"], "title": "Benchmark for Antibody Binding Affinity Maturation and Design", "categories": ["q-bio.QM", "cs.AI", "cs.CE", "cs.LG", "q-bio.BM"], "comment": null, "summary": "We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking\nframework for antibody binding affinity maturation and design. Unlike existing\nantibody evaluation strategies that rely on antibody alone and its similarity\nto natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench\nconsiders an antibody-antigen (Ab-Ag) complex as a functional unit and\nevaluates the potential of an antibody design binding to given antigen by\nmeasuring protein model's likelihood on the Ab-Ag complex. We first curate,\nstandardize, and share 9 datasets containing 9 antigens (involving influenza,\nanti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain\nmutated antibodies. Using these datasets, we systematically compare 14 protein\nmodels including masked language models, autoregressive language models,\ninverse folding models, diffusion-based generative models, and geometric graph\nmodels. The correlation between model likelihood and experimental affinity\nvalues is used to evaluate model performance. Additionally, in a case study to\nincrease binding affinity of antibody F045-092 to antigen influenza H1N1, we\nevaluate the generative power of the top-performing models by sampling a set of\nnew antibodies binding to the antigen and ranking them based on structural\nintegrity and biophysical properties of the Ab-Ag complex. As a result,\nstructure-conditioned inverse folding models outperform others in both affinity\ncorrelation and generation tasks. Overall, AbBiBench provides a unified,\nbiologically grounded evaluation framework to facilitate the development of\nmore effective, function-aware antibody design models."}
{"id": "2506.04515", "pdf": "https://arxiv.org/pdf/2506.04515", "abs": "https://arxiv.org/abs/2506.04515", "authors": ["Salil Patel"], "title": "The Latent Space Hypothesis: Toward Universal Medical Representation Learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "51 pages, 12 figures. A position paper examining the latent space\n  hypothesis - the proposition that diverse medical data can be represented in\n  shared latent spaces reflecting fundamental biological processes. The paper\n  discusses theoretical foundations, reviews supporting evidence, and considers\n  potential implications for medical AI and representation learning", "summary": "Medical data range from genomic sequences and retinal photographs to\nstructured laboratory results and unstructured clinical narratives. Although\nthese modalities appear disparate, many encode convergent information about a\nsingle underlying physiological state. The Latent Space Hypothesis frames each\nobservation as a projection of a unified, hierarchically organized manifold --\nmuch like shadows cast by the same three-dimensional object. Within this\nlearned geometric representation, an individual's health status occupies a\npoint, disease progression traces a trajectory, and therapeutic intervention\ncorresponds to a directed vector. Interpreting heterogeneous evidence in a\nshared space provides a principled way to re-examine eponymous conditions --\nsuch as Parkinson's or Crohn's -- that often mask multiple pathophysiological\nentities and involve broader anatomical domains than once believed. By\nrevealing sub-trajectories and patient-specific directions of change, the\nframework supplies a quantitative rationale for personalised diagnosis,\nlongitudinal monitoring, and tailored treatment, moving clinical practice away\nfrom grouping by potentially misleading labels toward navigation of each\nperson's unique trajectory. Challenges remain -- bias amplification, data\nscarcity for rare disorders, privacy, and the correlation-causation divide --\nbut scale-aware encoders, continual learning on longitudinal data streams, and\nperturbation-based validation offer plausible paths forward."}
{"id": "2506.04235", "pdf": "https://arxiv.org/pdf/2506.04235", "abs": "https://arxiv.org/abs/2506.04235", "authors": ["Xinyan Zhao", "Yi-Ching Tang", "Akshita Singh", "Victor J Cantu", "KwanHo An", "Junseok Lee", "Adam E Stogsdill", "Ashwin Kumar Ramesh", "Zhiqiang An", "Xiaoqian Jiang", "Yejin Kim"], "title": "Benchmark for Antibody Binding Affinity Maturation and Design", "categories": ["q-bio.QM", "cs.AI", "cs.CE", "cs.LG", "q-bio.BM"], "comment": null, "summary": "We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking\nframework for antibody binding affinity maturation and design. Unlike existing\nantibody evaluation strategies that rely on antibody alone and its similarity\nto natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench\nconsiders an antibody-antigen (Ab-Ag) complex as a functional unit and\nevaluates the potential of an antibody design binding to given antigen by\nmeasuring protein model's likelihood on the Ab-Ag complex. We first curate,\nstandardize, and share 9 datasets containing 9 antigens (involving influenza,\nanti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain\nmutated antibodies. Using these datasets, we systematically compare 14 protein\nmodels including masked language models, autoregressive language models,\ninverse folding models, diffusion-based generative models, and geometric graph\nmodels. The correlation between model likelihood and experimental affinity\nvalues is used to evaluate model performance. Additionally, in a case study to\nincrease binding affinity of antibody F045-092 to antigen influenza H1N1, we\nevaluate the generative power of the top-performing models by sampling a set of\nnew antibodies binding to the antigen and ranking them based on structural\nintegrity and biophysical properties of the Ab-Ag complex. As a result,\nstructure-conditioned inverse folding models outperform others in both affinity\ncorrelation and generation tasks. Overall, AbBiBench provides a unified,\nbiologically grounded evaluation framework to facilitate the development of\nmore effective, function-aware antibody design models."}
{"id": "2506.04515", "pdf": "https://arxiv.org/pdf/2506.04515", "abs": "https://arxiv.org/abs/2506.04515", "authors": ["Salil Patel"], "title": "The Latent Space Hypothesis: Toward Universal Medical Representation Learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "51 pages, 12 figures. A position paper examining the latent space\n  hypothesis - the proposition that diverse medical data can be represented in\n  shared latent spaces reflecting fundamental biological processes. The paper\n  discusses theoretical foundations, reviews supporting evidence, and considers\n  potential implications for medical AI and representation learning", "summary": "Medical data range from genomic sequences and retinal photographs to\nstructured laboratory results and unstructured clinical narratives. Although\nthese modalities appear disparate, many encode convergent information about a\nsingle underlying physiological state. The Latent Space Hypothesis frames each\nobservation as a projection of a unified, hierarchically organized manifold --\nmuch like shadows cast by the same three-dimensional object. Within this\nlearned geometric representation, an individual's health status occupies a\npoint, disease progression traces a trajectory, and therapeutic intervention\ncorresponds to a directed vector. Interpreting heterogeneous evidence in a\nshared space provides a principled way to re-examine eponymous conditions --\nsuch as Parkinson's or Crohn's -- that often mask multiple pathophysiological\nentities and involve broader anatomical domains than once believed. By\nrevealing sub-trajectories and patient-specific directions of change, the\nframework supplies a quantitative rationale for personalised diagnosis,\nlongitudinal monitoring, and tailored treatment, moving clinical practice away\nfrom grouping by potentially misleading labels toward navigation of each\nperson's unique trajectory. Challenges remain -- bias amplification, data\nscarcity for rare disorders, privacy, and the correlation-causation divide --\nbut scale-aware encoders, continual learning on longitudinal data streams, and\nperturbation-based validation offer plausible paths forward."}
