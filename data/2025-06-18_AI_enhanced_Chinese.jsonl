{"id": "2506.13817", "pdf": "https://arxiv.org/pdf/2506.13817", "abs": "https://arxiv.org/abs/2506.13817", "authors": ["Saleem A. Al Dajani", "Abel Sanchez", "John R. Williams"], "title": "DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models", "categories": ["q-bio.GN", "cs.AI", "cs.LG", "cs.SE", "q-bio.QM"], "comment": "4 pages, 5 figures, Accepted by ICML 2025 FM4LS https://openreview.net/forum?id=zNjXOZxEYB . Workshop on Multi-modal Foundation Models and Large Language Models for Life Sciences (FM4LS)}, July 2025", "summary": "Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable of downstream tasks such as cell-typing and perturbation prediction. As data volume grows, these models may surpass human performance in labeling, paving the way for reliable inference in large-scale perturbation screens. This application demonstrates domain-specific innovation in health monitoring and diagnostics, aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0fAI\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u65f6\u7f51\u7edc\u641c\u7d22\u81ea\u52a8\u6807\u6ce8\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\uff0c\u51c6\u786e\u7387\u8fbe82.5%\uff0c\u89e3\u51b3\u4e86\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u6807\u6ce8\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u89c4\u6a21\u8fc5\u901f\u589e\u957f\u81f3\u6570\u5341\u4ebf\u7ec6\u80de\uff0c\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5177\u6709\u5b9e\u65f6\u7f51\u7edc\u641c\u7d22\u80fd\u529b\u7684\u4ee3\u7406\u57fa\u7840\u6a21\u578b\uff0c\u81ea\u52a8\u5316\u6807\u6ce8\u5b9e\u9a8c\u6570\u636e\u3002", "result": "\u5b9e\u73b0\u4e8682.5%\u7684\u6807\u6ce8\u51c6\u786e\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u6ce8\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u865a\u62df\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u7684\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u672a\u6765\u53ef\u80fd\u8d85\u8d8a\u4eba\u7c7b\u6807\u6ce8\u6027\u80fd\uff0c\u652f\u6301\u5927\u89c4\u6a21\u6270\u52a8\u7b5b\u9009\u7684\u53ef\u9760\u63a8\u65ad\u3002"}}
{"id": "2506.13830", "pdf": "https://arxiv.org/pdf/2506.13830", "abs": "https://arxiv.org/abs/2506.13830", "authors": ["Xiang Ma", "Supantha Dey", "Vaishnavey SR", "Casey Zelinski", "Qi Li", "Ratul Chowdhury"], "title": "Seq2Bind Webserver for Decoding Binding Hotspots directly from Sequences using Fine-Tuned Protein Language Models", "categories": ["q-bio.QM"], "comment": null, "summary": "Decoding protein-protein interactions (PPIs) at the residue level is crucial for understanding cellular mechanisms and developing targeted therapeutics. We present Seq2Bind Webserver, a computational framework that leverages fine-tuned protein language models (PLMs) to determine binding affinity between proteins and identify critical binding residues directly from sequences, eliminating the structural requirements that limit most affinity prediction tools. We fine-tuned four architectures including ProtBERT, ProtT5, ESM2, and BiLSTM on the SKEMPI 2.0 dataset containing 5,387 protein pairs with experimental binding affinities. Through systematic alanine mutagenesis on each residue for 14 therapeutically relevant protein complexes, we evaluated each model's ability to identify interface residues. Performance was assessed using N-factor metrics, where N-factor=3 evaluates whether true residues appear within 3n top predictions for n interface residues. ESM2 achieved 49.5% accuracy at N-factor=3, with both ESM2 (37.2%) and ProtBERT (35.1%) outperforming structural docking method HADDOCK3 (32.1%) at N-factor=2. Our sequence-based approach enables rapid screening (minutes versus hours for docking), handles disordered proteins, and provides comparable accuracy, making Seq2Bind a valuable prior to steer blind docking protocols to identify putative binding residues from each protein for therapeutic targets. Seq2Bind Webserver is accessible at https://agrivax.onrender.com under StructF suite.", "AI": {"tldr": "Seq2Bind Webserver\u5229\u7528\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u4ece\u5e8f\u5217\u9884\u6d4b\u86cb\u767d\u8d28\u7ed3\u5408\u4eb2\u548c\u529b\u548c\u5173\u952e\u7ed3\u5408\u6b8b\u57fa\uff0c\u65e0\u9700\u7ed3\u6784\u4fe1\u606f\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\uff08PPIs\uff09\u5bf9\u7ec6\u80de\u673a\u5236\u548c\u9776\u5411\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u4f9d\u8d56\u7ed3\u6784\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5e94\u7528\u3002", "method": "\u57fa\u4e8eSKEMPI 2.0\u6570\u636e\u96c6\uff0c\u5bf9ProtBERT\u3001ProtT5\u3001ESM2\u548cBiLSTM\u56db\u79cd\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u4e19\u6c28\u9178\u7a81\u53d8\u5b9e\u9a8c\u8bc4\u4f30\u6027\u80fd\u3002", "result": "ESM2\u5728N-factor=3\u65f6\u51c6\u786e\u7387\u8fbe49.5%\uff0c\u4f18\u4e8eHADDOCK3\uff0832.1%\uff09\u3002\u5e8f\u5217\u65b9\u6cd5\u901f\u5ea6\u5feb\uff0c\u9002\u7528\u4e8e\u65e0\u5e8f\u86cb\u767d\u8d28\u3002", "conclusion": "Seq2Bind\u4e3a\u5feb\u901f\u7b5b\u9009\u7ed3\u5408\u6b8b\u57fa\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u6cbb\u7597\u9776\u70b9\u7814\u7a76\u3002"}}
{"id": "2506.13835", "pdf": "https://arxiv.org/pdf/2506.13835", "abs": "https://arxiv.org/abs/2506.13835", "authors": ["Masakazu Inoue", "Motoshige Sato", "Kenichi Tomeoka", "Nathania Nah", "Eri Hatakeyama", "Kai Arulkumaran", "Ilya Horiguchi", "Shuntaro Sasai"], "title": "A Silent Speech Decoding System from EEG and EMG with Heterogenous Electrode Configurations", "categories": ["q-bio.QM", "cs.LG", "q-bio.NC"], "comment": "Accepted for presentation at Interspeech 2025. 5 pages, 4 figures, 2 tables", "summary": "Silent speech decoding, which performs unvocalized human speech recognition from electroencephalography/electromyography (EEG/EMG), increases accessibility for speech-impaired humans. However, data collection is difficult and performed using varying experimental setups, making it nontrivial to collect a large, homogeneous dataset. In this study we introduce neural networks that can handle EEG/EMG with heterogeneous electrode placements and show strong performance in silent speech decoding via multi-task training on large-scale EEG/EMG datasets. We achieve improved word classification accuracy in both healthy participants (95.3%), and a speech-impaired patient (54.5%), substantially outperforming models trained on single-subject data (70.1% and 13.2%). Moreover, our models also show gains in cross-language calibration performance. This increase in accuracy suggests the feasibility of developing practical silent speech decoding systems, particularly for speech-impaired patients.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5f02\u8d28\u7535\u6781\u653e\u7f6e\u7684EEG/EMG\u6570\u636e\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u8bad\u7ec3\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u65e0\u58f0\u8bed\u97f3\u89e3\u7801\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u65e0\u58f0\u8bed\u97f3\u89e3\u7801\u5bf9\u8a00\u8bed\u969c\u788d\u60a3\u8005\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u6570\u636e\u6536\u96c6\u56f0\u96be\u4e14\u5b9e\u9a8c\u8bbe\u7f6e\u591a\u6837\uff0c\u96be\u4ee5\u6784\u5efa\u5927\u89c4\u6a21\u540c\u8d28\u6570\u636e\u96c6\u3002", "method": "\u5f15\u5165\u795e\u7ecf\u7f51\u7edc\u5904\u7406\u5f02\u8d28\u7535\u6781\u653e\u7f6e\u7684EEG/EMG\u6570\u636e\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u8bad\u7ec3\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u65e0\u58f0\u8bed\u97f3\u89e3\u7801\u3002", "result": "\u5728\u5065\u5eb7\u53c2\u4e0e\u8005\u4e2d\u8fbe\u523095.3%\u7684\u5355\u8bcd\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u8a00\u8bed\u969c\u788d\u60a3\u8005\u4e2d\u8fbe\u523054.5%\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u53d7\u8bd5\u8005\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0870.1%\u548c13.2%\uff09\u3002\u8de8\u8bed\u8a00\u6821\u51c6\u6027\u80fd\u4e5f\u6709\u6240\u63d0\u5347\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u5f00\u53d1\u5b9e\u7528\u7684\u65e0\u58f0\u8bed\u97f3\u89e3\u7801\u7cfb\u7edf\u5177\u6709\u53ef\u884c\u6027\uff0c\u5c24\u5176\u5bf9\u8a00\u8bed\u969c\u788d\u60a3\u8005\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.13843", "pdf": "https://arxiv.org/pdf/2506.13843", "abs": "https://arxiv.org/abs/2506.13843", "authors": ["Alejandro Golfe", "Natalia P. Garc\u00eda-de-la-puente", "Adri\u00e1n Colomer", "Valery Naranjo"], "title": "BlastDiffusion: A Latent Diffusion Model for Generating Synthetic Embryo Images to Address Data Scarcity in In Vitro Fertilization", "categories": ["q-bio.QM"], "comment": null, "summary": "Accurately identifying oocytes that progress to the blastocyst stage is crucial in reproductive medicine, but the limited availability of annotated high-quality embryo images presents challenges for developing automated diagnostic tools. To address this, we propose BlastDiffusion, a generative model based on Latent Diffusion Models (LDMs) that synthesizes realistic oocyte images conditioned on developmental outcomes. Our approach utilizes a pretrained Variational Autoencoder (VAE) for latent space representation, combined with a diffusion process to generate images that distinguish between oocytes that reach the blastocyst stage and those that do not. When compared to Blastocyst-GAN, a GAN-based model we trained for this task, BlastDiffusion achieves superior performance, with a global Frechet Inception Distance (FID) of 94.32, significantly better than Blastocyst-GAN's FID of 232.73. Additionally, our model shows improvements in perceptual (LPIPS) and structural (SSIM) similarity to real oocyte images. Qualitative analysis further demonstrates that BlastDiffusion captures key morphological differences linked to developmental outcomes. These results highlight the potential of diffusion models in reproductive medicine, offering an effective tool for data augmentation and automated embryo assessment.", "AI": {"tldr": "BlastDiffusion\u662f\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u5408\u6210\u771f\u5b9e\u5375\u6bcd\u7ec6\u80de\u56fe\u50cf\uff0c\u4ee5\u533a\u5206\u80fd\u53d1\u80b2\u5230\u56ca\u80da\u9636\u6bb5\u7684\u5375\u6bcd\u7ec6\u80de\u3002\u76f8\u6bd4GAN\u6a21\u578b\uff0cBlastDiffusion\u5728\u6027\u80fd\u4e0a\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5728\u751f\u6b96\u533b\u5b66\u4e2d\uff0c\u51c6\u786e\u8bc6\u522b\u80fd\u53d1\u80b2\u5230\u56ca\u80da\u9636\u6bb5\u7684\u5375\u6bcd\u7ec6\u80de\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u8d28\u91cf\u6807\u6ce8\u80da\u80ce\u56fe\u50cf\u7684\u7a00\u7f3a\u6027\u9650\u5236\u4e86\u81ea\u52a8\u5316\u8bca\u65ad\u5de5\u5177\u7684\u5f00\u53d1\u3002", "method": "BlastDiffusion\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u6269\u6563\u8fc7\u7a0b\uff0c\u751f\u6210\u533a\u5206\u53d1\u80b2\u7ed3\u679c\u7684\u5375\u6bcd\u7ec6\u80de\u56fe\u50cf\u3002", "result": "BlastDiffusion\u7684FID\u4e3a94.32\uff0c\u663e\u8457\u4f18\u4e8eGAN\u6a21\u578b\u7684232.73\uff0c\u4e14\u5728\u611f\u77e5\u548c\u7ed3\u6784\u76f8\u4f3c\u6027\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6b96\u533b\u5b66\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u6570\u636e\u589e\u5f3a\u548c\u81ea\u52a8\u5316\u80da\u80ce\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2506.14014", "pdf": "https://arxiv.org/pdf/2506.14014", "abs": "https://arxiv.org/abs/2506.14014", "authors": ["Laurence Freeman", "Philip Shamash", "Vinam Arora", "Caswell Barry", "Tiago Branco", "Eva Dyer"], "title": "Beyond Black Boxes: Enhancing Interpretability of Transformers Trained on Neural Data", "categories": ["q-bio.QM"], "comment": null, "summary": "Transformer models have become state-of-the-art in decoding stimuli and behavior from neural activity, significantly advancing neuroscience research. Yet greater transparency in their decision-making processes would substantially enhance their utility in scientific and clinical contexts. Sparse autoencoders offer a promising solution by producing hidden units that respond selectively to specific variables, enhancing interpretability. Here, we introduce SAEs into a neural decoding framework by augmenting a transformer trained to predict visual stimuli from calcium imaging in the mouse visual cortex. The enhancement of the transformer model with an SAE preserved its original performance while yielding hidden units that selectively responded to interpretable features, such as stimulus orientation and genetic background. Furthermore, ablating units associated with a given variable impaired the model's ability to process that variable, revealing how specific internal representations support downstream computations. Together, these results demonstrate that integrating SAEs with transformers combines the power of modern deep learning with the interpretability essential for scientific understanding and clinical translation.", "AI": {"tldr": "\u5c06\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u4e0eTransformer\u7ed3\u5408\uff0c\u63d0\u5347\u795e\u7ecf\u89e3\u7801\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1Transformer\u5728\u795e\u7ecf\u89e3\u7801\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u9650\u5236\u4e86\u79d1\u5b66\u548c\u4e34\u5e8a\u5e94\u7528\u3002\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u80fd\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5728\u57fa\u4e8e\u5c0f\u9f20\u89c6\u89c9\u76ae\u5c42\u9499\u6210\u50cf\u7684Transformer\u6a21\u578b\u4e2d\u5f15\u5165SAE\uff0c\u751f\u6210\u9009\u62e9\u6027\u54cd\u5e94\u7279\u5b9a\u53d8\u91cf\u7684\u9690\u85cf\u5355\u5143\u3002", "result": "SAE\u589e\u5f3a\u7684Transformer\u6a21\u578b\u6027\u80fd\u672a\u53d7\u5f71\u54cd\uff0c\u4e14\u9690\u85cf\u5355\u5143\u80fd\u9009\u62e9\u6027\u54cd\u5e94\u53ef\u89e3\u91ca\u7279\u5f81\uff08\u5982\u523a\u6fc0\u65b9\u5411\u548c\u9057\u4f20\u80cc\u666f\uff09\u3002\u6d88\u878d\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u7279\u5b9a\u5185\u90e8\u8868\u5f81\u7684\u4f5c\u7528\u3002", "conclusion": "SAE\u4e0eTransformer\u7684\u7ed3\u5408\u65e2\u4fdd\u7559\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u53c8\u63d0\u4f9b\u4e86\u79d1\u5b66\u7406\u89e3\u548c\u4e34\u5e8a\u8f6c\u5316\u6240\u9700\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.14021", "pdf": "https://arxiv.org/pdf/2506.14021", "abs": "https://arxiv.org/abs/2506.14021", "authors": ["Saahil Chadha", "David Weiss", "Anastasia Janas", "Divya Ramakrishnan", "Thomas Hager", "Klara Osenberg", "Klara Willms", "Joshua Zhu", "Veronica Chiang", "Spyridon Bakas", "Nazanin Maleki", "Durga V. Sritharan", "Sven Schoenherr", "Malte Westerhoff", "Matthew Zawalich", "Melissa Davis", "Ajay Malhotra", "Khaled Bousabarah", "Cornelius Deuschl", "MingDe Lin", "Sanjay Aneja", "Mariam S. Aboian"], "title": "An 11,000-Study Open-Access Dataset of Longitudinal Magnetic Resonance Images of Brain Metastases", "categories": ["q-bio.QM"], "comment": null, "summary": "Brain metastases are a common complication of systemic cancer, affecting over 20% of patients with primary malignancies. Longitudinal magnetic resonance imaging (MRI) is essential for diagnosing patients, tracking disease progression, assessing therapeutic response, and guiding treatment selection. However, the manual review of longitudinal imaging is time-intensive, especially for patients with multifocal disease. Artificial intelligence (AI) offers opportunities to streamline image evaluation, but developing robust AI models requires comprehensive training data representative of real-world imaging studies. Thus, there is an urgent necessity for a large dataset with heterogeneity in imaging protocols and disease presentation. To address this, we present an open-access dataset of 11,884 longitudinal brain MRI studies from 1,430 patients with clinically confirmed brain metastases, paired with clinical and image metadata. The provided dataset will facilitate the development of AI models to assist in the long-term management of patients with brain metastasis.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5305\u542b11,884\u6b21\u7eb5\u5411\u8111MRI\u7814\u7a76\u7684\u5f00\u653e\u6570\u636e\u96c6\uff0c\u65e8\u5728\u652f\u6301AI\u6a21\u578b\u5f00\u53d1\uff0c\u4ee5\u4f18\u5316\u8111\u8f6c\u79fb\u60a3\u8005\u7684\u957f\u671f\u7ba1\u7406\u3002", "motivation": "\u8111\u8f6c\u79fb\u662f\u7cfb\u7edf\u6027\u764c\u75c7\u7684\u5e38\u89c1\u5e76\u53d1\u75c7\uff0c\u624b\u52a8\u5206\u6790\u7eb5\u5411MRI\u8017\u65f6\u4e14\u590d\u6742\uff0c\u9700\u8981\u4ee3\u8868\u6027\u6570\u636e\u5f00\u53d1AI\u6a21\u578b\u3002", "method": "\u6536\u96c6\u4e861,430\u540d\u8111\u8f6c\u79fb\u60a3\u8005\u768411,884\u6b21\u7eb5\u5411MRI\u7814\u7a76\uff0c\u5e76\u914d\u5bf9\u4e34\u5e8a\u548c\u56fe\u50cf\u5143\u6570\u636e\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u653e\u6570\u636e\u96c6\uff0c\u652f\u6301AI\u6a21\u578b\u7684\u5f00\u53d1\u548c\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u5c06\u4fc3\u8fdbAI\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u6539\u5584\u8111\u8f6c\u79fb\u60a3\u8005\u7684\u957f\u671f\u7ba1\u7406\u3002"}}
{"id": "2506.14052", "pdf": "https://arxiv.org/pdf/2506.14052", "abs": "https://arxiv.org/abs/2506.14052", "authors": ["Ananya Joshi", "George Khoury", "Christodoulas Floudas"], "title": "Inhibiting Alzheimer's Disease by Targeting Aggregation of Beta-Amyloid", "categories": ["q-bio.QM"], "comment": null, "summary": "Alzheimer's disease is characterized by dangerous amyloid plaques formed by deposits of the protein Beta-Amyloid aggregates in the brain. The specific amino acid sequence that is responsible for the aggregates of Beta-Amyloid is lys-leu-val-phe-phe (KLVFF). KLVFF aggregation inhibitors, which we design in this paper, prevent KLVFF from binding with itself to form oligomers or fibrils (and eventually plaques) that cause neuronal death. Our binder-blocker peptides are designed such that, on one side, they bind strongly to KLVFF, and on the other side, they disrupt critical interactions, thus preventing aggregation. Our methods use optimization techniques and molecular simulations and identify 10 candidate sequences for trial of the 3.2 million possible sequences. This approach for inhibitor identification can be generalized to other diseases characterized by protein aggregation, such as Parkinson's, Huntington's, and prion diseases.", "AI": {"tldr": "\u8bba\u6587\u8bbe\u8ba1\u4e86\u9488\u5bf9Beta-Amyloid\u86cb\u767d\u805a\u96c6\u5e8f\u5217KLVFF\u7684\u6291\u5236\u5242\uff0c\u901a\u8fc7\u963b\u65ad\u5176\u81ea\u805a\u96c6\u6765\u9884\u9632\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u6dc0\u7c89\u6837\u6591\u5757\u5f62\u6210\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u6dc0\u7c89\u6837\u6591\u5757\u7531Beta-Amyloid\u86cb\u767d\u805a\u96c6\u5f62\u6210\uff0cKLVFF\u5e8f\u5217\u662f\u5173\u952e\u3002\u8bbe\u8ba1\u6291\u5236\u5242\u53ef\u963b\u65ad\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u4ece\u800c\u9884\u9632\u75be\u75c5\u3002", "method": "\u4f7f\u7528\u4f18\u5316\u6280\u672f\u548c\u5206\u5b50\u6a21\u62df\uff0c\u4ece320\u4e07\u79cd\u53ef\u80fd\u5e8f\u5217\u4e2d\u7b5b\u9009\u51fa10\u79cd\u5019\u9009\u6291\u5236\u5242\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u51fa\u80fd\u7ed3\u5408KLVFF\u5e76\u963b\u65ad\u5176\u805a\u96c6\u7684\u80bd\u6bb5\u6291\u5236\u5242\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u7531\u86cb\u767d\u805a\u96c6\u5f15\u8d77\u7684\u75be\u75c5\uff0c\u5982\u5e15\u91d1\u68ee\u75c5\u3001\u4ea8\u5ef7\u987f\u75c5\u548c\u670a\u75c5\u6bd2\u75c5\u3002"}}
{"id": "2506.14120", "pdf": "https://arxiv.org/pdf/2506.14120", "abs": "https://arxiv.org/abs/2506.14120", "authors": ["Ziheng Chen", "Po T. Wang", "Mina Ibrahim", "Shivali Baveja", "Rong Mu", "An H. Do", "Zoran Nenadic"], "title": "Leveraging Transfer Learning and User-Specific Updates for Rapid Training of BCI Decoders", "categories": ["q-bio.QM"], "comment": "6 page conference proceeding preprint", "summary": "Lengthy subject- or session-specific data acquisition and calibration remain a key barrier to deploying electroencephalography (EEG)-based brain-computer interfaces (BCIs) outside the laboratory. Previous work has shown that cross subject, cross-session invariant features exist in EEG. We propose a transfer learning pipeline based on a two-layer convolutional neural network (CNN) that leverages these invariants to reduce the burden of data acquisition and calibration. A baseline model is trained on EEG data from five able-bodied individuals and then rapidly updated with a small amount of data from a sixth, holdout subject. The remaining holdout data were used to test the performance of both the baseline and updated models. We repeated this procedure via a leave-one-subject out (LOSO) validation framework. Averaged over six LOSO folds, the updated model improved classification accuracy upon the baseline by 10.0, 18.8, and 22.1 percentage points on two binary and one ternary classification tasks, respectively. These results demonstrate that decoding accuracy can be substantially improved with minimal subject-specific data. They also indicate that a CNN-based decoder can be personalized rapidly, enabling near plug-and-play BCI functionality for neurorehabilitation and other time-critical EEG applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e24\u5c42\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528EEG\u4e2d\u7684\u8de8\u4e3b\u4f53\u548c\u8de8\u4f1a\u8bdd\u4e0d\u53d8\u7279\u5f81\uff0c\u51cf\u5c11\u6570\u636e\u91c7\u96c6\u548c\u6821\u51c6\u8d1f\u62c5\u3002", "motivation": "\u89e3\u51b3EEG-BCI\u5728\u5b9e\u9a8c\u5ba4\u5916\u90e8\u7f72\u65f6\u56e0\u5197\u957f\u7684\u4e3b\u4f53\u6216\u4f1a\u8bdd\u7279\u5b9a\u6570\u636e\u91c7\u96c6\u548c\u6821\u51c6\u5e26\u6765\u7684\u969c\u788d\u3002", "method": "\u4f7f\u7528\u4e94\u540d\u5065\u5eb7\u4e2a\u4f53\u7684EEG\u6570\u636e\u8bad\u7ec3\u57fa\u7ebf\u6a21\u578b\uff0c\u7136\u540e\u7528\u5c11\u91cf\u7b2c\u516d\u540d\u53d7\u8bd5\u8005\u6570\u636e\u5feb\u901f\u66f4\u65b0\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u7559\u4e00\u53d7\u8bd5\u8005\u4ea4\u53c9\u9a8c\u8bc1\uff08LOSO\uff09\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5728\u516d\u4e2aLOSO\u6298\u53e0\u4e2d\uff0c\u66f4\u65b0\u6a21\u578b\u5728\u4e24\u9879\u4e8c\u5143\u548c\u4e00\u9879\u4e09\u5143\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e8610.0\u300118.8\u548c22.1\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u5c11\u91cf\u4e3b\u4f53\u7279\u5b9a\u6570\u636e\u53ef\u663e\u8457\u63d0\u9ad8\u89e3\u7801\u51c6\u786e\u7387\uff0c\u5b9e\u73b0\u8fd1\u4e4e\u5373\u63d2\u5373\u7528\u7684BCI\u529f\u80fd\uff0c\u9002\u7528\u4e8e\u795e\u7ecf\u5eb7\u590d\u7b49\u65f6\u95f4\u654f\u611f\u5e94\u7528\u3002"}}
{"id": "2506.14103", "pdf": "https://arxiv.org/pdf/2506.14103", "abs": "https://arxiv.org/abs/2506.14103", "authors": ["Rajitha Senanayake", "Pratheepa Jeganathan"], "title": "A Robust Nonparametric Framework for Detecting Repeated Spatial Patterns", "categories": ["stat.ME", "q-bio.QM"], "comment": "39 pages including an Appendix of 17 pages, 39 figures", "summary": "Identifying spatially contiguous clusters and repeated spatial patterns (RSP) characterized by similar underlying distributions that are spatially apart is a key challenge in modern spatial statistics. Existing constrained clustering methods enforce spatial contiguity but are limited in their ability to identify RSP. We propose a novel nonparametric framework that addresses this limitation by combining constrained clustering with a post-clustering reassigment step based on the maximum mean discrepancy (MMD) statistic. We employ a block permutation strategy within each cluster that preserves local attribute structure when approximating the null distribution of the MMD. We also show that the MMD$^2$ statistic is asymptotically consistent under second-order stationarity and spatial mixing conditions. This two-stage approach enables the detection of clusters that are both spatially distant and similar in distribution. Through simulation studies that vary spatial dependence, cluster sizes, shapes, and multivariate dimensionality, we demonstrate the robustness of our proposed framework in detecting RSP. We further illustrate its applicability through an analysis of spatial proteomics data from patients with triple-negative breast cancer. Overall, our framework presents a methodological advancement in spatial clustering, offering a flexible and robust solution for spatial datasets that exhibit repeated patterns.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ea6\u675f\u805a\u7c7b\u548c\u57fa\u4e8e\u6700\u5927\u5747\u503c\u5dee\u5f02\uff08MMD\uff09\u7684\u540e\u805a\u7c7b\u91cd\u5206\u914d\u6b65\u9aa4\u7684\u975e\u53c2\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u7a7a\u95f4\u4e0a\u5206\u6563\u4f46\u5206\u5e03\u76f8\u4f3c\u7684\u91cd\u590d\u7a7a\u95f4\u6a21\u5f0f\uff08RSP\uff09\u3002", "motivation": "\u73b0\u6709\u7ea6\u675f\u805a\u7c7b\u65b9\u6cd5\u867d\u80fd\u4fdd\u8bc1\u7a7a\u95f4\u8fde\u7eed\u6027\uff0c\u4f46\u96be\u4ee5\u8bc6\u522bRSP\uff0c\u9700\u4e00\u79cd\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7ea6\u675f\u805a\u7c7b\u786e\u4fdd\u7a7a\u95f4\u8fde\u7eed\u6027\uff0c\u540e\u805a\u7c7b\u57fa\u4e8eMMD\u7edf\u8ba1\u91cf\u91cd\u5206\u914d\u4ee5\u8bc6\u522b\u5206\u5e03\u76f8\u4f3c\u7684RSP\u3002\u4f7f\u7528\u5757\u7f6e\u6362\u7b56\u7565\u8fd1\u4f3cMMD\u7684\u96f6\u5206\u5e03\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7a7a\u95f4\u4f9d\u8d56\u3001\u7c07\u5927\u5c0f\u3001\u5f62\u72b6\u548c\u591a\u53d8\u91cf\u7ef4\u5ea6\u4e0b\u5747\u7a33\u5065\u3002\u5e94\u7528\u4e8e\u4e09\u9634\u6027\u4e73\u817a\u764c\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u6570\u636e\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7a7a\u95f4\u805a\u7c7b\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5177\u6709\u91cd\u590d\u6a21\u5f0f\u7684\u7a7a\u95f4\u6570\u636e\u96c6\u3002"}}
