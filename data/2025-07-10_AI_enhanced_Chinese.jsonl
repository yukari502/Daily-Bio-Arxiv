{"id": "2507.06280", "pdf": "https://arxiv.org/pdf/2507.06280", "abs": "https://arxiv.org/abs/2507.06280", "authors": ["Thibault Chassereau", "Florence Leclerc", "Eric Herbert"], "title": "Direct Evidence Of Apex Hypha Interactions During Vegetative Growth Of Fungal Thallus Via Comprehensive Network And Trajectory Extraction", "categories": ["q-bio.QM"], "comment": "5 pages, 2 figures, 1 table", "summary": "The mycelium of a filamentous fungus is a growing, branching network of\nnumerous entangled hyphae exhibiting polarised apical growth. Expansion occurs\nduring the vegetative phase from a single ascospore, driven by the need to\nexplore and occupy surrounding space limiting competitors, enhancing nutrient\nuptake, and promoting spore dispersal. Radial, rapid, and rectilinear growth\ncombined with frequent branching appears adaptive. However, passive growth\nwithout interactions or feedback may produce suboptimal networks, as neither\nlocal density nor potential connectivity is considered. Reorientations of the\napex near existing hyphae suggest apex hypha feedback. Yet, the diversity of\nbehaviours, spontaneous fluctuations, and limited apical trajectories studied\nleave open the question of active regulation. To investigate possible apex\nhypha interactions, we analyse a dataset of Podospora anserina thallus growth\nby reconstructing all apical trajectories postbranching and fitting them with a\nclassical Langevin model that incorporates potential interactions. Comparing\nisolated and nonisolated hyphae trajectories allows to identify a clear\nsignature of interaction composed of abrupt deceleration and reorientation.\nThis work opens the path towards a systematic exploration of hyphal\ninteractions.", "AI": {"tldr": "\u7814\u7a76\u771f\u83cc\u83cc\u4e1d\u751f\u957f\u4e2d\u7684\u9876\u7aef\u83cc\u4e1d\u76f8\u4e92\u4f5c\u7528\uff0c\u901a\u8fc7\u91cd\u5efa\u9876\u7aef\u8f68\u8ff9\u5e76\u62df\u5408Langevin\u6a21\u578b\uff0c\u53d1\u73b0\u660e\u663e\u7684\u76f8\u4e92\u4f5c\u7528\u7279\u5f81\u3002", "motivation": "\u63a2\u7d22\u771f\u83cc\u83cc\u4e1d\u751f\u957f\u4e2d\u662f\u5426\u5b58\u5728\u4e3b\u52a8\u8c03\u8282\u548c\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u4f18\u5316\u7f51\u7edc\u7ed3\u6784\u3002", "method": "\u91cd\u5efaPodospora anserina\u83cc\u4e1d\u9876\u7aef\u8f68\u8ff9\uff0c\u62df\u5408\u5305\u542b\u76f8\u4e92\u4f5c\u7528\u7684Langevin\u6a21\u578b\uff0c\u6bd4\u8f83\u5b64\u7acb\u4e0e\u975e\u5b64\u7acb\u83cc\u4e1d\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u9876\u7aef\u83cc\u4e1d\u76f8\u4e92\u4f5c\u7528\u8868\u73b0\u4e3a\u7a81\u7136\u51cf\u901f\u548c\u91cd\u65b0\u5b9a\u5411\u3002", "conclusion": "\u4e3a\u7cfb\u7edf\u7814\u7a76\u83cc\u4e1d\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.06336", "pdf": "https://arxiv.org/pdf/2507.06336", "abs": "https://arxiv.org/abs/2507.06336", "authors": ["Adam J Riesselman", "Evan M Cofer", "Therese LaRue", "Wim Meeussen"], "title": "Self-supervised learning predicts plant growth trajectories from multi-modal industrial greenhouse data", "categories": ["q-bio.QM", "cs.LG", "cs.RO"], "comment": null, "summary": "Quantifying organism-level phenotypes, such as growth dynamics and biomass\naccumulation, is fundamental to understanding agronomic traits and optimizing\ncrop production. However, quality growing data of plants at scale is difficult\nto generate. Here we use a mobile robotic platform to capture high-resolution\nenvironmental sensing and phenotyping measurements of a large-scale hydroponic\nleafy greens system. We describe a self-supervised modeling approach to build a\nmap from observed growing data to the entire plant growth trajectory. We\ndemonstrate our approach by forecasting future plant height and harvest mass of\ncrops in this system. This approach represents a significant advance in\ncombining robotic automation and machine learning, as well as providing\nactionable insights for agronomic research and operational efficiency.", "AI": {"tldr": "\u5229\u7528\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u548c\u9ad8\u5206\u8fa8\u7387\u73af\u5883\u4f20\u611f\u6280\u672f\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u5efa\u6a21\u65b9\u6cd5\uff0c\u9884\u6d4b\u690d\u7269\u751f\u957f\u8f68\u8ff9\uff0c\u4e3a\u519c\u4e1a\u7814\u7a76\u548c\u6548\u7387\u63d0\u5347\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "motivation": "\u91cf\u5316\u690d\u7269\u751f\u957f\u52a8\u6001\u548c\u751f\u7269\u91cf\u79ef\u7d2f\u5bf9\u7406\u89e3\u519c\u827a\u6027\u72b6\u548c\u4f18\u5316\u4f5c\u7269\u751f\u4ea7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u751f\u957f\u6570\u636e\u96be\u4ee5\u83b7\u53d6\u3002", "method": "\u4f7f\u7528\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u91c7\u96c6\u9ad8\u5206\u8fa8\u7387\u73af\u5883\u4f20\u611f\u548c\u8868\u578b\u6570\u636e\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u5efa\u6a21\u65b9\u6cd5\u6784\u5efa\u751f\u957f\u6570\u636e\u5230\u690d\u7269\u751f\u957f\u8f68\u8ff9\u7684\u6620\u5c04\u3002", "result": "\u6210\u529f\u9884\u6d4b\u4e86\u690d\u7269\u672a\u6765\u9ad8\u5ea6\u548c\u6536\u83b7\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u673a\u5668\u4eba\u81ea\u52a8\u5316\u548c\u673a\u5668\u5b66\u4e60\uff0c\u4e3a\u519c\u827a\u7814\u7a76\u548c\u64cd\u4f5c\u6548\u7387\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.06418", "pdf": "https://arxiv.org/pdf/2507.06418", "abs": "https://arxiv.org/abs/2507.06418", "authors": ["Changchun Yang", "Haoyang Li", "Yushuai Wu", "Yilan Zhang", "Yifeng Jiao", "Yu Zhang", "Rihan Huang", "Yuan Cheng", "Yuan Qi", "Xin Guo", "Xin Gao"], "title": "PAST: A multimodal single-cell foundation model for histopathology and spatial transcriptomics in cancer", "categories": ["q-bio.QM", "cs.CV", "stat.AP"], "comment": null, "summary": "While pathology foundation models have transformed cancer image analysis,\nthey often lack integration with molecular data at single-cell resolution,\nlimiting their utility for precision oncology. Here, we present PAST, a\npan-cancer single-cell foundation model trained on 20 million paired\nhistopathology images and single-cell transcriptomes spanning multiple tumor\ntypes and tissue contexts. By jointly encoding cellular morphology and gene\nexpression, PAST learns unified cross-modal representations that capture both\nspatial and molecular heterogeneity at the cellular level. This approach\nenables accurate prediction of single-cell gene expression, virtual molecular\nstaining, and multimodal survival analysis directly from routine pathology\nslides. Across diverse cancers and downstream tasks, PAST consistently exceeds\nthe performance of existing approaches, demonstrating robust generalizability\nand scalability. Our work establishes a new paradigm for pathology foundation\nmodels, providing a versatile tool for high-resolution spatial omics,\nmechanistic discovery, and precision cancer research.", "AI": {"tldr": "PAST\u662f\u4e00\u4e2a\u57fa\u4e8e\u75c5\u7406\u56fe\u50cf\u548c\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u6570\u636e\u7684\u8de8\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u7f16\u7801\u7ec6\u80de\u5f62\u6001\u548c\u57fa\u56e0\u8868\u8fbe\uff0c\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u7684\u7a7a\u95f4\u7ec4\u5b66\u548c\u7cbe\u51c6\u764c\u75c7\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684\u75c5\u7406\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u4e0e\u5355\u7ec6\u80de\u5206\u5b50\u6570\u636e\u7684\u6574\u5408\uff0c\u9650\u5236\u4e86\u5176\u5728\u7cbe\u51c6\u80bf\u7624\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "method": "PAST\u901a\u8fc7\u8bad\u7ec32000\u4e07\u5bf9\u75c5\u7406\u56fe\u50cf\u548c\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u6570\u636e\uff0c\u5b66\u4e60\u8de8\u6a21\u6001\u7684\u7edf\u4e00\u8868\u793a\u3002", "result": "PAST\u5728\u5355\u7ec6\u80de\u57fa\u56e0\u8868\u8fbe\u9884\u6d4b\u3001\u865a\u62df\u5206\u5b50\u67d3\u8272\u548c\u591a\u6a21\u6001\u751f\u5b58\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PAST\u4e3a\u75c5\u7406\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u652f\u6301\u9ad8\u5206\u8fa8\u7387\u7a7a\u95f4\u7ec4\u5b66\u548c\u7cbe\u51c6\u764c\u75c7\u7814\u7a76\u3002"}}
{"id": "2507.06853", "pdf": "https://arxiv.org/pdf/2507.06853", "abs": "https://arxiv.org/abs/2507.06853", "authors": ["Liang Wang", "Yu Rong", "Tingyang Xu", "Zhenyi Zhong", "Zhiyuan Liu", "Pengju Wang", "Deli Zhao", "Qiang Liu", "Shu Wu", "Liang Wang"], "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.chem-ph", "q-bio.MN"], "comment": null, "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.", "AI": {"tldr": "DiffSpectra\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u591a\u6a21\u6001\u5149\u8c31\u6570\u636e\u63a8\u65ad2D\u548c3D\u5206\u5b50\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u89e3\u91ca\u4e14\u7f3a\u4e4f\u6269\u5c55\u6027\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u6709\u9650\u5e93\u548c\u5ffd\u75653D\u51e0\u4f55\u4fe1\u606f\u3002DiffSpectra\u65e8\u5728\u901a\u8fc7\u751f\u6210\u6a21\u578b\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "DiffSpectra\u91c7\u7528\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408SE(3)-\u7b49\u53d8\u67b6\u6784\u7684\u6269\u6563\u5206\u5b50\u53d8\u6362\u5668\u548c\u57fa\u4e8eTransformer\u7684\u5149\u8c31\u7f16\u7801\u5668SpecFormer\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u5149\u8c31\u6570\u636e\u7684\u6761\u4ef6\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiffSpectra\u5728\u7ed3\u6784\u89e3\u6790\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u91c7\u6837\u5b9e\u73b0\u4e8616.01%\u7684top-1\u51c6\u786e\u7387\u548c96.86%\u7684top-20\u51c6\u786e\u7387\u3002", "conclusion": "DiffSpectra\u9996\u6b21\u7edf\u4e00\u4e86\u591a\u6a21\u6001\u5149\u8c31\u63a8\u7406\u548c2D/3D\u751f\u6210\u5efa\u6a21\uff0c\u4e3a\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06521", "pdf": "https://arxiv.org/pdf/2507.06521", "abs": "https://arxiv.org/abs/2507.06521", "authors": ["Belinda Neo", "Noel Nannup", "Dale Tilbrook", "Carol Michie", "Cindy Prior", "Eleanor Dunlop", "Brad Farrant", "Won Sun Chen", "Carrington C. J. Shepherd", "Lucinda J. Black"], "title": "Serum 25-hydroxyvitamin D concentration is not associated with mental health among Aboriginal and Torres Strait Islander Peoples in Australia: a cross-sectional exploratory study", "categories": ["q-bio.QM"], "comment": null, "summary": "Objective: To investigate the association between serum 25-hydroxyvitamin D\n[25(OH)D] concentration and mental health, measured using the Kessler\nPsychological Distress Scale 5 (K5), among Aboriginal and Torres Strait\nIslander Peoples. Methods: We used cross-sectional data from the 2012-2013\nAustralian Aboriginal and Torres Strait Islander Health Survey. Multiple linear\nregression was used to test the association between serum 25(OH)D concentration\nand K5, adjusting for age, sex, education, remoteness, socioeconomic status,\nseason of blood collection, smoking, and alcohol intake (n = 1,983). We also\nstratified the analysis by sex and by remoteness. Results: There was no\nstatistically significant association between serum 25(OH) concentration and K5\nin the total population, nor when stratified by sex. When stratified by\nremoteness, higher serum 25(OH)D concentration was statistically significantly\nassociated with lower K5 scores among those living remotely (adjusted \\b{eta}:\n-0.18; 95% CI: -0.35, -0.01). Conclusions: Serum 25(OH)D concentration was\ninversely associated with psychological distress only among those living\nremotely. Implications for Public Health: Given the prevalence of vitamin D\ndeficiency and the observed association between serum 25(OH)D concentration and\npsychological distress among Aboriginal and Torres Strait Islander Peoples\nliving remotely, public health strategies to improve vitamin D status among\nthis population group are warranted.", "AI": {"tldr": "\u7814\u7a76\u8840\u6e0525-\u7f9f\u57fa\u7ef4\u751f\u7d20D\u6d53\u5ea6\u4e0e\u5fc3\u7406\u5065\u5eb7\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u4ec5\u5728\u504f\u8fdc\u5730\u533a\u5c45\u6c11\u4e2d\u5b58\u5728\u663e\u8457\u8d1f\u76f8\u5173\u3002", "motivation": "\u63a2\u8ba8\u7ef4\u751f\u7d20D\u4e0e\u5fc3\u7406\u5065\u5eb7\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5bf9\u539f\u4f4f\u6c11\u548c\u6258\u96f7\u65af\u6d77\u5ce1\u5c9b\u6c11\u7684\u5f71\u54cd\u3002", "method": "\u5229\u75282012-2013\u5e74\u6fb3\u5927\u5229\u4e9a\u539f\u4f4f\u6c11\u548c\u6258\u96f7\u65af\u6d77\u5ce1\u5c9b\u6c11\u5065\u5eb7\u8c03\u67e5\u7684\u6a2a\u65ad\u9762\u6570\u636e\uff0c\u901a\u8fc7\u591a\u5143\u7ebf\u6027\u56de\u5f52\u5206\u6790\u8840\u6e0525(OH)D\u6d53\u5ea6\u4e0eK5\u5fc3\u7406\u56f0\u6270\u8bc4\u5206\u7684\u5173\u7cfb\u3002", "result": "\u603b\u4f53\u4eba\u7fa4\u53ca\u6309\u6027\u522b\u5206\u5c42\u540e\u65e0\u663e\u8457\u5173\u8054\uff0c\u4f46\u5728\u504f\u8fdc\u5730\u533a\u5c45\u6c11\u4e2d\uff0c\u8840\u6e0525(OH)D\u6d53\u5ea6\u4e0e\u5fc3\u7406\u56f0\u6270\u5448\u663e\u8457\u8d1f\u76f8\u5173\u3002", "conclusion": "\u7ef4\u751f\u7d20D\u6c34\u5e73\u4e0e\u5fc3\u7406\u5065\u5eb7\u7684\u5173\u8054\u4ec5\u5b58\u5728\u4e8e\u504f\u8fdc\u5730\u533a\u5c45\u6c11\uff0c\u63d0\u793a\u9700\u9488\u5bf9\u8be5\u4eba\u7fa4\u5236\u5b9a\u6539\u5584\u7ef4\u751f\u7d20D\u72b6\u6001\u7684\u516c\u5171\u536b\u751f\u7b56\u7565\u3002"}}
{"id": "2507.07060", "pdf": "https://arxiv.org/pdf/2507.07060", "abs": "https://arxiv.org/abs/2507.07060", "authors": ["Shreyas Vinaya Sathyanarayana", "Rahil Shah", "Sharanabasava D. Hiremath", "Rishikesh Panda", "Rahul Jana", "Riya Singh", "Rida Irfan", "Ashwin Murali", "Bharath Ramsundar"], "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG", "q-bio.BM", "q-bio.MN"], "comment": "51 pages,", "summary": "Retrosynthesis, the identification of precursor molecules for a target\ncompound, is pivotal for synthesizing complex molecules, but faces challenges\nin discovering novel pathways beyond predefined templates. Recent large\nlanguage model (LLM) approaches to retrosynthesis have shown promise but\neffectively harnessing LLM reasoning capabilities for effective multi-step\nplanning remains an open question. To address this challenge, we introduce\nDeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic\nframework. Our approach integrates the strengths of conventional\ntemplate-based/Monte Carlo tree search tools with the generative power of LLMs\nin a step-wise, feedback-driven loop. Initially, synthesis planning is\nattempted with a template-based engine. If this fails, the LLM subsequently\nproposes single-step retrosynthetic disconnections. Crucially, these\nsuggestions undergo rigorous validity, stability, and hallucination checks\nbefore the resulting precursors are recursively fed back into the pipeline for\nfurther evaluation. This iterative refinement allows for dynamic pathway\nexploration and correction. We demonstrate the potential of this pipeline\nthrough benchmark evaluations and case studies, showcasing its ability to\nidentify viable and potentially novel retrosynthetic routes. In particular, we\ndevelop an interactive graphical user interface that allows expert human\nchemists to provide human-in-the-loop feedback to the reasoning algorithm. This\napproach successfully generates novel pathways for complex natural product\ncompounds, demonstrating the potential for iterative LLM reasoning to advance\nstate-of-art in complex chemical syntheses.", "AI": {"tldr": "DeepRetro\u662f\u4e00\u4e2a\u7ed3\u5408\u4f20\u7edf\u6a21\u677f\u548cLLM\u7684\u8fed\u4ee3\u5f0f\u9006\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u5f84\u63a2\u7d22\u548c\u4fee\u6b63\uff0c\u5c55\u793a\u4e86\u751f\u6210\u65b0\u9896\u5408\u6210\u8def\u5f84\u7684\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u9006\u5408\u6210\u4e2d\u8d85\u8d8a\u9884\u5b9a\u4e49\u6a21\u677f\u53d1\u73b0\u65b0\u8def\u5f84\u7684\u6311\u6218\uff0c\u5145\u5206\u5229\u7528LLM\u7684\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u591a\u6b65\u89c4\u5212\u3002", "method": "\u7ed3\u5408\u6a21\u677f\u5f15\u64ce\u548cLLM\u7684\u751f\u6210\u80fd\u529b\uff0c\u901a\u8fc7\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u5faa\u73af\uff0c\u8fdb\u884c\u8def\u5f84\u63a2\u7d22\u548c\u4fee\u6b63\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u6848\u4f8b\u7814\u7a76\u4e2d\u5c55\u793a\u4e86\u751f\u6210\u53ef\u884c\u4e14\u65b0\u9896\u7684\u9006\u5408\u6210\u8def\u5f84\u7684\u80fd\u529b\u3002", "conclusion": "DeepRetro\u901a\u8fc7\u8fed\u4ee3LLM\u63a8\u7406\u548c\u4eba\u7c7b\u4e13\u5bb6\u53cd\u9988\uff0c\u6709\u671b\u63a8\u52a8\u590d\u6742\u5316\u5b66\u5408\u6210\u7684\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2507.06433", "pdf": "https://arxiv.org/pdf/2507.06433", "abs": "https://arxiv.org/abs/2507.06433", "authors": ["Niloy Sikder", "Paul Zerr", "Mahdad Jafarzadeh Esfahani", "Martin Dresler", "Matthias Krauledat"], "title": "eegFloss: A Python package for refining sleep EEG recordings using machine learning models", "categories": ["cs.LG", "eess.SP", "q-bio.QM"], "comment": "The eegFloss package is available under the MIT License at\n  https://github.com/Niloy333/eegFloss", "summary": "Electroencephalography (EEG) allows monitoring of brain activity, providing\ninsights into the functional dynamics of various brain regions and their roles\nin cognitive processes. EEG is a cornerstone in sleep research, serving as the\nprimary modality of polysomnography, the gold standard in the field. However,\nEEG signals are prone to artifacts caused by both internal (device-specific)\nfactors and external (environmental) interferences. As sleep studies are\nbecoming larger, most rely on automatic sleep staging, a process highly\nsusceptible to artifacts, leading to erroneous sleep scores. This paper\naddresses this challenge by introducing eegFloss, an open-source Python package\nto utilize eegUsability, a novel machine learning (ML) model designed to detect\nsegments with artifacts in sleep EEG recordings. eegUsability has been trained\nand evaluated on manually artifact-labeled EEG data collected from 15\nparticipants over 127 nights using the Zmax headband. It demonstrates solid\noverall classification performance (F1-score is approximately 0.85, Cohens\nkappa is 0.78), achieving a high recall rate of approximately 94% in\nidentifying channel-wise usable EEG data, and extends beyond Zmax.\nAdditionally, eegFloss offers features such as automatic time-in-bed detection\nusing another ML model named eegMobility, filtering out certain artifacts, and\ngenerating hypnograms and sleep statistics. By addressing a fundamental\nchallenge faced by most sleep studies, eegFloss can enhance the precision and\nrigor of their analysis as well as the accuracy and reliability of their\noutcomes.", "AI": {"tldr": "eegFloss\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5305\uff0c\u7528\u4e8e\u68c0\u6d4b\u7761\u7720EEG\u8bb0\u5f55\u4e2d\u7684\u4f2a\u5f71\uff0c\u63d0\u9ad8\u7761\u7720\u7814\u7a76\u7684\u51c6\u786e\u6027\u3002", "motivation": "EEG\u4fe1\u53f7\u6613\u53d7\u4f2a\u5f71\u5e72\u6270\uff0c\u5f71\u54cd\u81ea\u52a8\u7761\u7720\u5206\u671f\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u68c0\u6d4b\u548c\u8fc7\u6ee4\u4f2a\u5f71\u3002", "method": "\u5f00\u53d1\u4e86eegUsability\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u57fa\u4e8e\u624b\u52a8\u6807\u8bb0\u7684EEG\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5e76\u96c6\u6210\u5230eegFloss\u5305\u4e2d\u3002", "result": "eegUsability\u8868\u73b0\u51fa\u8272\uff08F1\u5206\u6570\u7ea60.85\uff0cCohen's kappa\u4e3a0.78\uff09\uff0c\u80fd\u9ad8\u53ec\u56de\u7387\uff0894%\uff09\u8bc6\u522b\u53ef\u7528EEG\u6570\u636e\u3002", "conclusion": "eegFloss\u901a\u8fc7\u68c0\u6d4b\u4f2a\u5f71\u548c\u63d0\u4f9b\u9644\u52a0\u529f\u80fd\uff0c\u63d0\u5347\u4e86\u7761\u7720\u7814\u7a76\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.07032", "pdf": "https://arxiv.org/pdf/2507.07032", "abs": "https://arxiv.org/abs/2507.07032", "authors": ["Hanqun Cao", "Xinyi Zhou", "Zijun Gao", "Chenyu Wang", "Xin Gao", "Zhi Zhang", "Chunbin Gu", "Ge Liu", "Pheng-Ann Heng"], "title": "PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Protein structure prediction is essential for drug discovery and\nunderstanding biological functions. While recent advancements like AlphaFold\nhave achieved remarkable accuracy, most folding models rely heavily on multiple\nsequence alignments (MSAs) to boost prediction performance. This dependency\nlimits their effectiveness on low-homology proteins and orphan proteins, where\nMSA information is sparse or unavailable. To address this limitation, we\npropose PLAME, a novel MSA design model that leverages evolutionary embeddings\nfrom pretrained protein language models. Unlike existing methods, PLAME\nintroduces pretrained representations to enhance evolutionary information and\nemploys a conservation-diversity loss to enhance generation quality.\nAdditionally, we propose a novel MSA selection method to effectively screen\nhigh-quality MSAs and improve folding performance. We also propose a sequence\nquality assessment metric that provides an orthogonal perspective to evaluate\nMSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,\nPLAME achieves state-of-the-art performance in folding enhancement and sequence\nquality assessment, with consistent improvements demonstrated on AlphaFold3.\nAblation studies validate the effectiveness of the MSA selection method, while\nextensive case studies on various protein types provide insights into the\nrelationship between AlphaFold's prediction quality and MSA characteristics.\nFurthermore, we demonstrate that PLAME can serve as an adapter achieving\nAlphaFold2-level accuracy with the ESMFold's inference speed.", "AI": {"tldr": "PLAME\u662f\u4e00\u4e2a\u65b0\u7684MSA\u8bbe\u8ba1\u6a21\u578b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5316\u5d4c\u5165\uff0c\u63d0\u5347\u4f4e\u540c\u6e90\u6027\u548c\u5b64\u513f\u86cb\u767d\u8d28\u7684\u7ed3\u6784\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6298\u53e0\u6a21\u578b\u4f9d\u8d56\u591a\u5e8f\u5217\u6bd4\u5bf9\uff08MSA\uff09\uff0c\u4f46\u5728\u4f4e\u540c\u6e90\u6027\u548c\u5b64\u513f\u86cb\u767d\u8d28\u4e0a\u6548\u679c\u6709\u9650\uff0cPLAME\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "PLAME\u5f15\u5165\u9884\u8bad\u7ec3\u8868\u793a\u589e\u5f3a\u8fdb\u5316\u4fe1\u606f\uff0c\u91c7\u7528\u4fdd\u5b88-\u591a\u6837\u6027\u635f\u5931\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u65b0\u7684MSA\u7b5b\u9009\u65b9\u6cd5\u548c\u5e8f\u5217\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728AlphaFold2\u548cAlphaFold3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPLAME\u5728\u6298\u53e0\u589e\u5f3a\u548c\u5e8f\u5217\u8d28\u91cf\u8bc4\u4f30\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "PLAME\u6709\u6548\u63d0\u5347\u4f4e\u540c\u6e90\u6027\u548c\u5b64\u513f\u86cb\u767d\u8d28\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u53ef\u4f5c\u4e3a\u9002\u914d\u5668\u5b9e\u73b0AlphaFold2\u7ea7\u7cbe\u5ea6\u4e0eESMFold\u63a8\u7406\u901f\u5ea6\u3002"}}
