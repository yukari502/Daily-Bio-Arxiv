{"id": "2507.14269", "pdf": "https://arxiv.org/pdf/2507.14269", "abs": "https://arxiv.org/abs/2507.14269", "authors": ["Kunyang Li", "Hongfu Lou", "Dinan Peng"], "title": "MP-GCAN: a highly accurate classifier for $α$-helical membrane proteins and $β$-barrel proteins", "categories": ["q-bio.QM"], "comment": "8pages,4figures", "summary": "Membrane protein classification is a fundamental task in structural\nbioinformatics, critical to understanding protein functions and accelerating\ndrug discovery. In this study, we propose MP-GCAN, a novel graph-based\nclassification model that leverages both spatial and sequential features of\nproteins. MP-GCAN combines GCN, GAT, and GIN layers to capture hierarchical\nstructural representations from 3D protein graphs, constructed from\nhigh-resolution PDB files with $\\alpha$-carbon coordinates and residue types.\nTo evaluate performance, we curated a high-quality dataset of 500 membrane and\n500 non-membrane proteins, and compared MP-GCAN with two baselines: a\nstructure-confidence-based SGD classifier utilizing AlphaFold's pLDDT scores,\nand DeepTMHMM, a sequence-based deep learning model. Our experiments\ndemonstrate that MP-GCAN significantly outperforms baselines, achieving an\naccuracy of 96% and strong F1-scores on both classes. The results highlight the\nimportance of integrating pretrained GNN architectures with domain-specific\nstructural data to enhance membrane protein classification."}
{"id": "2507.14639", "pdf": "https://arxiv.org/pdf/2507.14639", "abs": "https://arxiv.org/abs/2507.14639", "authors": ["Saleh Alwer", "Ronan Fleming"], "title": "KinForm: Kinetics Informed Feature Optimised Representation Models for Enzyme $k_{cat}$ and $K_{M}$ Prediction", "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "Kinetic parameters such as the turnover number ($k_{cat}$) and Michaelis\nconstant ($K_{\\mathrm{M}}$) are essential for modelling enzymatic activity but\nexperimental data remains limited in scale and diversity. Previous methods for\npredicting enzyme kinetics typically use mean-pooled residue embeddings from a\nsingle protein language model to represent the protein. We present KinForm, a\nmachine learning framework designed to improve predictive accuracy and\ngeneralisation for kinetic parameters by optimising protein feature\nrepresentations. KinForm combines several residue-level embeddings\n(Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and\nProtT5-XL-UniRef50), taken from empirically selected intermediate transformer\nlayers and applies weighted pooling based on per-residue binding-site\nprobability. To counter the resulting high dimensionality, we apply\ndimensionality reduction using principal--component analysis (PCA) on\nconcatenated protein features, and rebalance the training data via a\nsimilarity-based oversampling strategy. KinForm outperforms baseline methods on\ntwo benchmark datasets. Improvements are most pronounced in low sequence\nsimilarity bins. We observe improvements from binding-site probability pooling,\nintermediate-layer selection, PCA, and oversampling of low-identity proteins.\nWe also find that removing sequence overlap between folds provides a more\nrealistic evaluation of generalisation and should be the standard over random\nsplitting when benchmarking kinetic prediction models."}
{"id": "2507.15651", "pdf": "https://arxiv.org/pdf/2507.15651", "abs": "https://arxiv.org/abs/2507.15651", "authors": ["Lorenzo Rosset", "Martin Weigt", "Francesco Zamponi"], "title": "Data augmentation enables label-specific generation of homologous protein sequences", "categories": ["q-bio.QM", "cond-mat.dis-nn"], "comment": "13 pages, 4 figures", "summary": "Accurately annotating and controlling protein function from sequence data\nremains a major challenge, particularly within homologous families where\nannotated sequences are scarce and structural variation is minimal. We present\na two-stage approach for semi-supervised functional annotation and conditional\nsequence generation in protein families using representation learning. First,\nwe demonstrate that protein language models, pretrained on large and diverse\nsequence datasets and possibly finetuned via contrastive learning, provide\nembeddings that robustly capture fine-grained functional specificities, even\nwith limited labeled data. Second, we use the inferred annotations to train a\ngenerative probabilistic model, an annotation-aware Restricted Boltzmann\nMachine, capable of producing synthetic sequences with prescribed functional\nlabels. Across several protein families, we show that this approach achieves\nhighly accurate annotation quality and supports the generation of functionally\ncoherent sequences. Our findings underscore the power of combining\nself-supervised learning with light supervision to overcome data scarcity in\nprotein function prediction and design."}
{"id": "2507.14368", "pdf": "https://arxiv.org/pdf/2507.14368", "abs": "https://arxiv.org/abs/2507.14368", "authors": ["Praneeth Namburi", "Roger Pallarès-López", "Jessica Rosendorf", "Duarte Folgado", "Brian W. Anthony"], "title": "DUSTrack: Semi-automated point tracking in ultrasound videos", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Ultrasound technology enables safe, non-invasive imaging of dynamic tissue\nbehavior, making it a valuable tool in medicine, biomechanics, and sports\nscience. However, accurately tracking tissue motion in B-mode ultrasound\nremains challenging due to speckle noise, low edge contrast, and out-of-plane\nmovement. These challenges complicate the task of tracking anatomical landmarks\nover time, which is essential for quantifying tissue dynamics in many clinical\nand research applications. This manuscript introduces DUSTrack (Deep learning\nand optical flow-based toolkit for UltraSound Tracking), a semi-automated\nframework for tracking arbitrary points in B-mode ultrasound videos. We combine\ndeep learning with optical flow to deliver high-quality and robust tracking\nacross diverse anatomical structures and motion patterns. The toolkit includes\na graphical user interface that streamlines the generation of high-quality\ntraining data and supports iterative model refinement. It also implements a\nnovel optical-flow-based filtering technique that reduces high-frequency\nframe-to-frame noise while preserving rapid tissue motion. DUSTrack\ndemonstrates superior accuracy compared to contemporary zero-shot point\ntrackers and performs on par with specialized methods, establishing its\npotential as a general and foundational tool for clinical and biomechanical\nresearch. We demonstrate DUSTrack's versatility through three use cases:\ncardiac wall motion tracking in echocardiograms, muscle deformation analysis\nduring reaching tasks, and fascicle tracking during ankle plantarflexion. As an\nopen-source solution, DUSTrack offers a powerful, flexible framework for point\ntracking to quantify tissue motion from ultrasound videos. DUSTrack is\navailable at https://github.com/praneethnamburi/DUSTrack."}
{"id": "2507.15050", "pdf": "https://arxiv.org/pdf/2507.15050", "abs": "https://arxiv.org/abs/2507.15050", "authors": ["Iman Tavassoly", "Adel Mehrpooya", "Parsa Mirlohi", "Zahra Abbaspourasadollah"], "title": "Systems-Level Analysis of Multisite Protein Phosphorylation: Mathematical Induction, Geometric Series, and Entropy", "categories": ["q-bio.MN", "q-bio.QM"], "comment": null, "summary": "Multisite protein phosphorylation plays a pivotal role in regulating cellular\nsignaling and decision-making processes. In this study, we focus on the\nmathematical underpinnings and informational aspects of sequential,\ndistributive phosphorylation systems. We first provide rigorous steady-state\nsolutions derived using geometric series arguments and formal mathematical\ninduction, demonstrating that the distribution of phosphorylation states\nfollows a geometric progression determined by the kinase-to-phosphatase\nactivity ratio. We then extend the analysis with entropy-based insights,\nquantifying uncertainty in phosphorylation states and examining the mutual\ninformation between kinase activity and phosphorylation levels through a\ntruncated Poisson model. These results highlight how phosphorylation dynamics\nintroduce both structured patterns and inherent signal variability. By\ncombining exact mathematical proofs with entropy analysis, this work clarifies\nkey quantitative features of multisite phosphorylation from a systems-level\nperspective."}
{"id": "2507.15620", "pdf": "https://arxiv.org/pdf/2507.15620", "abs": "https://arxiv.org/abs/2507.15620", "authors": ["Qipeng Wang", "Shaolun Ruan", "Rui Sheng", "Yong Wang", "Min Zhu", "Huamin Qu"], "title": "TrajLens: Visual Analysis for Constructing Cell Developmental Trajectories in Cross-Sample Exploration", "categories": ["cs.CG", "q-bio.QM"], "comment": null, "summary": "Constructing cell developmental trajectories is a critical task in\nsingle-cell RNA sequencing (scRNA-seq) analysis, enabling the inference of\npotential cellular progression paths. However, current automated methods are\nlimited to establishing cell developmental trajectories within individual\nsamples, necessitating biologists to manually link cells across samples to\nconstruct complete cross-sample evolutionary trajectories that consider\ncellular spatial dynamics. This process demands substantial human effort due to\nthe complex spatial correspondence between each pair of samples. To address\nthis challenge, we first proposed a GNN-based model to predict cross-sample\ncell developmental trajectories. We then developed TrajLens, a visual analytics\nsystem that supports biologists in exploring and refining the cell\ndevelopmental trajectories based on predicted links. Specifically, we designed\nthe visualization that integrates features on cell distribution and\ndevelopmental direction across multiple samples, providing an overview of the\nspatial evolutionary patterns of cell populations along trajectories.\nAdditionally, we included contour maps superimposed on the original cell\ndistribution data, enabling biologists to explore them intuitively. To\ndemonstrate our system's performance, we conducted quantitative evaluations of\nour model with two case studies and expert interviews to validate its\nusefulness and effectiveness."}
