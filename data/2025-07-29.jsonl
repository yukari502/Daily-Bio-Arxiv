{"id": "2507.19553", "pdf": "https://arxiv.org/pdf/2507.19553", "abs": "https://arxiv.org/abs/2507.19553", "authors": ["Haizhao Guan", "Yiyuan Niu", "Chuanjin Zu", "Ju Kang"], "title": "Theoretical modeling and quantitative research on aquatic ecosystems driven by multiple factors", "categories": ["q-bio.QM"], "comment": "9 pages, 7 figures", "summary": "Understanding the complex interactions between water temperature, nutrient\nlevels, and chlorophyll-a dynamics is essential for addressing eutrophication\nand the proliferation of harmful algal blooms in freshwater ecosystems algal.\nHowever, many existing studies tend to oversimplify thse relationships often\nneglecting the non-linear effects and long-term temporal variations that\ninfluence chlorophyll-a growth. Here, we conducted multi-year field monitoring\n(2020-2024) of the key environmental factors, including total nitrogen (TN),\ntotal phosphorus (TP), water temperature, and chlorophyll-a, across three water\nbodies in Guangdong Province, China: Tiantangshan Reservoir(S1), Baisha River\nReservoir(S2) and Meizhou Reservoir(S3). Based on the collected data, we\ndeveloped a multi-factor interaction model to quantitatively assess the\nspatiotemporal dynamics of chlorophyll-a and its environmental drivers. Our\nresearch reveal significant temporal and spatial variability in chlorophyll-a\nconcentrations, with strong positive correlations to TN, TP, and water\ntemperature. Long-term data from S1 and S2 demonstrate a clear trend of\nincreasing eutrophication, with TN emerging as a more influential factor than\nTP in chlorophyll-a proliferation. The developed model accurately reproduces\nobserved patterns, offering a robust theoretical basis for future predictive\nand management-oriented studies of aquatic ecosystem health."}
{"id": "2507.19565", "pdf": "https://arxiv.org/pdf/2507.19565", "abs": "https://arxiv.org/abs/2507.19565", "authors": ["Brady K. Zhou", "Jason J. Hu", "Jane K. J. Lee", "Z. Hong Zhou", "Demetri Terzopoulos"], "title": "Review of Deep Learning Applications to Structural Proteomics Enabled by Cryogenic Electron Microscopy and Tomography", "categories": ["q-bio.QM", "cs.CV", "cs.LG"], "comment": "16 pages", "summary": "The past decade's \"cryoEM revolution\" has produced exponential growth in\nhigh-resolution structural data through advances in cryogenic electron\nmicroscopy (cryoEM) and tomography (cryoET). Deep learning integration into\nstructural proteomics workflows addresses longstanding challenges including low\nsignal-to-noise ratios, preferred orientation artifacts, and missing-wedge\nproblems that historically limited efficiency and scalability. This review\nexamines AI applications across the entire cryoEM pipeline, from automated\nparticle picking using convolutional neural networks (Topaz, crYOLO,\nCryoSegNet) to computational solutions for preferred orientation bias\n(spIsoNet, cryoPROS) and advanced denoising algorithms (Topaz-Denoise). In\ncryoET, tools like IsoNet employ U-Net architectures for simultaneous\nmissing-wedge correction and noise reduction, while TomoNet streamlines\nsubtomogram averaging through AI-driven particle detection. The workflow\nculminates with automated atomic model building using sophisticated tools like\nModelAngelo, DeepTracer, and CryoREAD that translate density maps into\ninterpretable biological structures. These AI-enhanced approaches have achieved\nnear-atomic resolution reconstructions with minimal manual intervention,\nresolved previously intractable datasets suffering from severe orientation\nbias, and enabled successful application to diverse biological systems from HIV\nvirus-like particles to in situ ribosomal complexes. As deep learning evolves,\nparticularly with large language models and vision transformers, the future\npromises sophisticated automation and accessibility in structural biology,\npotentially revolutionizing our understanding of macromolecular architecture\nand function."}
{"id": "2507.19979", "pdf": "https://arxiv.org/pdf/2507.19979", "abs": "https://arxiv.org/abs/2507.19979", "authors": ["Boseung Choi", "Hey-Won Kang", "Grzegorz A. Rempala"], "title": "Inference for stochastic reaction networks via logistic regression", "categories": ["q-bio.QM", "62P10, 92C60, 60J28 (Primary) 62F15, 37N25, 92C45 (Secondary)"], "comment": "44 pages, 14 figures", "summary": "Identifying network structure and inferring parameters are central challenges\nin modeling chemical reaction networks. In this study, we propose\nlikelihood-based methods grounded in multinomial logistic regression to infer\nboth stoichiometries and network connectivity structure from full time-series\ntrajectories of stochastic chemical reaction networks. When complete molecular\ncount trajectories are observed for all species, stoichiometric coefficients\nare identifiable, provided each reaction occurs at least once during the\nobservation window. However, identifying catalytic species remains difficult,\nas their molecular counts remain unchanged before and after each reaction\nevent. Through three illustrative stochastic models involving catalytic\ninteractions in open networks, we demonstrate that the logistic regression\nframework, when applied properly, can recover the full network structure,\nincluding stoichiometric relationships. We further apply Bayesian logistic\nregression to estimate model parameters in real-world epidemic settings, using\nthe COVID-19 outbreak in the Greater Seoul area of South Korea as a case study.\nOur analysis focuses on a Susceptible--Infected--Recovered (SIR) network model\nthat incorporates demographic effects. To address the challenge of partial\nobservability, particularly the availability of data only for the infectious\nsubset of the population, we develop a method that integrates Bayesian logistic\nregression with differential equation models. This approach enables robust\ninference of key SIR parameters from observed COVID-19 case trajectories.\nOverall, our findings demonstrate that simple, likelihood-based techniques such\nas logistic regression can recover meaningful mechanistic insights from both\nsynthetic and empirical time-series data."}
{"id": "2507.20401", "pdf": "https://arxiv.org/pdf/2507.20401", "abs": "https://arxiv.org/abs/2507.20401", "authors": ["Anna A. Andreeva", "Konstantin A. Klochkov", "Alexey I. Lobanov"], "title": "Mathematical model of blood coagulation during endovenous laser therapy", "categories": ["q-bio.QM", "92-10 (Primary) 92C45, 80A30, 34A12 (Secondary)"], "comment": null, "summary": "Endovenous laser therapy (ELT) as a minimally invasive procedure for ablation\nof large superficial veins, nevertheless, can cause complications of thrombotic\nnature. In this regard, the study of the main patterns of thrombus formation\nduring ELT and modelling of endovenous heat-induced thrombosis (EHIT) is\nrelevant. Based on the assumption of diffusion limiting of biochemical\nprocesses occurring during the coagulation of blood, by recalculating the\nreaction rates according to the Stokes-Einstein equation, a simple point model\nof blood coagulation during ELT was built in this paper. As a result of the use\nof this model, it was demonstrated that blood heating entails an increase in\nthe rate of thrombin production, a decrease in the time for achieving the peak\nof its concentration by 5-6 times with its almost constant amplitude. Heating\nleads to the rapid formation of fibrin clusters and the appearance of a\nfibrin-polymer network with a smaller cell size. The quantitative dependence on\nthe selected rheological model was also shown. All the data necessary for using\nthe model are given in this article for reproducibility."}
{"id": "2507.20406", "pdf": "https://arxiv.org/pdf/2507.20406", "abs": "https://arxiv.org/abs/2507.20406", "authors": ["Justin Boone"], "title": "A Topology-Based Machine Learning Model Decisively Outperforms Flux Balance Analysis in Predicting Metabolic Gene Essentiality", "categories": ["q-bio.MN"], "comment": null, "summary": "Background: The rational identification of essential genes is a cornerstone\nof drug discovery, yet standard computational methods like Flux Balance\nAnalysis (FBA) often struggle to produce accurate predictions in complex,\nredundant metabolic networks. Hypothesis: We hypothesized that the topological\nstructure of a metabolic network contains a more robust predictive signal for\nessentiality than functional simulations alone. Methodology: To test this\nhypothesis, we developed a machine learning pipeline by first constructing a\nreaction-reaction graph from the e_coli_core metabolic model. Graph-theoretic\nfeatures, including betweenness centrality and PageRank, were engineered to\ndescribe the topological role of each gene. A RandomForestClassifier was\ntrained on these features, and its performance was rigorously benchmarked\nagainst a standard FBA single-gene deletion analysis using a curated\nground-truth dataset. Results: Our machine learning model achieved a solid\npredictive performance with an F1-Score of 0.400 (Precision: 0.412, Recall:\n0.389). In profound contrast, the standard FBA baseline method failed to\ncorrectly identify any of the known essential genes, resulting in an F1-Score\nof 0.000. Conclusion: This work demonstrates that a \"structure-first\" machine\nlearning approach is a significantly superior strategy for predicting gene\nessentiality compared to traditional FBA on the E. coli core network. By\nlearning the topological signatures of critical network roles, our model\nsuccessfully overcomes the known limitations of simulation-based methods in\nhandling biological redundancy. While the performance of topology-only models\nis expected to face challenges on more complex genome-scale networks, this\nvalidated framework represents a significant step forward and highlights the\nprimacy of network architecture in determining biological function."}
{"id": "2507.19637", "pdf": "https://arxiv.org/pdf/2507.19637", "abs": "https://arxiv.org/abs/2507.19637", "authors": ["Reza Ahmadi", "Shahram Rasoulian", "Hamidreza Heidary", "Saied Jalal Aboodarda", "Thomas K. Uchida", "Walter Herzog", "Amin Komeili"], "title": "Quantifying lower-limb muscle coordination during cycling using electromyography-informed muscle synergies", "categories": ["physics.med-ph", "q-bio.QM"], "comment": "21 pages, 6 figures", "summary": "Assessment of muscle coordination during cycling may provide insight into\nmotor control strategies and movement efficiency. This study evaluated muscle\nsynergies and coactivation patterns as indicators of neuromuscular coordination\nin lower-limb across three power levels of cycling. Twenty recreational\ncyclists performed a graded cycling test on a stationary bicycle ergometer.\nElectromyography was recorded bilaterally from seven lower-limb muscles and\nmuscle synergies were extracted using non-negative matrix factorization. The\nCoactivation Index (CI), Synergy Index (SI), and Synergy Coordination Index\n(SCI) were calculated to assess muscle coordination patterns. Four muscle\nsynergies were identified consistently across power levels, with changes in\nsynergy composition and activation timing correlated with increased muscular\ndemands. As power level increased, the CI showed reduced muscle coactivation at\nthe knee and greater muscle coactivation at the ankle. The SI revealed a\ngreater contribution of the synergy weights of the extensor muscles than those\nof the flexor muscles at the knee. In contrast, the relative EMG contribution\nof hip extensor and flexor muscles remained consistent with increasing power\nlevels. The SCI increased significantly with increasing power level, suggesting\na reduction in the size of the synergy space and improved neuromuscular\ncoordination. These findings provide insight into how the central nervous\nsystem modulates its response to increasing mechanical demands. Combining\nsynergy and coactivation indices offers a promising approach to assess motor\ncontrol, inform rehabilitation, and optimize performance in cycling tasks."}
{"id": "2507.19734", "pdf": "https://arxiv.org/pdf/2507.19734", "abs": "https://arxiv.org/abs/2507.19734", "authors": ["Qinlong Li", "Pu Sun", "Guanlin Zhu", "Tianjiao Liang", "Honggang QI"], "title": "A Metabolic-Imaging Integrated Model for Prognostic Prediction in Colorectal Liver Metastases", "categories": ["eess.IV", "cs.CV", "cs.LG", "q-bio.QM"], "comment": "8 pages,4 figues", "summary": "Prognostic evaluation in patients with colorectal liver metastases (CRLM)\nremains challenging due to suboptimal accuracy of conventional clinical models.\nThis study developed and validated a robust machine learning model for\npredicting postoperative recurrence risk. Preliminary ensemble models achieved\nexceptionally high performance (AUC $>$ 0.98) but incorporated postoperative\nfeatures, introducing data leakage risks. To enhance clinical applicability, we\nrestricted input variables to preoperative baseline clinical parameters and\nradiomic features from contrast-enhanced CT imaging, specifically targeting\nrecurrence prediction at 3, 6, and 12 months postoperatively. The 3-month\nrecurrence prediction model demonstrated optimal performance with an AUC of\n0.723 in cross-validation. Decision curve analysis revealed that across\nthreshold probabilities of 0.55-0.95, the model consistently provided greater\nnet benefit than \"treat-all\" or \"treat-none\" strategies, supporting its utility\nin postoperative surveillance and therapeutic decision-making. This study\nsuccessfully developed a robust predictive model for early CRLM recurrence with\nconfirmed clinical utility. Importantly, it highlights the critical risk of\ndata leakage in clinical prognostic modeling and proposes a rigorous framework\nto mitigate this issue, enhancing model reliability and translational value in\nreal-world settings."}
{"id": "2507.19755", "pdf": "https://arxiv.org/pdf/2507.19755", "abs": "https://arxiv.org/abs/2507.19755", "authors": ["Ziqi Zhang", "Shiheng Chen", "Runze Yang", "Zhisheng Wei", "Wei Zhang", "Lei Wang", "Zhanzhi Liu", "Fengshan Zhang", "Jing Wu", "Xiaoyong Pan", "Hongbin Shen", "Longbing Cao", "Zhaohong Deng"], "title": "Modeling enzyme temperature stability from sequence segment perspective", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "comment": null, "summary": "Developing enzymes with desired thermal properties is crucial for a wide\nrange of industrial and research applications, and determining temperature\nstability is an essential step in this process. Experimental determination of\nthermal parameters is labor-intensive, time-consuming, and costly. Moreover,\nexisting computational approaches are often hindered by limited data\navailability and imbalanced distributions. To address these challenges, we\nintroduce a curated temperature stability dataset designed for model\ndevelopment and benchmarking in enzyme thermal modeling. Leveraging this\ndataset, we present the \\textit{Segment Transformer}, a novel deep learning\nframework that enables efficient and accurate prediction of enzyme temperature\nstability. The model achieves state-of-the-art performance with an RMSE of\n24.03, MAE of 18.09, and Pearson and Spearman correlations of 0.33,\nrespectively. These results highlight the effectiveness of incorporating\nsegment-level representations, grounded in the biological observation that\ndifferent regions of a protein sequence contribute unequally to thermal\nbehavior. As a proof of concept, we applied the Segment Transformer to guide\nthe engineering of a cutinase enzyme. Experimental validation demonstrated a\n1.64-fold improvement in relative activity following heat treatment, achieved\nthrough only 17 mutations and without compromising catalytic function."}
{"id": "2507.20288", "pdf": "https://arxiv.org/pdf/2507.20288", "abs": "https://arxiv.org/abs/2507.20288", "authors": ["Tyler Cassidy", "Stuart T. Johnston", "Michael Plank", "Imke Botha", "Jennifer A. Flegg", "Ryan J. Murphy", "Sara Hamis"], "title": "A nonparametric approach to practical identifiability of nonlinear mixed effects models", "categories": ["stat.ME", "q-bio.QM"], "comment": null, "summary": "Mathematical modelling is a widely used approach to understand and interpret\nclinical trial data. This modelling typically involves fitting mechanistic\nmathematical models to data from individual trial participants. Despite the\nwidespread adoption of this individual-based fitting, it is becoming\nincreasingly common to take a hierarchical approach to parameter estimation,\nwhere modellers characterize the population parameter distributions, rather\nthan considering each individual independently. This hierarchical parameter\nestimation is standard in pharmacometric modelling. However, many of the\nexisting techniques for parameter identifiability do not immediately translate\nfrom the individual-based fitting to the hierarchical setting. Here, we propose\na nonparametric approach to study practical identifiability within a\nhierarchical parameter estimation framework. We focus on the commonly used\nnonlinear mixed effects framework and investigate two well-studied examples\nfrom the pharmacometrics and viral dynamics literature to illustrate the\npotential utility of our approach."}
{"id": "2507.20714", "pdf": "https://arxiv.org/pdf/2507.20714", "abs": "https://arxiv.org/abs/2507.20714", "authors": ["Asma Sadia Khan", "Fariba Tasnia Khan", "Tanjim Mahmud", "Salman Karim Khan", "Rishita Chakma", "Nahed Sharmen", "Mohammad Shahadat Hossain", "Karl Andersson"], "title": "Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI", "categories": ["cs.LG", "cs.AI", "q-bio.QM", "stat.AP"], "comment": null, "summary": "Prostate cancer, the second most prevalent male malignancy, requires advanced\ndiagnostic tools. We propose an explainable AI system combining BERT (for\ntextual clinical notes) and Random Forest (for numerical lab data) through a\nnovel multimodal fusion strategy, achieving superior classification performance\non PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is\nestablished, our work demonstrates that a simple yet interpretable BERT+RF\npipeline delivers clinically significant improvements - particularly for\nintermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824\nnumerical/0.725 textual). SHAP analysis provides transparent feature importance\nrankings, while ablation studies prove textual features' complementary value.\nThis accessible approach offers hospitals a balance of high performance\n(F1=89%), computational efficiency, and clinical interpretability - addressing\ncritical needs in prostate cancer diagnostics."}
{"id": "2507.20925", "pdf": "https://arxiv.org/pdf/2507.20925", "abs": "https://arxiv.org/abs/2507.20925", "authors": ["Hongzhi Zhang", "Zhonglie Liu", "Kun Meng", "Jiameng Chen", "Jia Wu", "Bo Du", "Di Lin", "Yan Che", "Wenbin Hu"], "title": "Zero-Shot Learning with Subsequence Reordering Pretraining for Compound-Protein Interaction", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Given the vastness of chemical space and the ongoing emergence of previously\nuncharacterized proteins, zero-shot compound-protein interaction (CPI)\nprediction better reflects the practical challenges and requirements of\nreal-world drug development. Although existing methods perform adequately\nduring certain CPI tasks, they still face the following challenges: (1)\nRepresentation learning from local or complete protein sequences often\noverlooks the complex interdependencies between subsequences, which are\nessential for predicting spatial structures and binding properties. (2)\nDependence on large-scale or scarce multimodal protein datasets demands\nsignificant training data and computational resources, limiting scalability and\nefficiency. To address these challenges, we propose a novel approach that\npretrains protein representations for CPI prediction tasks using subsequence\nreordering, explicitly capturing the dependencies between protein subsequences.\nFurthermore, we apply length-variable protein augmentation to ensure excellent\npretraining performance on small training datasets. To evaluate the model's\neffectiveness and zero-shot learning ability, we combine it with various\nbaseline methods. The results demonstrate that our approach can improve the\nbaseline model's performance on the CPI task, especially in the challenging\nzero-shot scenario. Compared to existing pre-training models, our model\ndemonstrates superior performance, particularly in data-scarce scenarios where\ntraining samples are limited. Our implementation is available at\nhttps://github.com/Hoch-Zhang/PSRP-CPI."}
