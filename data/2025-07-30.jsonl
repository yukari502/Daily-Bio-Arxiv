{"id": "2507.21348", "pdf": "https://arxiv.org/pdf/2507.21348", "abs": "https://arxiv.org/abs/2507.21348", "authors": ["Izabel Valdez", "Paramahansa Pramanik"], "title": "Exploring the Interplay of Adiposity, Ethnicity, and Hormone Receptor Profiles in Breast Cancer Subtypes", "categories": ["q-bio.QM", "stat.AP"], "comment": "27 pages, 2 figures, 4 tables", "summary": "This study explores how obesity and race jointly influence the development\nand prognosis of Luminal subtypes of breast cancer, with a focus on\ndistinguishing Luminal A from the more aggressive Luminal B tumors. Drawing on\nlarge-scale epidemiological data and employing statistical approaches such as\nlogistic regression and mediation analysis, the research examines biological\nfactors like estrogen metabolism, adipokines, and chronic inflammation\nalongside social determinants including healthcare access, socioeconomic\nstatus, and cultural attitudes toward body weight. The findings reveal that\nboth obesity and racial background are significant predictors of risk for\nLuminal B breast cancers. The study highlights the need for a dual approach\nthat combines medical treatment with targeted social interventions aimed at\nreducing disparities. These insights can improve individualized risk\nassessments, guide tailored screening programs, and support policies that\naddress the heightened cancer burden experienced by marginalized communities."}
{"id": "2507.21260", "pdf": "https://arxiv.org/pdf/2507.21260", "abs": "https://arxiv.org/abs/2507.21260", "authors": ["Amartya Banerjee", "Xingyu Xu", "Caroline MoosmÃ¼ller", "Harlin Lee"], "title": "Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "Code: https://github.com/amartya21/Adam-PnP", "summary": "In an inverse problem, the goal is to recover an unknown parameter (e.g., an\nimage) that has typically undergone some lossy or noisy transformation during\nmeasurement. Recently, deep generative models, particularly diffusion models,\nhave emerged as powerful priors for protein structure generation. However,\nintegrating noisy experimental data from multiple sources to guide these models\nremains a significant challenge. Existing methods often require precise\nknowledge of experimental noise levels and manually tuned weights for each data\nmodality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that\nguides a pre-trained protein diffusion model using gradients from multiple,\nheterogeneous experimental sources. Our framework features an adaptive noise\nestimation scheme and a dynamic modality weighting mechanism integrated into\nthe diffusion process, which reduce the need for manual hyperparameter tuning.\nExperiments on complex reconstruction tasks demonstrate significantly improved\naccuracy using Adam-PnP."}
{"id": "2507.21404", "pdf": "https://arxiv.org/pdf/2507.21404", "abs": "https://arxiv.org/abs/2507.21404", "authors": ["Amber Huang", "Ian Scott Knight", "Slava Naprienko"], "title": "Data Leakage and Redundancy in the LIT-PCBA Benchmark", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "LIT-PCBA is a widely used benchmark for virtual screening, but our audit\nreveals it is fundamentally compromised. The dataset suffers from egregious\ndata leakage, rampant duplication, and pervasive analog redundancy -- flaws\nthat invalidate its use for fair model evaluation. Notably, we identify 2,491\ninactives duplicated across training and validation sets, and thousands more\nrepeated within individual data splits (2,945 in training, 789 in validation).\nCritically, three ligands in the query set -- meant to represent unseen test\ncases -- are leaked: two appear in the training set, one in validation.\nStructural redundancy compounds these issues: for some targets, over 80% of\nquery ligands are near duplicates, with Tanimoto similarity >= 0.9. In ALDH1\nalone, we find 323 highly similar active pairs between training and validation\nsets, invalidating claims of chemical diversity. These and other flaws\ncollectively cause models trained on LIT-PCBA to memorize rather than\ngeneralize. To demonstrate the consequences of these data integrity failures,\nwe implement a trivial memorization-based baseline -- using no learning, no\nphysics, and no modeling -- that outperforms state-of-the-art models, including\ndeep neural networks like CHEESE, on LIT-PCBA simply by exploiting these\nartifacts. Our findings render the benchmark unfit for its intended purpose and\ncall into question previous results based on its use. We share this audit to\nraise awareness and provide tooling to help the community develop more rigorous\nand reliable datasets going forward. All scripts necessary to reproduce our\naudit and the baseline implementation are available at:\nhttps://github.com/sievestack/LIT-PCBA-audit"}
