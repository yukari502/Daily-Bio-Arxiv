{"id": "2507.22088", "pdf": "https://arxiv.org/pdf/2507.22088", "abs": "https://arxiv.org/abs/2507.22088", "authors": ["Dylan Hamitouche", "Tihare Zamorano", "Youcef Barkat", "Deven Parekh", "Lena Palaniyappan", "David Benrimoh"], "title": "Sleep and Activity Patterns as Transdiagnostic Behavioral Biomarkers in Psychiatry: Initial Insights from the DeeP-DD study", "categories": ["q-bio.QM"], "comment": null, "summary": "Background: Symptom rating scales in psychiatry are limited by reliance on\nself-report, and lack of predictive power. Actigraphy, a passive wearable-based\nmethod for measuring sleep and physical activity, offers objective,\nhigh-resolution behavioral data that may better reflect symptom fluctuations,\nbut most studies have focused on narrow diagnostic groups or fixed time\nwindows, limiting clinical translation. Objective: To examine whether\nactigraphy-derived sleep and activity features correlate with psychiatric\nsymptom severity in a transdiagnostic psychiatric sample, and to identify which\nfeatures are most clinically relevant across multiple temporal resolutions.\nMethods: We present a feasibility case series study with preliminary data from\neight outpatients enrolled in the DeeP-DD study, a transdiagnostic study of\ndigital phenotyping. Participants wore GENEActiv actigraphy devices and symptom\nseverity was measured using a variety of validated scales. We performed\nintra-individual Spearman correlations and inter-individual repeated measures\ncorrelations across daily, weekly, monthly, and full-duration averages.\nResults: Intra-individual analyses revealed that later rise times were\nsignificantly associated with higher weekly PHQ-9 scores in participant #7\n(\\r{ho} = 0.74, P=.0003) and participant #4 (\\r{ho} = 0.78, P=.022), as well as\nhigher weekly GAD-7 scores in participant #7 (\\r{ho} = 0.59, P=.026).\nInter-individual analyses showed that weeks with later average rise time\ncorrelated with higher PHQ-9 (r = 0.48, P=.0003) and GAD-7 scores (r = 0.38,\nP=.032). Increased light physical activity was linked to lower PHQ-9 scores\nweekly (r = -0.44, P=.001) and monthly (r = -0.53, P=.014). Conclusion:\nConsistent associations between actigraphy features and symptoms across\ntemporal scales and diagnostic groups underscore their potential utility for\nscalable, real-world clinical monitoring.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6d3b\u52a8\u8bb0\u5f55\u4eea\uff08actigraphy\uff09\u884d\u751f\u7684\u7761\u7720\u548c\u6d3b\u52a8\u7279\u5f81\u4e0e\u8de8\u8bca\u65ad\u7cbe\u795e\u75c5\u6837\u672c\u4e2d\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u7684\u76f8\u5173\u6027\uff0c\u53d1\u73b0\u665a\u8d77\u65f6\u95f4\u548c\u8f7b\u5ea6\u6d3b\u52a8\u4e0e\u6291\u90c1\u548c\u7126\u8651\u75c7\u72b6\u663e\u8457\u76f8\u5173\u3002", "motivation": "\u4f20\u7edf\u7cbe\u795e\u75c5\u75c7\u72b6\u8bc4\u5206\u91cf\u8868\u4f9d\u8d56\u81ea\u6211\u62a5\u544a\u4e14\u9884\u6d4b\u80fd\u529b\u6709\u9650\uff0c\u6d3b\u52a8\u8bb0\u5f55\u4eea\u63d0\u4f9b\u5ba2\u89c2\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u884c\u4e3a\u6570\u636e\uff0c\u53ef\u80fd\u66f4\u597d\u5730\u53cd\u6620\u75c7\u72b6\u6ce2\u52a8\u3002", "method": "\u5bf98\u540d\u95e8\u8bca\u60a3\u8005\u8fdb\u884c\u53ef\u884c\u6027\u6848\u4f8b\u7cfb\u5217\u7814\u7a76\uff0c\u4f7f\u7528GENEActiv\u6d3b\u52a8\u8bb0\u5f55\u4eea\u6536\u96c6\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u9a8c\u8bc1\u91cf\u8868\u6d4b\u91cf\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\uff0c\u8fdb\u884c\u4e2a\u4f53\u5185\u548c\u4e2a\u4f53\u95f4\u7684\u76f8\u5173\u6027\u5206\u6790\u3002", "result": "\u665a\u8d77\u65f6\u95f4\u4e0e\u66f4\u9ad8\u7684\u6291\u90c1\u548c\u7126\u8651\u75c7\u72b6\u8bc4\u5206\u663e\u8457\u76f8\u5173\uff0c\u800c\u8f7b\u5ea6\u6d3b\u52a8\u589e\u52a0\u4e0e\u6291\u90c1\u75c7\u72b6\u51cf\u8f7b\u76f8\u5173\u3002", "conclusion": "\u6d3b\u52a8\u8bb0\u5f55\u4eea\u7279\u5f81\u4e0e\u75c7\u72b6\u7684\u5173\u8054\u8868\u660e\u5176\u5728\u4e34\u5e8a\u76d1\u6d4b\u4e2d\u7684\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.22092", "pdf": "https://arxiv.org/pdf/2507.22092", "abs": "https://arxiv.org/abs/2507.22092", "authors": ["Gianluca Carloni", "Biagio Brattoli", "Seongho Keum", "Jongchan Park", "Taebum Lee", "Chang Ho Ahn", "Sergio Pereira"], "title": "Pathology Foundation Models are Scanner Sensitive: Benchmark and Mitigation with Contrastive ScanGen Loss", "categories": ["q-bio.QM", "cs.AI", "cs.CV", "eess.IV", "q-bio.TO", "I.2; I.2.6; I.4; I.4.7; I.5; J.3; J.6"], "comment": "Accepted (Oral) in MedAGI 2025 International Workshop at MICCAI\n  Conference", "summary": "Computational pathology (CPath) has shown great potential in mining\nactionable insights from Whole Slide Images (WSIs). Deep Learning (DL) has been\nat the center of modern CPath, and while it delivers unprecedented performance,\nit is also known that DL may be affected by irrelevant details, such as those\nintroduced during scanning by different commercially available scanners. This\nmay lead to scanner bias, where the model outputs for the same tissue acquired\nby different scanners may vary. In turn, it hinders the trust of clinicians in\nCPath-based tools and their deployment in real-world clinical practices. Recent\npathology Foundation Models (FMs) promise to provide better domain\ngeneralization capabilities. In this paper, we benchmark FMs using a\nmulti-scanner dataset and show that FMs still suffer from scanner bias.\nFollowing this observation, we propose ScanGen, a contrastive loss function\napplied during task-specific fine-tuning that mitigates scanner bias, thereby\nenhancing the models' robustness to scanner variations. Our approach is applied\nto the Multiple Instance Learning task of Epidermal Growth Factor Receptor\n(EGFR) mutation prediction from H\\&E-stained WSIs in lung cancer. We observe\nthat ScanGen notably enhances the ability to generalize across scanners, while\nretaining or improving the performance of EGFR mutation prediction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aScanGen\u7684\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u51cf\u5c11\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u626b\u63cf\u4eea\u504f\u5dee\uff0c\u63d0\u5347\u6a21\u578b\u5728\u4e0d\u540c\u626b\u63cf\u4eea\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u626b\u63cf\u4eea\u5f15\u5165\u7684\u65e0\u5173\u7ec6\u8282\u5f71\u54cd\uff0c\u5bfc\u81f4\u626b\u63cf\u4eea\u504f\u5dee\uff0c\u5f71\u54cd\u4e34\u5e8a\u4fe1\u4efb\u548c\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faScanGen\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff0c\u5728\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u4e2d\u5e94\u7528\uff0c\u4ee5\u51cf\u5c11\u626b\u63cf\u4eea\u504f\u5dee\u3002\u5b9e\u9a8c\u57fa\u4e8e\u591a\u626b\u63cf\u4eea\u6570\u636e\u96c6\u548cEGFR\u7a81\u53d8\u9884\u6d4b\u4efb\u52a1\u3002", "result": "ScanGen\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u626b\u63cf\u4eea\u95f4\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u6539\u8fdb\u4e86EGFR\u7a81\u53d8\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "ScanGen\u6709\u6548\u89e3\u51b3\u4e86\u626b\u63cf\u4eea\u504f\u5dee\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u6a21\u578b\u7684\u4e34\u5e8a\u9002\u7528\u6027\u3002"}}
{"id": "2507.22210", "pdf": "https://arxiv.org/pdf/2507.22210", "abs": "https://arxiv.org/abs/2507.22210", "authors": ["Aviv Spinner", "Erika DeBenedictis", "Corey M. Hudson"], "title": "Scaling and Data Saturation in Protein Language Models", "categories": ["q-bio.QM"], "comment": "Presented at the GenBio Workshop, ICML 2025", "summary": "Data in biology is redundant, noisy, and sparse. How does the type and scale\nof available data impact model performance? In this work, we specifically\ninvestigate how protein language models (pLMs) scale with increasing\npretraining data. We investigate this relationship by measuring the performance\nof protein function prediction on a suite of pLMs pretrained on yearly\nsnapshots of UniRef100 from 2011 to 2024. We find no evidence of model\nsaturation on this task: performance improves--but not monotonically--with\nadded data, and this trend differs between unsupervised and supervised\nexperiments. Using a well-characterized Beta-Lactamase protein from E. coli, we\nfind that unsupervised model predictions get better year-over-year, though they\ndo not yet consistently perform better than the supervised baseline. Our\nresults underscore the need for targeted data acquisition and deeper study of\ndata scaling in protein modeling. All training, inference, analysis, and\nvisualization code is available at:\nhttps://github.com/Align-to-Innovate/data-saturation-and-scaling.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff08pLMs\uff09\u5728\u4e0d\u65ad\u589e\u52a0\u9884\u8bad\u7ec3\u6570\u636e\u65f6\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u6027\u80fd\u867d\u975e\u5355\u8c03\u63d0\u5347\u4f46\u6301\u7eed\u6539\u5584\uff0c\u4e14\u65e0\u9971\u548c\u8ff9\u8c61\u3002", "motivation": "\u751f\u7269\u5b66\u6570\u636e\u5197\u4f59\u3001\u566a\u58f0\u591a\u4e14\u7a00\u758f\uff0c\u7814\u7a76\u6570\u636e\u89c4\u6a21\u548c\u7c7b\u578b\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u4f7f\u75282011\u81f32024\u5e74UniRef100\u7684\u5e74\u5feb\u7167\u9884\u8bad\u7ec3pLMs\uff0c\u6d4b\u91cf\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u6027\u80fd\u3002", "result": "\u6027\u80fd\u968f\u6570\u636e\u589e\u52a0\u800c\u63d0\u5347\uff0c\u4f46\u975e\u5355\u8c03\uff0c\u4e14\u65e0\u76d1\u7763\u4e0e\u6709\u76d1\u7763\u5b9e\u9a8c\u8868\u73b0\u4e0d\u540c\u3002\u65e0\u76d1\u7763\u6a21\u578b\u9010\u5e74\u6539\u8fdb\uff0c\u4f46\u672a\u8d85\u8fc7\u6709\u76d1\u7763\u57fa\u7ebf\u3002", "conclusion": "\u9700\u9488\u5bf9\u6027\u6570\u636e\u91c7\u96c6\u548c\u66f4\u6df1\u5165\u7814\u7a76\u6570\u636e\u89c4\u6a21\u5bf9\u86cb\u767d\u8d28\u5efa\u6a21\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.22212", "pdf": "https://arxiv.org/pdf/2507.22212", "abs": "https://arxiv.org/abs/2507.22212", "authors": ["Levin M Moser", "Ahmad Kamal Hamid", "Esteban Miglietta", "Nodar Gogoberidze", "Beth A Cimini"], "title": "Evaluating Integrative Strategies for Incorporating Phenotypic Features in Spatial Transcriptomics", "categories": ["q-bio.QM"], "comment": null, "summary": "Spatial transcriptomics (ST) technologies not only offer an unprecedented\nopportunity to interrogate intact biological samples in a spatially informed\nmanner, but also set the stage for integration with other imaging-based\nmodalities. However, how to best exploit spatial context and integrate ST with\nimaging-based modalities remains an open question. To address this,\nparticularly under real-world experimental constraints such as limited dataset\nsize, class imbalance, and bounding-box-based segmentation, we used a publicly\navailable murine ileum MERFISH dataset to evaluate whether a minimally tuned\nvariational autoencoder (VAE) could extract informative low-dimensional\nrepresentations from cell crops of spot counts, nuclear stain, membrane stain,\nor a combination thereof. We assessed the resulting embeddings through\nPERMANOVA, cross-validated classification, and unsupervised Leiden clustering,\nand compared them to classical image-based feature vectors extracted via\nCellProfiler. While transcript counts (TC) generally outperformed other feature\nspaces, the VAE-derived latent spaces (LSs) captured meaningful biological\nvariation and enabled improved label recovery for specific cell types. LS2, in\nparticular, trained solely on morphological input, also exhibited moderate\npredictive power for a handful of genes in a ridge regression model. Notably,\ncombining TC with LSs through multiplex clustering led to consistent gains in\ncluster homogeneity, a trend that also held when augmenting only subsets of TC\nwith the stain-derived LS2. In contrast, CellProfiler-derived features\nunderperformed relative to LSs, highlighting the advantage of learned\nrepresentations over hand-crafted features. Collectively, these findings\ndemonstrate that even under constrained conditions, VAEs can extract\nbiologically meaningful signals from imaging data and constitute a promising\nstrategy for multi-modal integration.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u4ece\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u4e2d\u63d0\u53d6\u4f4e\u7ef4\u8868\u793a\uff0c\u5e76\u4e0e\u6210\u50cf\u6570\u636e\u6574\u5408\uff0c\u63d0\u5347\u751f\u7269\u5b66\u4fe1\u53f7\u7684\u5206\u6790\u6548\u679c\u3002", "motivation": "\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\uff08ST\uff09\u6280\u672f\u4e3a\u751f\u7269\u6837\u672c\u7684\u7a7a\u95f4\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5229\u7528\u7a7a\u95f4\u4fe1\u606f\u5e76\u6574\u5408\u6210\u50cf\u6570\u636e\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u516c\u5f00\u7684\u5c0f\u9f20\u56de\u80a0MERFISH\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86VAE\u5728\u591a\u79cd\u8f93\u5165\uff08\u5982\u8f6c\u5f55\u8ba1\u6570\u3001\u6838\u67d3\u8272\u3001\u819c\u67d3\u8272\uff09\u4e0b\u63d0\u53d6\u4f4e\u7ef4\u8868\u793a\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u4f20\u7edf\u65b9\u6cd5\uff08CellProfiler\uff09\u6bd4\u8f83\u3002", "result": "VAE\u63d0\u53d6\u7684\u6f5c\u5728\u7a7a\u95f4\uff08LSs\uff09\u80fd\u6355\u6349\u6709\u610f\u4e49\u7684\u751f\u7269\u5b66\u53d8\u5f02\uff0c\u63d0\u5347\u7279\u5b9a\u7ec6\u80de\u7c7b\u578b\u7684\u5206\u7c7b\u6548\u679c\uff0c\u4e14\u4e0e\u8f6c\u5f55\u8ba1\u6570\u7ed3\u5408\u53ef\u63d0\u9ad8\u805a\u7c7b\u540c\u8d28\u6027\u3002", "conclusion": "\u5373\u4f7f\u5728\u53d7\u9650\u6761\u4ef6\u4e0b\uff0cVAE\u4e5f\u80fd\u4ece\u6210\u50cf\u6570\u636e\u4e2d\u63d0\u53d6\u751f\u7269\u5b66\u4fe1\u53f7\uff0c\u662f\u591a\u6a21\u6001\u6574\u5408\u7684\u6709\u524d\u666f\u7b56\u7565\u3002"}}
{"id": "2507.22256", "pdf": "https://arxiv.org/pdf/2507.22256", "abs": "https://arxiv.org/abs/2507.22256", "authors": ["Jun Won Park", "Kangyu Zhao", "Sanket Rane"], "title": "Spatiodynamic inference using vision-based generative modelling", "categories": ["q-bio.QM", "q-bio.PE"], "comment": null, "summary": "Biological systems commonly exhibit complex spatiotemporal patterns whose\nunderlying generative mechanisms pose a significant analytical challenge.\nTraditional approaches to spatiodynamic inference rely on dimensionality\nreduction through summary statistics, which sacrifice complexity and\ninterdependent structure intrinsic to these data in favor of parameter\nidentifiability. This imposes a fundamental constraint on reliably extracting\nmechanistic insights from spatiotemporal data, highlighting the need for\nanalytical frameworks that preserve the full richness of these dynamical\nsystems. To address this, we developed a simulation-based inference framework\nthat employs vision transformer-driven variational encoding to generate compact\nrepresentations of the data, exploiting the inherent contextual dependencies.\nThese representations are subsequently integrated into a likelihood-free\nBayesian approach for parameter inference. The central idea is to construct a\nfine-grained, structured mesh of latent representations from simulated dynamics\nthrough systematic exploration of the parameter space. This encoded mesh of\nlatent embeddings then serves as a reference map for retrieving parameter\nvalues that correspond to observed data. By integrating generative modeling\nwith Bayesian principles, our approach provides a unified inference framework\nto identify both spatial and temporal patterns that manifest in multivariate\ndynamical systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u53d8\u6362\u5668\u548c\u53d8\u5206\u7f16\u7801\u7684\u6a21\u62df\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u65f6\u7a7a\u6570\u636e\u7684\u53c2\u6570\u63a8\u65ad\uff0c\u4fdd\u7559\u6570\u636e\u7684\u5b8c\u6574\u6027\u548c\u4f9d\u8d56\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u964d\u7ef4\u727a\u7272\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u4f9d\u8d56\u6027\uff0c\u9650\u5236\u4e86\u4ece\u65f6\u7a7a\u6570\u636e\u4e2d\u63d0\u53d6\u673a\u5236\u6027\u89c1\u89e3\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u53d8\u6362\u5668\u9a71\u52a8\u7684\u53d8\u5206\u7f16\u7801\u751f\u6210\u6570\u636e\u7684\u7d27\u51d1\u8868\u793a\uff0c\u7ed3\u5408\u65e0\u4f3c\u7136\u8d1d\u53f6\u65af\u65b9\u6cd5\u8fdb\u884c\u53c2\u6570\u63a8\u65ad\u3002", "result": "\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u591a\u5143\u52a8\u6001\u7cfb\u7edf\u4e2d\u7684\u65f6\u7a7a\u6a21\u5f0f\uff0c\u4fdd\u7559\u6570\u636e\u7684\u5b8c\u6574\u7ed3\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u65f6\u7a7a\u6570\u636e\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u63a8\u7406\u6846\u67b6\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.22587", "pdf": "https://arxiv.org/pdf/2507.22587", "abs": "https://arxiv.org/abs/2507.22587", "authors": ["Alexandre Durrmeyer", "Jean-Christophe Palauqui", "Philippe Andrey"], "title": "Deep learning of geometrical cell division rules", "categories": ["cs.LG", "q-bio.CB", "q-bio.QM", "I.2.6; I.6; J.3"], "comment": "44 pages, 6 figures, 1 supplementary table, 15 supplementary figures", "summary": "The positioning of new cellular walls during cell division plays a key role\nin shaping plant tissue organization. The influence of cell geometry on the\npositioning of division planes has been previously captured into various\ngeometrical rules. Accordingly, linking cell shape to division orientation has\nrelied on the comparison between observed division patterns and predictions\nunder specific rules. The need to define a priori the tested rules is a\nfundamental limitation of this hypothesis-driven approach. As an alternative,\nwe introduce a data-based approach to investigate the relation between cell\ngeometry and division plane positioning, exploiting the ability of deep neural\nnetwork to learn complex relationships across multidimensional spaces. Adopting\nan image-based cell representation, we show how division patterns can be\nlearned and predicted from mother cell geometry using a UNet architecture\nmodified to operate on cell masks. Using synthetic data and A. thaliana embryo\ncells, we evaluate the model performances on a wide range of diverse cell\nshapes and division patterns. We find that the trained model accounted for\nembryo division patterns that were previously irreconcilable under existing\ngeometrical rules. Our work shows the potential of deep networks to understand\ncell division patterns and to generate new hypotheses on the control of cell\ndivision positioning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7814\u7a76\u7ec6\u80de\u51e0\u4f55\u5f62\u72b6\u4e0e\u5206\u88c2\u5e73\u9762\u5b9a\u4f4d\u7684\u5173\u7cfb\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u51e0\u4f55\u89c4\u5219\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5148\u9a8c\u7684\u51e0\u4f55\u89c4\u5219\uff0c\u9650\u5236\u4e86\u7ec6\u80de\u5206\u88c2\u6a21\u5f0f\u7684\u7814\u7a76\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u66f4\u5168\u9762\u5730\u7406\u89e3\u7ec6\u80de\u51e0\u4f55\u4e0e\u5206\u88c2\u5e73\u9762\u7684\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u6539\u8fdb\u7684UNet\u67b6\u6784\uff0c\u57fa\u4e8e\u7ec6\u80de\u63a9\u6a21\u56fe\u50cf\uff0c\u4ece\u6bcd\u7ec6\u80de\u51e0\u4f55\u5f62\u72b6\u4e2d\u5b66\u4e60\u548c\u9884\u6d4b\u5206\u88c2\u6a21\u5f0f\u3002", "result": "\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u548c\u62df\u5357\u82a5\u80da\u80ce\u7ec6\u80de\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u80fd\u591f\u89e3\u91ca\u4f20\u7edf\u51e0\u4f55\u89c4\u5219\u65e0\u6cd5\u6db5\u76d6\u7684\u5206\u88c2\u6a21\u5f0f\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u5728\u7406\u89e3\u7ec6\u80de\u5206\u88c2\u6a21\u5f0f\u548c\u751f\u6210\u65b0\u5047\u8bbe\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.22710", "pdf": "https://arxiv.org/pdf/2507.22710", "abs": "https://arxiv.org/abs/2507.22710", "authors": ["Filippo Utro", "Meltem Tolunay", "Kahn Rhrissorrakrai", "Tanvi P. Gujarati", "Jie Shi", "Sara Capponi", "Mirko Amico", "Nate Earnest-Noble", "Laxmi Parida"], "title": "Enhanced Prediction of CAR T-Cell Cytotoxicity with Quantum-Kernel Methods", "categories": ["cs.LG", "q-bio.QM", "quant-ph"], "comment": null, "summary": "Chimeric antigen receptor (CAR) T-cells are T-cells engineered to recognize\nand kill specific tumor cells. Through their extracellular domains, CAR T-cells\nbind tumor cell antigens which triggers CAR T activation and proliferation.\nThese processes are regulated by co-stimulatory domains present in the\nintracellular region of the CAR T-cell. Through integrating novel signaling\ncomponents into the co-stimulatory domains, it is possible to modify CAR T-cell\nphenotype. Identifying and experimentally testing new CAR constructs based on\nlibraries of co-stimulatory domains is nontrivial given the vast combinatorial\nspace defined by such libraries. This leads to a highly data constrained,\npoorly explored combinatorial problem, where the experiments undersample all\npossible combinations. We propose a quantum approach using a Projected Quantum\nKernel (PQK) to address this challenge. PQK operates by embedding classical\ndata into a high dimensional Hilbert space and employs a kernel method to\nmeasure sample similarity. Using 61 qubits on a gate-based quantum computer, we\ndemonstrate the largest PQK application to date and an enhancement in the\nclassification performance over purely classical machine learning methods for\nCAR T cytotoxicity prediction. Importantly, we show improved learning for\nspecific signaling domains and domain positions, particularly where there was\nlower information highlighting the potential for quantum computing in\ndata-constrained problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7684\u65b9\u6cd5\uff08PQK\uff09\u6765\u89e3\u51b3CAR T\u7ec6\u80de\u8bbe\u8ba1\u4e2d\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u91cf\u5b50\u8ba1\u7b97\u5728\u6570\u636e\u53d7\u9650\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "CAR T\u7ec6\u80de\u8bbe\u8ba1\u4e2d\u7684\u7ec4\u5408\u7a7a\u95f4\u5de8\u5927\u4e14\u5b9e\u9a8c\u6570\u636e\u6709\u9650\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u6295\u5f71\u91cf\u5b50\u6838\uff08PQK\uff09\u5c06\u7ecf\u5178\u6570\u636e\u5d4c\u5165\u9ad8\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u6838\u65b9\u6cd5\u6d4b\u91cf\u6837\u672c\u76f8\u4f3c\u6027\u3002", "result": "\u572861\u91cf\u5b50\u6bd4\u7279\u7684\u95e8\u63a7\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u5b9e\u73b0\u4e86\u6700\u5927\u7684PQK\u5e94\u7528\uff0c\u5206\u7c7b\u6027\u80fd\u4f18\u4e8e\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u5728\u6570\u636e\u53d7\u9650\u7684CAR T\u7ec6\u80de\u8bbe\u8ba1\u4e2d\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u4fe1\u606f\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u3002"}}
