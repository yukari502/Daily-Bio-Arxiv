{"id": "2508.14106", "pdf": "https://arxiv.org/pdf/2508.14106", "abs": "https://arxiv.org/abs/2508.14106", "authors": ["Surajit Das", "Gourav Roy", "Pavel Zun"], "title": "High-Throughput Low-Cost Segmentation of Brightfield Microscopy Live Cell Images", "categories": ["q-bio.QM", "cs.AI", "cs.CV", "eess.IV"], "comment": null, "summary": "Live cell culture is crucial in biomedical studies for analyzing cell\nproperties and dynamics in vitro. This study focuses on segmenting unstained\nlive cells imaged with bright-field microscopy. While many segmentation\napproaches exist for microscopic images, none consistently address the\nchallenges of bright-field live-cell imaging with high throughput, where\ntemporal phenotype changes, low contrast, noise, and motion-induced blur from\ncellular movement remain major obstacles. We developed a low-cost CNN-based\npipeline incorporating comparative analysis of frozen encoders within a unified\nU-Net architecture enhanced with attention mechanisms, instance-aware systems,\nadaptive loss functions, hard instance retraining, dynamic learning rates,\nprogressive mechanisms to mitigate overfitting, and an ensemble technique. The\nmodel was validated on a public dataset featuring diverse live cell variants,\nshowing consistent competitiveness with state-of-the-art methods, achieving 93%\ntest accuracy and an average F1-score of 89% (std. 0.07) on low-contrast,\nnoisy, and blurry images. Notably, the model was trained primarily on\nbright-field images with limited exposure to phase-contrast microscopy (<10%),\nyet it generalized effectively to the phase-contrast LIVECell dataset,\ndemonstrating modality, robustness and strong performance. This highlights its\npotential for real-world laboratory deployment across imaging conditions. The\nmodel requires minimal compute power and is adaptable using basic deep learning\nsetups such as Google Colab, making it practical for training on other cell\nvariants. Our pipeline outperforms existing methods in robustness and precision\nfor bright-field microscopy segmentation. The code and dataset are available\nfor reproducibility"}
{"id": "2508.14321", "pdf": "https://arxiv.org/pdf/2508.14321", "abs": "https://arxiv.org/abs/2508.14321", "authors": ["Mohammad M. Amirian", "Andrew J. Irwin"], "title": "piCurve: an R package for modeling photosynthesis-irradiance curves", "categories": ["stat.AP", "q-bio.QM", "stat.CO"], "comment": null, "summary": "Photosynthesis-irradiance (PI) curves are foundational for quantifying\nprimary production, parameterizing ecosystem and biogeochemical models, and\ninterpreting physiological acclimation to light. Despite their broad use,\nresearchers lack a unified, reproducible toolkit to fit, compare, and diagnose\nthe many PI formulations that have accumulated over the last century. We\nintroduce piCurve, an R package that standardizes the modeling of PI\nrelationships, with a library of widely used light-limited, light-saturated,\nand photoinhibited formulations and a consistent statistical framework for\nestimation and comparison. With the total of 24 PI models, piCurve supports\nmean squared error (MSE) and maximum likelihood estimation (MLE), provides\nuncertainty quantification via information matrix (Hessian), and includes\nautomated, data-informed initialization to improve convergence. Utilities\nclassify PI data into light-limited, light-saturated, and photoinhibited\nregions, while plotting and 'tidy' helpers streamline workflow and reporting.\nTogether, these features enable reproducible analyses and fair model\ncomparisons, including for curves exhibiting a plateau followed by\nphotoinhibition."}
