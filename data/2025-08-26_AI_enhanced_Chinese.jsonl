{"id": "2508.17345", "pdf": "https://arxiv.org/pdf/2508.17345", "abs": "https://arxiv.org/abs/2508.17345", "authors": ["Yuxuan Song", "Zhe Zhang", "Yu Pei", "Jingjing Gong", "Qiying Yu", "Zheng Zhang", "Mingxuan Wang", "Hao Zhou", "Jingjing Liu", "Wei-Ying Ma"], "title": "ShortListing Model: A Streamlined SimplexDiffusion for Discrete Variable Generation", "categories": ["cs.LG", "q-bio.GN"], "comment": null, "summary": "Generative modeling of discrete variables is challenging yet crucial for\napplications in natural language processing and biological sequence design. We\nintroduce the Shortlisting Model (SLM), a novel simplex-based diffusion model\ninspired by progressive candidate pruning. SLM operates on simplex centroids,\nreducing generation complexity and enhancing scalability. Additionally, SLM\nincorporates a flexible implementation of classifier-free guidance, enhancing\nunconditional generation performance. Extensive experiments on DNA promoter and\nenhancer design, protein design, character-level and large-vocabulary language\nmodeling demonstrate the competitive performance and strong potential of SLM.\nOur code can be found at https://github.com/GenSI-THUAIR/SLM", "AI": {"tldr": "SLM\u662f\u4e00\u79cd\u57fa\u4e8e\u5355\u7eaf\u5f62\u7684\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5019\u9009\u526a\u679d\u673a\u5236\u5728\u79bb\u6563\u53d8\u91cf\u751f\u6210\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u8272\u6027\u80fd", "motivation": "\u79bb\u6563\u53d8\u91cf\u7684\u751f\u6210\u5efa\u6a21\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u751f\u7269\u5e8f\u5217\u8bbe\u8ba1\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u9762\u4e34\u6311\u6218\u3002\u9700\u8981\u5f00\u53d1\u65b0\u7684\u65b9\u6cd5\u6765\u964d\u4f4e\u751f\u6210\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u53ef\u6269\u5c55\u6027", "method": "\u63d0\u51faShortlisting Model (SLM)\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u5355\u7eaf\u5f62\u8d28\u5fc3\u7684\u6269\u6563\u6a21\u578b\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u5019\u9009\u526a\u679d\u673a\u5236\uff0c\u5e76\u6574\u5408\u4e86\u7075\u6d3b\u7684\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u5b9e\u73b0", "result": "\u5728DNA\u542f\u52a8\u5b50\u548c\u589e\u5f3a\u5b50\u8bbe\u8ba1\u3001\u86cb\u767d\u8d28\u8bbe\u8ba1\u3001\u5b57\u7b26\u7ea7\u548c\u5927\u8bcd\u6c47\u91cf\u8bed\u8a00\u5efa\u6a21\u7b49\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86SLM\u7684\u7ade\u4e89\u6027\u6027\u80fd\u548c\u5f3a\u5927\u6f5c\u529b", "conclusion": "SLM\u4e3a\u79bb\u6563\u53d8\u91cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\u5c55\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u5177\u6709\u91cd\u8981\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2508.16740", "pdf": "https://arxiv.org/pdf/2508.16740", "abs": "https://arxiv.org/abs/2508.16740", "authors": ["Robert Amevor", "Emmanuel Kubuafor", "Dennis Baidoo", "Junaidu Salifu", "Koshali Muthunama Gonnage", "Onyedikachi Joshua Okeke"], "title": "Integrative Prognostic Modeling of Breast Cancer Survival with Gene Expression and Clinical Data", "categories": ["q-bio.QM"], "comment": "14 pages, 8 figures", "summary": "Background: Accurate survival prediction in breast cancer is essential for\npatient stratification and personalized therapy. Integrating gene expression\ndata with clinical factors may enhance prognostic performance and support\nprecision medicine. Objective: To develop an integrative survival prediction\nmodel combining clinical variables and gene expression signatures, and to\nassess their contributions using penalized Cox regression and machine learning.\nMethods: We analyzed 1,867 patients from the METABRIC cohort with clinical\nannotations and microarray-based gene expression profiles. The top 5,000 most\nvariable genes were retained. Elastic Net-penalized Cox regression identified\n75 predictors (70 genes and 5 clinical variables: tumor size, stage, surgery\ntype, age at diagnosis, and Nottingham Prognostic Index). Model performance was\nevaluated with Harrell's concordance index (C-index) and 36-month\ntime-dependent AUC. Random Survival Forests (RSF) trained on the top 20 genes\nassessed nonlinear effects and validated variable importance. PCA and heatmaps\nvisualized gene expression patterns across risk groups. Results: The\nintegrative Cox model achieved a C-index of 0.922 and a 36-month AUC of 0.94,\noutperforming clinical-only models (C=0.64). RSF confirmed the prognostic value\nof top genes (e.g., OR2T27, TBATA, LINC01165, SLC10A4), yielding a 36-month AUC\nof 0.88. Conclusions: Combining gene expression signatures with clinical\nvariables substantially improves survival prediction in breast cancer and\nprovides a framework for individualized prognostic assessment and clinical\ndecision-making.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e34\u5e8a\u53d8\u91cf\u548c\u57fa\u56e0\u8868\u8fbe\u7279\u5f81\u7684\u4e73\u817a\u764c\u751f\u5b58\u9884\u6d4b\u6a21\u578b\uff0c\u4f7f\u7528\u5f39\u6027\u7f51\u7edc\u60e9\u7f5aCox\u56de\u5f52\u548c\u968f\u673a\u751f\u5b58\u68ee\u6797\uff0c\u5728METABRIC\u961f\u5217\u4e2d\u9a8c\u8bc1\u4e86\u6574\u5408\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u51c6\u786e\u7684\u4e73\u817a\u764c\u751f\u5b58\u9884\u6d4b\u5bf9\u4e8e\u60a3\u8005\u5206\u5c42\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u6574\u5408\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u548c\u4e34\u5e8a\u56e0\u7d20\u53ef\u80fd\u63d0\u9ad8\u9884\u540e\u6027\u80fd\u5e76\u652f\u6301\u7cbe\u51c6\u533b\u7597\u3002", "method": "\u5206\u6790\u4e86METABRIC\u961f\u5217\u4e2d1,867\u540d\u60a3\u8005\u7684\u4e34\u5e8a\u6ce8\u91ca\u548c\u5fae\u9635\u5217\u57fa\u56e0\u8868\u8fbe\u6570\u636e\uff0c\u4f7f\u7528\u5f39\u6027\u7f51\u7edc\u60e9\u7f5aCox\u56de\u5f52\u8bc6\u522b75\u4e2a\u9884\u6d4b\u56e0\u5b50\uff0870\u4e2a\u57fa\u56e0\u548c5\u4e2a\u4e34\u5e8a\u53d8\u91cf\uff09\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u751f\u5b58\u68ee\u6797\u8bc4\u4f30\u975e\u7ebf\u6027\u6548\u5e94\u548c\u53d8\u91cf\u91cd\u8981\u6027\u3002", "result": "\u6574\u5408Cox\u6a21\u578b\u83b7\u5f97C-index\u4e3a0.922\u548c36\u4e2a\u6708AUC\u4e3a0.94\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5\u4e34\u5e8a\u6a21\u578b\uff08C=0.64\uff09\u3002\u968f\u673a\u751f\u5b58\u68ee\u6797\u786e\u8ba4\u4e86\u5173\u952e\u57fa\u56e0\u7684\u9884\u540e\u4ef7\u503c\uff0c\u83b7\u5f9736\u4e2a\u6708AUC\u4e3a0.88\u3002", "conclusion": "\u57fa\u56e0\u8868\u8fbe\u7279\u5f81\u4e0e\u4e34\u5e8a\u53d8\u91cf\u7684\u7ed3\u5408\u663e\u8457\u63d0\u9ad8\u4e86\u4e73\u817a\u764c\u751f\u5b58\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a\u4e2a\u4f53\u5316\u9884\u540e\u8bc4\u4f30\u548c\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2508.17010", "pdf": "https://arxiv.org/pdf/2508.17010", "abs": "https://arxiv.org/abs/2508.17010", "authors": ["Yue Hu", "Zanxia Cao", "Yingchao Liu"], "title": "Lie-RMSD: A Gradient-Based Framework for Protein Structural Alignment using Lie Algebra", "categories": ["q-bio.QM", "92C40"], "comment": "7 pages, 1 figure, 1 table", "summary": "The comparison of protein structures is a fundamental task in computational\nbiology, crucial for understanding protein function, evolution, and for drug\ndesign. While analytical methods like the Kabsch algorithm provide an exact,\nclosed-form solution for minimizing the Root Mean Square Deviation (RMSD)\nbetween two sets of corresponding atoms, their application is limited to this\nspecific metric. The rise of deep learning and automatic differentiation\nframeworks offers a new, more flexible paradigm for such optimization problems.\nWe present Lie-RMSD, a novel, fully differentiable framework for protein\nstructural alignment. Our method represents the rigid-body transformation\n(rotation and translation) as a 6-dimensional vector in the Lie algebra se(3)\nof the special Euclidean group SE(3). This representation allows the RMSD to be\nformulated as a loss function that can be directly minimized by modern\ngradient-based optimizers. We benchmarked our framework by aligning two\nallosteric conformations of Adenylate Kinase (PDB IDs: 4AKE and 1AKE). We\ndemonstrate that a suite of standard optimizers (SGD, Adam, AdamW, and Sophia)\ncan robustly converge to the global minimum, achieving precision effectively\nidentical to the analytical Kabsch algorithm. This work validates the accuracy\nof the Lie algebra-based gradient descent approach and establishes a robust\nfoundation for its extension to more sophisticated and biologically relevant\nscoring functions where no analytical solutions exist.", "AI": {"tldr": "\u63d0\u51fa\u4e86Lie-RMSD\uff0c\u4e00\u79cd\u57fa\u4e8e\u674e\u4ee3\u6570\u7684\u5b8c\u5168\u53ef\u5fae\u5206\u86cb\u767d\u8d28\u7ed3\u6784\u6bd4\u5bf9\u6846\u67b6\uff0c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u5668\u5b9e\u73b0\u4e0eKabsch\u7b97\u6cd5\u76f8\u5f53\u7684\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u5982Kabsch\u7b97\u6cd5\u4ec5\u9650\u4e8eRMSD\u6307\u6807\uff0c\u65e0\u6cd5\u5904\u7406\u66f4\u590d\u6742\u7684\u751f\u7269\u76f8\u5173\u8bc4\u5206\u51fd\u6570\u3002\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e3a\u8fd9\u7c7b\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7075\u6d3b\u8303\u5f0f", "method": "\u5c06\u521a\u4f53\u53d8\u6362\u8868\u793a\u4e3a\u7279\u6b8a\u6b27\u51e0\u91cc\u5f97\u7fa4SE(3)\u7684\u674e\u4ee3\u6570se(3)\u4e2d\u76846\u7ef4\u5411\u91cf\uff0c\u5c06RMSD\u6784\u5efa\u4e3a\u53ef\u76f4\u63a5\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u5668\u6700\u5c0f\u5316\u7684\u635f\u5931\u51fd\u6570", "result": "\u5728\u817a\u82f7\u9178\u6fc0\u9176\u53d8\u6784\u6784\u8c61\u6bd4\u5bf9\u4e2d\uff0c\u591a\u79cd\u6807\u51c6\u4f18\u5316\u5668\uff08SGD\u3001Adam\u3001AdamW\u3001Sophia\uff09\u90fd\u80fd\u7a33\u5065\u6536\u655b\u5230\u5168\u5c40\u6700\u5c0f\u503c\uff0c\u7cbe\u5ea6\u4e0eKabsch\u7b97\u6cd5\u57fa\u672c\u76f8\u540c", "conclusion": "\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u674e\u4ee3\u6570\u7684\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u6269\u5c55\u5230\u66f4\u590d\u6742\u3001\u65e0\u89e3\u6790\u89e3\u7684\u751f\u7269\u5b66\u76f8\u5173\u8bc4\u5206\u51fd\u6570\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840"}}
{"id": "2508.17389", "pdf": "https://arxiv.org/pdf/2508.17389", "abs": "https://arxiv.org/abs/2508.17389", "authors": ["Bokai Zhao", "Weiyang Shi", "Hanqing Chao", "Zijiang Yang", "Yiyang Zhang", "Ming Song", "Tianzi Jiang"], "title": "Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction", "categories": ["q-bio.QM", "cs.AI", "cs.CV"], "comment": "MICCAI 2025", "summary": "Spatial proteomics maps protein distributions in tissues, providing\ntransformative insights for life sciences. However, current sequencing-based\ntechnologies suffer from low spatial resolution, and substantial inter-tissue\nvariability in protein expression further compromises the performance of\nexisting molecular data prediction methods. In this work, we introduce the\nnovel task of spatial super-resolution for sequencing-based spatial proteomics\n(seq-SP) and, to the best of our knowledge, propose the first deep learning\nmodel for this task--Neural Proteomics Fields (NPF). NPF formulates seq-SP as a\nprotein reconstruction problem in continuous space by training a dedicated\nnetwork for each tissue. The model comprises a Spatial Modeling Module, which\nlearns tissue-specific protein spatial distributions, and a Morphology Modeling\nModule, which extracts tissue-specific morphological features. Furthermore, to\nfacilitate rigorous evaluation, we establish an open-source benchmark dataset,\nPseudo-Visium SP, for this task. Experimental results demonstrate that NPF\nachieves state-of-the-art performance with fewer learnable parameters,\nunderscoring its potential for advancing spatial proteomics research. Our code\nand dataset are publicly available at https://github.com/Bokai-Zhao/NPF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u6d4b\u5e8f\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u7a7a\u95f4\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\uff08seq-SP\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bNPF\uff0c\u901a\u8fc7\u8fde\u7eed\u7a7a\u95f4\u86cb\u767d\u8d28\u91cd\u5efa\u548c\u53cc\u6a21\u5757\u67b6\u6784\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd", "motivation": "\u5f53\u524d\u6d4b\u5e8f\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u6280\u672f\u5b58\u5728\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\u3001\u7ec4\u7ec7\u95f4\u86cb\u767d\u8868\u8fbe\u53d8\u5f02\u5927\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5206\u5b50\u6570\u636e\u9884\u6d4b\u65b9\u6cd5\u6027\u80fd\u53d7\u9650", "method": "NPF\u6a21\u578b\u5c06seq-SP\u5efa\u6a21\u4e3a\u8fde\u7eed\u7a7a\u95f4\u86cb\u767d\u8d28\u91cd\u5efa\u95ee\u9898\uff0c\u5305\u542b\u7a7a\u95f4\u5efa\u6a21\u6a21\u5757\uff08\u5b66\u4e60\u7ec4\u7ec7\u7279\u5f02\u6027\u86cb\u767d\u7a7a\u95f4\u5206\u5e03\uff09\u548c\u5f62\u6001\u5efa\u6a21\u6a21\u5757\uff08\u63d0\u53d6\u7ec4\u7ec7\u7279\u5f02\u6027\u5f62\u6001\u7279\u5f81\uff09\uff0c\u6bcf\u4e2a\u7ec4\u7ec7\u8bad\u7ec3\u4e13\u7528\u7f51\u7edc", "result": "NPF\u5728\u5efa\u7acb\u7684Pseudo-Visium SP\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u91cf\u66f4\u5c11", "conclusion": "NPF\u6a21\u578b\u5177\u6709\u63a8\u52a8\u7a7a\u95f4\u86cb\u767d\u8d28\u7ec4\u5b66\u7814\u7a76\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90"}}
{"id": "2508.18058", "pdf": "https://arxiv.org/pdf/2508.18058", "abs": "https://arxiv.org/abs/2508.18058", "authors": ["Muheng Shang", "Jin Zhang", "Junwei Han", "Lei Du"], "title": "Comprehensively stratifying MCIs into distinct risk subtypes based on brain imaging genetics fusion learning", "categories": ["q-bio.QM"], "comment": null, "summary": "Mild cognitive impairment (MCI) is the prodromal stage of Alzheimer's disease\n(AD) and thus enrolling MCI subjects to undergo clinical trials is worthwhile.\nHowever, MCI groups usually show significant diversity and heterogeneity in the\npathology and symptom, which pose great challenge to accurately select\nappropriate subjects. This study aimed to stratify MCI subjects into distinct\nsubgroups with substantial difference in the risk of transitioning to AD by\nfusing multimodal brain imaging genetic data. The integrated imaging genetics\nmethod comprised three modules, i.e., the whole-genome-oriented risk genetic\ninformation extraction module (RGE), the genetic-to-brain mapping module\n(RG2PG), and the genetic-guided pseudo-brain fusion module (CMPF). We used data\nfrom AD Neuroimaging Initiative (ADNI) and identified two MCI subtypes, called\nlow-risk MCI (lsMCI) and high-risk MCI (hsMCI). We also validated that the two\nsubgroups showed distinct patterns of in terms of multiple biomarkers including\ngenetics, demographics, fluid biomarkers, brain imaging features, clinical\nsymptoms and cognitive functioning at baseline, as well as their longitudinal\ndevelopmental trajectories. Furthermore, we also identified potential\nbiomarkers that may implicate the risk of MCIs, providing critical insights for\npatient stratification at early stage.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u878d\u5408\u591a\u6a21\u6001\u8111\u6210\u50cf\u9057\u4f20\u6570\u636e\uff0c\u5c06\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d(MCI)\u60a3\u8005\u5206\u4e3a\u4f4e\u98ce\u9669\u548c\u9ad8\u98ce\u9669\u4e9a\u578b\uff0c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u60a3\u8005\u5206\u5c42\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "MCI\u4f5c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u524d\u9a71\u9636\u6bb5\u5177\u6709\u663e\u8457\u7684\u5f02\u8d28\u6027\uff0c\u51c6\u786e\u7b5b\u9009\u5408\u9002\u7684\u4e34\u5e8a\u8bd5\u9a8c\u53d7\u8bd5\u8005\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u533a\u5206\u4e0d\u540c\u98ce\u9669\u4e9a\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e09\u6a21\u5757\u6574\u5408\u6210\u50cf\u9057\u4f20\u5b66\u65b9\u6cd5\uff1a\u5168\u57fa\u56e0\u7ec4\u98ce\u9669\u9057\u4f20\u4fe1\u606f\u63d0\u53d6\u6a21\u5757(RGE)\u3001\u9057\u4f20\u5230\u8111\u6620\u5c04\u6a21\u5757(RG2PG)\u548c\u9057\u4f20\u5f15\u5bfc\u7684\u4f2a\u8111\u878d\u5408\u6a21\u5757(CMPF)\uff0c\u57fa\u4e8eADNI\u6570\u636e\u8fdb\u884c\u5206\u6790\u3002", "result": "\u6210\u529f\u8bc6\u522b\u51fa\u4f4e\u98ce\u9669MCI(lsMCI)\u548c\u9ad8\u98ce\u9669MCI(hsMCI)\u4e24\u79cd\u4e9a\u578b\uff0c\u9a8c\u8bc1\u4e86\u5b83\u4eec\u5728\u9057\u4f20\u3001\u4eba\u53e3\u7edf\u8ba1\u3001\u6d41\u4f53\u751f\u7269\u6807\u5fd7\u7269\u3001\u8111\u6210\u50cf\u7279\u5f81\u3001\u4e34\u5e8a\u75c7\u72b6\u548c\u8ba4\u77e5\u529f\u80fd\u7b49\u591a\u65b9\u9762\u7684\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u533a\u5206MCI\u60a3\u8005\u7684\u98ce\u9669\u7b49\u7ea7\uff0c\u4e3a\u65e9\u671f\u60a3\u8005\u5206\u5c42\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5e76\u8bc6\u522b\u51fa\u53ef\u80fd\u6697\u793aMCI\u98ce\u9669\u7684\u6f5c\u5728\u751f\u7269\u6807\u5fd7\u7269\u3002"}}
{"id": "2508.16650", "pdf": "https://arxiv.org/pdf/2508.16650", "abs": "https://arxiv.org/abs/2508.16650", "authors": ["James K Ruffle", "Samia Mohinta", "Guilherme Pombo", "Asthik Biswas", "Alan Campbell", "Indran Davagnanam", "David Doig", "Ahmed Hamman", "Harpreet Hyare", "Farrah Jabeen", "Emma Lim", "Dermot Mallon", "Stephanie Owen", "Sophie Wilkinson", "Sebastian Brandner", "Parashkev Nachev"], "title": "Predicting brain tumour enhancement from non-contrast MR imaging with artificial intelligence", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "comment": "38 pages", "summary": "Brain tumour imaging assessment typically requires both pre- and\npost-contrast MRI, but gadolinium administration is not always desirable, such\nas in frequent follow-up, renal impairment, allergy, or paediatric patients. We\naimed to develop and validate a deep learning model capable of predicting brain\ntumour contrast enhancement from non-contrast MRI sequences alone. We assembled\n11089 brain MRI studies from 10 international datasets spanning adult and\npaediatric populations with various neuro-oncological states, including glioma,\nmeningioma, metastases, and post-resection appearances. Deep learning models\n(nnU-Net, SegResNet, SwinUNETR) were trained to predict and segment enhancing\ntumour using only non-contrast T1-, T2-, and T2/FLAIR-weighted images.\nPerformance was evaluated on 1109 held-out test patients using patient-level\ndetection metrics and voxel-level segmentation accuracy. Model predictions were\ncompared against 11 expert radiologists who each reviewed 100 randomly selected\npatients. The best-performing nnU-Net achieved 83% balanced accuracy, 91.5%\nsensitivity, and 74.4% specificity in detecting enhancing tumour. Enhancement\nvolume predictions strongly correlated with ground truth (R2 0.859). The model\noutperformed expert radiologists, who achieved 69.8% accuracy, 75.9%\nsensitivity, and 64.7% specificity. 76.8% of test patients had Dice over 0.3\n(acceptable detection), 67.5% had Dice over 0.5 (good detection), and 50.2% had\nDice over 0.7 (excellent detection). Deep learning can identify\ncontrast-enhancing brain tumours from non-contrast MRI with clinically relevant\nperformance. These models show promise as screening tools and may reduce\ngadolinium dependence in neuro-oncology imaging. Future work should evaluate\nclinical utility alongside radiology experts.", "AI": {"tldr": "\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ece\u975e\u5bf9\u6bd4MRI\u9884\u6d4b\u8111\u80bf\u7624\u5bf9\u6bd4\u589e\u5f3a\uff0c\u572811089\u4f8bMRI\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u6027\u80fd\u4f18\u4e8e\u4e13\u5bb6\u653e\u5c04\u79d1\u533b\u751f\uff0c\u51c6\u786e\u7387\u8fbe83%", "motivation": "\u9486\u5bf9\u6bd4\u5242\u5728\u9891\u7e41\u968f\u8bbf\u3001\u80be\u529f\u80fd\u4e0d\u5168\u3001\u8fc7\u654f\u6216\u513f\u79d1\u60a3\u8005\u4e2d\u4e0d\u53ef\u53d6\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u5bf9\u6bd4\u5242\u7684\u8111\u80bf\u7624\u589e\u5f3a\u68c0\u6d4b\u65b9\u6cd5", "method": "\u4f7f\u7528nnU-Net\u3001SegResNet\u3001SwinUNETR\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ec5\u57fa\u4e8e\u975e\u5bf9\u6bd4T1\u3001T2\u3001T2/FLAIR\u52a0\u6743\u56fe\u50cf\u9884\u6d4b\u548c\u5206\u5272\u589e\u5f3a\u80bf\u7624", "result": "\u6700\u4f73\u6a21\u578bnnU-Net\u68c0\u6d4b\u589e\u5f3a\u80bf\u7624\u7684\u5e73\u8861\u51c6\u786e\u7387\u8fbe83%\uff0c\u654f\u611f\u602791.5%\uff0c\u7279\u5f02\u602774.4%\uff0c\u589e\u5f3a\u4f53\u79ef\u9884\u6d4b\u4e0e\u771f\u5b9e\u503c\u5f3a\u76f8\u5173(R\u00b2 0.859)\uff0c\u6027\u80fd\u4f18\u4e8e\u4e13\u5bb6\u653e\u5c04\u79d1\u533b\u751f", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u53ef\u4ece\u975e\u5bf9\u6bd4MRI\u8bc6\u522b\u5bf9\u6bd4\u589e\u5f3a\u8111\u80bf\u7624\uff0c\u5177\u6709\u4e34\u5e8a\u76f8\u5173\u6027\u80fd\uff0c\u6709\u671b\u4f5c\u4e3a\u7b5b\u67e5\u5de5\u5177\u5e76\u51cf\u5c11\u795e\u7ecf\u80bf\u7624\u6210\u50cf\u5bf9\u9486\u7684\u4f9d\u8d56"}}
{"id": "2508.16803", "pdf": "https://arxiv.org/pdf/2508.16803", "abs": "https://arxiv.org/abs/2508.16803", "authors": ["Yu Wang", "Xiao Chen", "Hubert Schwarz", "V\u00e9ronique Chotteau", "Elling W. Jacobsen"], "title": "A predictive modular approach to constraint satisfaction under uncertainty - with application to glycosylation in continuous monoclonal antibody biosimilar production", "categories": ["eess.SY", "cs.SY", "math.OC", "q-bio.QM"], "comment": null, "summary": "The paper proposes a modular-based approach to constraint handling in process\noptimization and control. This is partly motivated by the recent interest in\nlearning-based methods, e.g., within bioproduction, for which constraint\nhandling under uncertainty is a challenge. The proposed constraint handler,\ncalled predictive filter, is combined with an adaptive constraint margin and a\nconstraint violation cost monitor to minimize the cost of violating soft\nconstraints due to model uncertainty and disturbances. The module can be\ncombined with any controller and is based on minimally modifying the controller\noutput, in a least squares sense, such that constraints are satisfied within\nthe considered horizon. The proposed method is computationally efficient and\nsuitable for real-time applications. The effectiveness of the method is\nillustrated through a realistic simulation case study of glycosylation\nconstraint satisfaction in continuous monoclonal antibody biosimilar production\nusing Chinese hamster ovary cells, for which the metabolic network model\nconsists of 23 extracellular metabolites and 126 reactions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u5757\u5316\u7684\u9884\u6d4b\u6ee4\u6ce2\u5668\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7ea6\u675f\u8fb9\u754c\u548c\u7ea6\u675f\u8fdd\u53cd\u6210\u672c\u76d1\u63a7\uff0c\u7528\u4e8e\u5904\u7406\u8fc7\u7a0b\u4f18\u5316\u63a7\u5236\u4e2d\u7684\u7ea6\u675f\u95ee\u9898\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b58\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270\u7684\u573a\u666f\u3002", "motivation": "\u8fd1\u5e74\u6765\u5b66\u4e60\u578b\u65b9\u6cd5\u5728\u751f\u7269\u751f\u4ea7\u7b49\u9886\u57df\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u7ea6\u675f\u5904\u7406\u4ecd\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u8f6f\u7ea6\u675f\u8fdd\u53cd\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u7684\u9884\u6d4b\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6700\u5c0f\u5316\u4fee\u6539\u63a7\u5236\u5668\u8f93\u51fa\uff0c\u786e\u4fdd\u5728\u9884\u6d4b\u65f6\u57df\u5185\u6ee1\u8db3\u7ea6\u675f\u6761\u4ef6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7ea6\u675f\u8fb9\u754c\u548c\u7ea6\u675f\u8fdd\u53cd\u6210\u672c\u76d1\u63a7\u673a\u5236\u3002", "result": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\uff0c\u9002\u5408\u5b9e\u65f6\u5e94\u7528\uff0c\u5728\u7cd6\u57fa\u5316\u7ea6\u675f\u6ee1\u8db3\u7684\u8fde\u7eed\u5355\u514b\u9686\u6297\u4f53\u751f\u7269\u7c7b\u4f3c\u7269\u751f\u4ea7\u4eff\u771f\u6848\u4f8b\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6548\u679c\uff0c\u6a21\u578b\u5305\u542b23\u79cd\u80de\u5916\u4ee3\u8c22\u7269\u548c126\u4e2a\u53cd\u5e94\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ea6\u675f\u5904\u7406\u5668\u6a21\u5757\u5316\u8bbe\u8ba1\u7075\u6d3b\uff0c\u53ef\u4e0e\u4efb\u4f55\u63a7\u5236\u5668\u7ed3\u5408\uff0c\u6709\u6548\u5904\u7406\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270\u5bfc\u81f4\u7684\u8f6f\u7ea6\u675f\u8fdd\u53cd\u95ee\u9898\uff0c\u5728\u751f\u7269\u751f\u4ea7\u8fc7\u7a0b\u4f18\u5316\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.16934", "pdf": "https://arxiv.org/pdf/2508.16934", "abs": "https://arxiv.org/abs/2508.16934", "authors": ["Tim Mach", "Daniel Rueckert", "Alex Berger", "Laurin Lux", "Ivan Ezhov"], "title": "Addressing Annotation Scarcity in Hyperspectral Brain Image Segmentation with Unsupervised Domain Adaptation", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "This work presents a novel deep learning framework for segmenting cerebral\nvasculature in hyperspectral brain images. We address the critical challenge of\nsevere label scarcity, which impedes conventional supervised training. Our\napproach utilizes a novel unsupervised domain adaptation methodology, using a\nsmall, expert-annotated ground truth alongside unlabeled data. Quantitative and\nqualitative evaluations confirm that our method significantly outperforms\nexisting state-of-the-art approaches, demonstrating the efficacy of domain\nadaptation for label-scarce biomedical imaging tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65e0\u76d1\u7763\u57df\u9002\u5e94\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8111\u90e8\u9ad8\u5149\u8c31\u56fe\u50cf\u4e2d\u7684\u8111\u8840\u7ba1\u5206\u5272\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898", "motivation": "\u89e3\u51b3\u8111\u90e8\u9ad8\u5149\u8c31\u56fe\u50cf\u4e2d\u8111\u8840\u7ba1\u5206\u5272\u9762\u4e34\u7684\u4e25\u91cd\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\uff0c\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5e94\u7528", "method": "\u91c7\u7528\u65b0\u9896\u7684\u65e0\u76d1\u7763\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c11\u91cf\u4e13\u5bb6\u6807\u6ce8\u7684\u771f\u5b9e\u6807\u7b7e\u6570\u636e\u548c\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3", "result": "\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u5747\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5", "conclusion": "\u8bc1\u660e\u4e86\u57df\u9002\u5e94\u65b9\u6cd5\u5728\u6807\u7b7e\u7a00\u7f3a\u7684\u751f\u7269\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2508.17555", "pdf": "https://arxiv.org/pdf/2508.17555", "abs": "https://arxiv.org/abs/2508.17555", "authors": ["Kairi Furui", "Masahito Ohue"], "title": "Boltzina: Efficient and Accurate Virtual Screening via Docking-Guided Binding Prediction with Boltz-2", "categories": ["q-bio.BM", "cs.CE", "cs.LG", "q-bio.QM"], "comment": null, "summary": "In structure-based drug discovery, virtual screening using conventional\nmolecular docking methods can be performed rapidly but suffers from limitations\nin prediction accuracy. Recently, Boltz-2 was proposed, achieving extremely\nhigh accuracy in binding affinity prediction, but requiring approximately 20\nseconds per compound per GPU, making it difficult to apply to large-scale\nscreening of hundreds of thousands to millions of compounds. This study\nproposes Boltzina, a novel framework that leverages Boltz-2's high accuracy\nwhile significantly improving computational efficiency. Boltzina achieves both\naccuracy and speed by omitting the rate-limiting structure prediction from\nBoltz-2's architecture and directly predicting affinity from AutoDock Vina\ndocking poses. We evaluate on eight assays from the MF-PCBA dataset and show\nthat while Boltzina performs below Boltz-2, it provides significantly higher\nscreening performance compared to AutoDock Vina and GNINA. Additionally,\nBoltzina achieved up to 11.8$\\times$ faster through reduced recycling\niterations and batch processing. Furthermore, we investigated multi-pose\nselection strategies and two-stage screening combining Boltzina and Boltz-2,\npresenting optimization methods for accuracy and efficiency according to\napplication requirements. This study represents the first attempt to apply\nBoltz-2's high-accuracy predictions to practical-scale screening, offering a\npipeline that combines both accuracy and efficiency in computational biology.\nThe Boltzina is available on github; https://github.com/ohuelab/boltzina.", "AI": {"tldr": "Boltzina\u662f\u4e00\u4e2a\u65b0\u578b\u865a\u62df\u7b5b\u9009\u6846\u67b6\uff0c\u5728\u4fdd\u6301Boltz-2\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u901a\u8fc7\u8df3\u8fc7\u7ed3\u6784\u9884\u6d4b\u6b65\u9aa4\u76f4\u63a5\u4eceAutoDock Vina\u5bf9\u63a5\u6784\u8c61\u9884\u6d4b\u4eb2\u548c\u529b", "motivation": "\u89e3\u51b3Boltz-2\u867d\u7136\u9884\u6d4b\u7cbe\u5ea6\u6781\u9ad8\u4f46\u8ba1\u7b97\u8017\u65f6\u8fc7\u957f\uff08\u7ea620\u79d2/\u5316\u5408\u7269/GPU\uff09\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u5316\u5408\u7269\u5e93\u7b5b\u9009\u7684\u95ee\u9898", "method": "\u7701\u7565Boltz-2\u4e2d\u7684\u9650\u901f\u7ed3\u6784\u9884\u6d4b\u6b65\u9aa4\uff0c\u76f4\u63a5\u4eceAutoDock Vina\u5bf9\u63a5\u6784\u8c61\u9884\u6d4b\u4eb2\u548c\u529b\uff0c\u901a\u8fc7\u51cf\u5c11\u5faa\u73af\u8fed\u4ee3\u548c\u6279\u5904\u7406\u63d0\u5347\u6548\u7387", "result": "\u5728MF-PCBA\u6570\u636e\u96c6\u76848\u4e2a\u6d4b\u8bd5\u4e2d\uff0cBoltzina\u6027\u80fd\u4f4e\u4e8eBoltz-2\u4f46\u663e\u8457\u4f18\u4e8eAutoDock Vina\u548cGNINA\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u8fbe11.8\u500d", "conclusion": "\u9996\u6b21\u5c06Boltz-2\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u5e94\u7528\u4e8e\u5b9e\u9645\u89c4\u6a21\u7b5b\u9009\uff0c\u63d0\u4f9b\u4e86\u5728\u8ba1\u7b97\u751f\u7269\u5b66\u4e2d\u517c\u987e\u7cbe\u5ea6\u548c\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.17699", "pdf": "https://arxiv.org/pdf/2508.17699", "abs": "https://arxiv.org/abs/2508.17699", "authors": ["Z. Rafati", "M. Hoseyni", "J. Khoramdel", "A. Nikoofard"], "title": "Benchmarking Class Activation Map Methods for Explainable Brain Hemorrhage Classification on Hemorica Dataset", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Explainable Artificial Intelligence (XAI) has become an essential component\nof medical imaging research, aiming to increase transparency and clinical trust\nin deep learning models. This study investigates brain hemorrhage diagnosis\nwith a focus on explainability through Class Activation Mapping (CAM)\ntechniques. A pipeline was developed to extract pixellevel segmentation and\ndetection annotations from classification models using nine state-of-the-art\nCAM algorithms, applied across multiple network stages, and quantitatively\nevaluated on the Hemorica dataset, which uniquely provides both slice-level\nlabels and high-quality segmentation masks. Metrics including Dice, IoU, and\npixel-wise overlap were employed to benchmark CAM variants. Results show that\nthe strongest localization performance occurred at stage 5 of EfficientNetV2S,\nwith HiResCAM yielding the highest bounding-box alignment and AblationCAM\nachieving the best pixel-level Dice (0.57) and IoU (0.40), representing strong\naccuracy given that models were trained solely for classification without\nsegmentation supervision. To the best of current knowledge, this is among the f\nirst works to quantitatively compare CAM methods for brain hemorrhage\ndetection, establishing a reproducible benchmark and underscoring the potential\nof XAI-driven pipelines for clinically meaningful AI-assisted diagnosis.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc79\u79cdCAM\u6280\u672f\u5728\u8111\u51fa\u8840\u8bca\u65ad\u4e2d\u5b9e\u73b0\u53ef\u89e3\u91caAI\uff0c\u5728EfficientNetV2S\u7b2c5\u9636\u6bb5\u83b7\u5f97\u6700\u4f73\u5b9a\u4f4d\u6027\u80fd\uff0cHiResCAM\u548cAblationCAM\u5206\u522b\u53d6\u5f97\u6700\u4f73\u8fb9\u754c\u6846\u5bf9\u9f50\u548c\u50cf\u7d20\u7ea7\u5206\u5272\u6548\u679c\u3002", "motivation": "\u63d0\u9ad8\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u900f\u660e\u5ea6\u548c\u4e34\u5e8a\u4fe1\u4efb\u5ea6\uff0c\u901a\u8fc7\u53ef\u89e3\u91caAI\u6280\u672f\u589e\u5f3a\u8111\u51fa\u8840\u8bca\u65ad\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2apipeline\uff0c\u4f7f\u75289\u79cd\u6700\u5148\u8fdb\u7684CAM\u7b97\u6cd5\u4ece\u5206\u7c7b\u6a21\u578b\u4e2d\u63d0\u53d6\u50cf\u7d20\u7ea7\u5206\u5272\u548c\u68c0\u6d4b\u6807\u6ce8\uff0c\u5728Hemorica\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\uff0c\u91c7\u7528Dice\u3001IoU\u548c\u50cf\u7d20\u91cd\u53e0\u7b49\u6307\u6807\u3002", "result": "\u5728EfficientNetV2S\u7b2c5\u9636\u6bb5\u83b7\u5f97\u6700\u4f73\u5b9a\u4f4d\u6027\u80fd\uff0cHiResCAM\u5b9e\u73b0\u6700\u9ad8\u8fb9\u754c\u6846\u5bf9\u9f50\uff0cAblationCAM\u83b7\u5f97\u6700\u4f73\u50cf\u7d20\u7ea7Dice(0.57)\u548cIoU(0.40)\u3002", "conclusion": "\u8fd9\u662f\u9996\u6279\u5b9a\u91cf\u6bd4\u8f83CAM\u65b9\u6cd5\u7528\u4e8e\u8111\u51fa\u8840\u68c0\u6d4b\u7684\u7814\u7a76\u4e4b\u4e00\uff0c\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86XAI\u9a71\u52a8pipeline\u5728\u4e34\u5e8aAI\u8f85\u52a9\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.18066", "pdf": "https://arxiv.org/pdf/2508.18066", "abs": "https://arxiv.org/abs/2508.18066", "authors": ["Alberto Silvio Chiappa", "Boshi An", "Merkourios Simos", "Chengkun Li", "Alexander Mathis"], "title": "Arnold: a generalist muscle transformer policy", "categories": ["cs.RO", "cs.AI", "cs.LG", "q-bio.QM"], "comment": "A.S.C. and B.A. contributed equally. Code is available at\n  https://github.com/amathislab/arnold-the-generalist", "summary": "Controlling high-dimensional and nonlinear musculoskeletal models of the\nhuman body is a foundational scientific challenge. Recent machine learning\nbreakthroughs have heralded policies that master individual skills like\nreaching, object manipulation and locomotion in musculoskeletal systems with\nmany degrees of freedom. However, these agents are merely \"specialists\",\nachieving high performance for a single skill. In this work, we develop Arnold,\na generalist policy that masters multiple tasks and embodiments. Arnold\ncombines behavior cloning and fine-tuning with PPO to achieve expert or\nsuper-expert performance in 14 challenging control tasks from dexterous object\nmanipulation to locomotion. A key innovation is Arnold's sensorimotor\nvocabulary, a compositional representation of the semantics of heterogeneous\nsensory modalities, objectives, and actuators. Arnold leverages this vocabulary\nvia a transformer architecture to deal with the variable observation and action\nspaces of each task. This framework supports efficient multi-task,\nmulti-embodiment learning and facilitates rapid adaptation to novel tasks.\nFinally, we analyze Arnold to provide insights into biological motor control,\ncorroborating recent findings on the limited transferability of muscle\nsynergies across tasks.", "AI": {"tldr": "\u5f00\u53d1\u4e86Arnold\u901a\u7528\u7b56\u7565\uff0c\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u548cPPO\u5fae\u8c03\uff0c\u572814\u4e2a\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u6027\u80fd\uff0c\u4f7f\u7528\u4f20\u611f\u5668\u8fd0\u52a8\u8bcd\u6c47\u548cTransformer\u67b6\u6784\u5904\u7406\u591a\u4efb\u52a1\u591a\u4f53\u73b0\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4\u975e\u7ebf\u6027\u4eba\u4f53\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u7684\u63a7\u5236\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u638c\u63e1\u5355\u4e00\u6280\u80fd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u638c\u63e1\u591a\u4efb\u52a1\u548c\u591a\u4f53\u73b0\u7684\u901a\u7528\u7b56\u7565\u3002", "method": "\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u548cPPO\u5fae\u8c03\uff0c\u521b\u65b0\u6027\u5730\u4f7f\u7528\u4f20\u611f\u5668\u8fd0\u52a8\u8bcd\u6c47\u4f5c\u4e3a\u5f02\u6784\u611f\u5b98\u6a21\u6001\u3001\u76ee\u6807\u548c\u6267\u884c\u5668\u7684\u7ec4\u5408\u8868\u793a\uff0c\u901a\u8fc7Transformer\u67b6\u6784\u5904\u7406\u53ef\u53d8\u89c2\u5bdf\u548c\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u572814\u4e2a\u6311\u6218\u6027\u63a7\u5236\u4efb\u52a1\uff08\u4ece\u7075\u5de7\u7269\u4f53\u64cd\u4f5c\u5230\u8fd0\u52a8\uff09\u4e2d\u8fbe\u5230\u4e13\u5bb6\u6216\u8d85\u4e13\u5bb6\u7ea7\u6027\u80fd\uff0c\u652f\u6301\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u591a\u4f53\u73b0\u5b66\u4e60\u5e76\u4fc3\u8fdb\u5bf9\u65b0\u4efb\u52a1\u7684\u5feb\u901f\u9002\u5e94\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u751f\u7269\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u8bc1\u5b9e\u4e86\u808c\u8089\u534f\u540c\u4f5c\u7528\u5728\u8de8\u4efb\u52a1\u4e2d\u53ef\u8f6c\u79fb\u6027\u6709\u9650\u7684\u6700\u65b0\u53d1\u73b0\uff0c\u4e3a\u901a\u7528\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\u5f00\u53d1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
