{"id": "2509.25274", "pdf": "https://arxiv.org/pdf/2509.25274", "abs": "https://arxiv.org/abs/2509.25274", "authors": ["Darren King", "Yaser Atlasi", "Gholamreza Rafiee"], "title": "DNABERT-2: Fine-Tuning a Genomic Language Model for Colorectal Gene Enhancer Classification", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "comment": "10 pages, 10 figures, 2 tables", "summary": "Gene enhancers control when and where genes switch on, yet their sequence\ndiversity and tissue specificity make them hard to pinpoint in colorectal\ncancer. We take a sequence-only route and fine-tune DNABERT-2, a transformer\ngenomic language model that uses byte-pair encoding to learn variable-length\ntokens from DNA. Using assays curated via the Johnston Cancer Research Centre\nat Queen's University Belfast, we assembled a balanced corpus of 2.34 million 1\nkb enhancer sequences, applied summit-centered extraction and rigorous\nde-duplication including reverse-complement collapse, and split the data\nstratified by class. With a 4096-term vocabulary and a 232-token context chosen\nempirically, the DNABERT-2-117M classifier was trained with Optuna-tuned\nhyperparameters and evaluated on 350742 held-out sequences. The model reached\nPR-AUC 0.759, ROC-AUC 0.743, and best F1 0.704 at an optimized threshold\n(0.359), with recall 0.835 and precision 0.609. Against a CNN-based EnhancerNet\ntrained on the same data, DNABERT-2 delivered stronger threshold-independent\nranking and higher recall, although point accuracy was lower. To our knowledge,\nthis is the first study to apply a second-generation genomic language model\nwith BPE tokenization to enhancer classification in colorectal cancer,\ndemonstrating the feasibility of capturing tumor-associated regulatory signals\ndirectly from DNA sequence alone. Overall, our results show that\ntransformer-based genomic models can move beyond motif-level encodings toward\nholistic classification of regulatory elements, offering a novel path for\ncancer genomics. Next steps will focus on improving precision, exploring hybrid\nCNN-transformer designs, and validating across independent datasets to\nstrengthen real-world utility."}
{"id": "2509.25573", "pdf": "https://arxiv.org/pdf/2509.25573", "abs": "https://arxiv.org/abs/2509.25573", "authors": ["David Laub", "Ethan Armand", "Arda Pekis", "Zekai Chen", "Irsyad Adam", "Shaun Porwal", "Bing Ren", "Kevin Brown", "Hannah Carter"], "title": "GenVarFormer: Predicting gene expression from long-range mutations in cancer", "categories": ["q-bio.GN"], "comment": null, "summary": "Distinguishing the rare \"driver\" mutations that fuel cancer progression from\nthe vast background of \"passenger\" mutations in the non-coding genome is a\nfundamental challenge in cancer biology. A primary mechanism that non-coding\ndriver mutations contribute to cancer is by affecting gene expression,\npotentially from millions of nucleotides away. However, existing predictors of\ngene expression from mutations are unable to simultaneously handle interactions\nspanning millions of base pairs, the extreme sparsity of somatic mutations, and\ngeneralize to unseen genes. To overcome these limitations, we introduce\nGenVarFormer (GVF), a novel transformer-based architecture designed to learn\nmutation representations and their impact on gene expression. GVF efficiently\npredicts the effect of mutations up to 8 million base pairs away from a gene by\nonly considering mutations and their local DNA context, while omitting the vast\nintermediate sequence. Using data from 864 breast cancer samples from The\nCancer Genome Atlas, we demonstrate that GVF predicts gene expression with\n26-fold higher correlation across samples than current models. In addition, GVF\nis the first model of its kind to generalize to unseen genes and samples\nsimultaneously. Finally, we find that GVF patient embeddings are more\ninformative than ground-truth gene expression for predicting overall patient\nsurvival in the most prevalent breast cancer subtype, luminal A. GVF embeddings\nand gene expression yielded concordance indices of $0.706^{\\pm0.136}$ and\n$0.573^{\\pm0.234}$, respectively. Our work establishes a new state-of-the-art\nfor modeling the functional impact of non-coding mutations in cancer and\nprovides a powerful new tool for identifying potential driver events and\nprognostic biomarkers."}
{"id": "2509.25884", "pdf": "https://arxiv.org/pdf/2509.25884", "abs": "https://arxiv.org/abs/2509.25884", "authors": ["Ping Xu", "Zaitian Wang", "Zhirui Wang", "Pengjiang Li", "Ran Zhang", "Gaoyang Li", "Hanyu Xie", "Jiajia Wang", "Yuanchun Zhou", "Pengfei Wang"], "title": "scUnified: An AI-Ready Standardized Resource for Single-Cell RNA Sequencing Analysis", "categories": ["q-bio.GN", "cs.AI"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) technology enables systematic\ndelineation of cellular states and interactions, providing crucial insights\ninto cellular heterogeneity. Building on this potential, numerous computational\nmethods have been developed for tasks such as cell clustering, cell type\nannotation, and marker gene identification. To fully assess and compare these\nmethods, standardized, analysis-ready datasets are essential. However, such\ndatasets remain scarce, and variations in data formats, preprocessing\nworkflows, and annotation strategies hinder reproducibility and complicate\nsystematic evaluation of existing methods. To address these challenges, we\npresent scUnified, an AI-ready standardized resource for single-cell RNA\nsequencing data that consolidates 13 high-quality datasets spanning two species\n(human and mouse) and nine tissue types. All datasets undergo standardized\nquality control and preprocessing and are stored in a uniform format to enable\ndirect application in diverse computational analyses without additional data\ncleaning. We further demonstrate the utility of scUnified through experimental\nanalyses of representative biological tasks, providing a reproducible\nfoundation for the standardized evaluation of computational methods on a\nunified dataset."}
{"id": "2509.26223", "pdf": "https://arxiv.org/pdf/2509.26223", "abs": "https://arxiv.org/abs/2509.26223", "authors": ["Chenyu Li", "Elias Ziyadeh", "Yash Sharma", "Bernhard Dumoulin", "Jonathan Levinsohn", "Eunji Ha", "Siyu Pan", "Vishwanatha Rao", "Madhav Subramaniyam", "Mario Szegedy", "Nancy Zhang", "Katalin Susztak"], "title": "Nephrobase Cell+: Multimodal Single-Cell Foundation Model for Decoding Kidney Biology", "categories": ["q-bio.GN"], "comment": null, "summary": "Background: Large foundation models have revolutionized single-cell analysis,\nyet no kidney-specific model currently exists, and it remains unclear whether\norgan-focused models can outperform generalized models. The kidney's complex\ncellular architecture further complicate integration of large-scale omics data,\nwhere current frameworks trained on limited datasets struggle to correct batch\neffects, capture cross-modality variation, and generalize across species.\nMethods: We developed Nephrobase Cell+, the first kidney-focused large\nfoundation model, pretrained on ~100 billion tokens from ~39.5 million\nsingle-cell and single-nucleus profiles across 4,319 samples. Nephrobase Cell+\nuses a transformer-based encoder-decoder architecture with gene-token\ncross-attention and a mixture-of-experts module for scalable representation\nlearning. Results: Nephrobase Cell+ sets a new benchmark for kidney single-cell\nanalysis. It produces tightly clustered, biologically coherent embeddings in\nhuman and mouse kidneys, far surpassing previous foundation models such as\nGeneformer, scGPT, and UCE, as well as traditional methods such as PCA and\nautoencoders. It achieves the highest cluster concordance and batch-mixing\nscores, effectively removing donor/assay batch effects while preserving\ncell-type structure. Cross-species evaluation shows superior alignment of\nhomologous cell types and >90% zero-shot annotation accuracy for major kidney\nlineages in both human and mouse. Even its 1B-parameter and 500M variants\nconsistently outperform all existing models. Conclusions: Nephrobase Cell+\ndelivers a unified, high-fidelity representation of kidney biology that is\nrobust, cross-species transferable, and unmatched by current single-cell\nfoundation models, offering a powerful resource for kidney genomics and disease\nresearch."}
{"id": "2509.25872", "pdf": "https://arxiv.org/pdf/2509.25872", "abs": "https://arxiv.org/abs/2509.25872", "authors": ["Yan Wang", "Hao Wu", "Simon Olsson"], "title": "Marginal Girsanov Reweighting: Stable Variance Reduction via Neural Ratio Estimation", "categories": ["q-bio.QM", "q-bio.BM"], "comment": null, "summary": "Recovering unbiased properties from biased or perturbed simulations is a\ncentral challenge in rare-event sampling. Classical Girsanov Reweighting (GR)\noffers a principled solution by yielding exact pathwise probability ratios\nbetween perturbed and reference processes. However, the variance of GR weights\ngrows rapidly with time, rendering it impractical for long-horizon reweighting.\nWe introduce Marginal Girsanov Reweighting (MGR), which mitigates variance\nexplosion by marginalizing over intermediate paths, producing stable and\nscalable weights for long-timescale dynamics. Experiments demonstrate that MGR\n(i) accurately recovers kinetic properties from umbrella-sampling trajectories\nin molecular dynamics, and (ii) enables efficient Bayesian parameter inference\nfor stochastic differential equations with temporally sparse observations."}
{"id": "2509.25346", "pdf": "https://arxiv.org/pdf/2509.25346", "abs": "https://arxiv.org/abs/2509.25346", "authors": ["Lawrence Phillips", "Marc Boubnovski Martell", "Aditya Misra", "Josefa Lia Stoisser", "Cesar A. Prada-Medina", "Rory Donovan-Maiye", "Kaspar MÃ¤rtens"], "title": "SynthPert: Enhancing LLM Biological Reasoning via Synthetic Reasoning Traces for Cellular Perturbation Prediction", "categories": ["cs.AI", "cs.LG", "q-bio.CB", "q-bio.GN"], "comment": null, "summary": "Predicting cellular responses to genetic perturbations represents a\nfundamental challenge in systems biology, critical for advancing therapeutic\ndiscovery and virtual cell modeling. While large language models (LLMs) show\npromise for biological reasoning, their application to perturbation prediction\nremains underexplored due to challenges in adapting them to structured\nexperimental data. We present SynthPert, a novel method that enhances LLM\nperformance through supervised fine-tuning on synthetic reasoning traces\ngenerated by frontier models. Using the PerturbQA benchmark, we demonstrate\nthat our approach not only achieves state-of-the-art performance but surpasses\nthe capabilities of the frontier model that generated the training data. Our\nresults reveal three key insights: (1) Synthetic reasoning traces effectively\ndistill biological knowledge even when partially inaccurate, (2) This approach\nenables cross-cell-type generalization with 87% accuracy on unseen RPE1 cells,\nand (3) Performance gains persist despite using only 2% of quality-filtered\ntraining data. This work shows the effectiveness of synthetic reasoning\ndistillation for enhancing domain-specific reasoning in LLMs."}
{"id": "2509.26566", "pdf": "https://arxiv.org/pdf/2509.26566", "abs": "https://arxiv.org/abs/2509.26566", "authors": ["JunJie Wee", "Faisal Suwayyid", "Mushal Zia", "Hongsong Feng", "Yuta Hozumi", "Guo-Wei Wei"], "title": "Commutative algebra neural network reveals genetic origins of diseases", "categories": ["q-bio.QM", "math.AC", "q-bio.BM"], "comment": null, "summary": "Genetic mutations can disrupt protein structure, stability, and solubility,\ncontributing to a wide range of diseases. Existing predictive models often lack\ninterpretability and fail to integrate physical and chemical interactions\ncritical to molecular mechanisms. Moreover, current approaches treat disease\nassociation, stability changes, and solubility alterations as separate tasks,\nlimiting model generalizability. In this study, we introduce a unified\nframework based on multiscale commutative algebra to capture intrinsic physical\nand chemical interactions for the first time. Leveraging Persistent\nStanley-Reisner Theory, we extract multiscale algebraic invariants to build a\nCommutative Algebra neural Network (CANet). Integrated with transformer\nfeatures and auxiliary physical features, we apply CANet to tackle three key\ndomains for the first time: disease-associated mutations, mutation-induced\nprotein stability changes, and solubility changes upon mutations. Across six\nbenchmark tasks, CANet and its gradient boosting tree counterpart, CATree,\nconsistently attain state-of-the-art performance, achieving up to 7.5%\nimprovement in predictive accuracy. Our approach offers multiscale,\nmechanistic, interpretable,and generalizable models for predicting\ndisease-mutation associations."}
{"id": "2509.25509", "pdf": "https://arxiv.org/pdf/2509.25509", "abs": "https://arxiv.org/abs/2509.25509", "authors": ["Langzhou He", "Junyou Zhu", "Fangxin Wang", "Junhua Liu", "Haoyan Xu", "Yue Zhao", "Philip S. Yu", "Qitian Wu"], "title": "Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy with Preference Optimization", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Molecular foundation models are rapidly advancing scientific discovery, but\ntheir unreliability on out-of-distribution (OOD) samples severely limits their\napplication in high-stakes domains such as drug discovery and protein design. A\ncritical failure mode is chemical hallucination, where models make\nhigh-confidence yet entirely incorrect predictions for unknown molecules. To\naddress this challenge, we introduce Molecular Preference-Aligned Instance\nRanking (Mole-PAIR), a simple, plug-and-play module that can be flexibly\nintegrated with existing foundation models to improve their reliability on OOD\ndata through cost-effective post-training. Specifically, our method formulates\nthe OOD detection problem as a preference optimization over the estimated OOD\naffinity between in-distribution (ID) and OOD samples, achieving this goal\nthrough a pairwise learning objective. We show that this objective essentially\noptimizes AUROC, which measures how consistently ID and OOD samples are ranked\nby the model. Extensive experiments across five real-world molecular datasets\ndemonstrate that our approach significantly improves the OOD detection\ncapabilities of existing molecular foundation models, achieving up to 45.8%,\n43.9%, and 24.3% improvements in AUROC under distribution shifts of size,\nscaffold, and assay, respectively."}
{"id": "2509.26577", "pdf": "https://arxiv.org/pdf/2509.26577", "abs": "https://arxiv.org/abs/2509.26577", "authors": ["Chiara Mattamira", "Olivia Prosper Feldman"], "title": "Stochasticity and Practical Identifiability in Epidemic Models: A Monte Carlo Perspective", "categories": ["stat.ME", "q-bio.QM"], "comment": null, "summary": "Assessing the practical identifiability of epidemic models is essential for\ndetermining whether parameters can be meaningfully estimated from observed\ndata. Monte Carlo (MC) methods provide an accessible and intuitive framework;\nhowever, their standard implementation - perturbing deterministic trajectories\nwith independent Gaussian noise - rests on assumptions poorly suited to\nepidemic processes, which are inherently stochastic, temporally correlated, and\nhighly variable, especially in small populations or under slow transmission. In\nthis study, we investigate the structure of stochastic variability in the\nclassic Susceptible-Infected-Recovered (SIR) model across a range of\nepidemiological regimes, and assess whether it can be represented within the\nindependent Gaussian noise framework. We show that continuous-time Markov chain\n(CTMC) trajectories consistently exhibit super-Poissonian variability and\nstrong temporal dependence. Through coverage analysis, we further demonstrate\nthat independent Gaussian noise systematically underestimates the variability\nof the underlying stochastic process, leading to overly optimistic conclusions\nabout parameter identifiability. In addition, we propose a hybrid simulation\napproach that introduces time- and amplitude-dependent variability into\ndeterministic ODE trajectories, preserving computational efficiency while\ncapturing key features of epidemic stochasticity. Our findings highlight the\nlimitations of the standard MC algorithm and provide a pathway for\nincorporating more realistic noise structures into epidemic inference."}
