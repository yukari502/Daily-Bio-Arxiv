<div id=toc></div>

# Table of Contents

- [q-bio.QM](#q-bio.QM) [Total: 4]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [1] [Interval-Censored Survival Analysis of Grapevine Phenology: Thermal Controls on Flowering and Fruit Ripening](https://arxiv.org/abs/2510.09702)
*Sara Behnamian,Fatemeh Fogh*

Main category: q-bio.QM

TL;DR: 开发了一个可重复的工作流程，使用区间删失生存模型分析葡萄物候数据，将物候视为时间到事件的结果，结合天气数据来提取气候信号。


<details>
  <summary>Details</summary>
Motivation: 欧洲葡萄是气候敏感的多年生植物，其开花和成熟时间影响产量和品质。物候监测数据通常在不规则时间间隔收集，存在区间删失和右删失问题，需要开发稳健的分析方法。

Method: 将USA-NPN的状态和强度观测转换为区间边界，链接NASA POWER每日天气数据，使用参数加速失效时间模型（威布尔和对数逻辑分布），在固定季节前窗口汇总和标准化天气条件。

Result: 温暖季节前条件与较早的成熟相关，而开花响应在这些数据中较为温和且不确定；降水最多起次要作用。该方法能提取稳健的气候信号并保留观测不确定性。

Conclusion: 该方法展示了如何使用区间删失生存模型和外生天气窗口从公民科学物候数据中提取稳健气候信号，并可轻松推广到其他物种和网络。

Abstract: European grapevine (\textit{Vitis vinifera} L.) is a climate-sensitive
perennial whose flowering and ripening govern yield and quality. Phenological
records from monitoring programs are typically collected at irregular
intervals, so true transition dates are interval-censored, and many site-years
are right-censored. We develop a reproducible workflow that treats phenology as
a time-to-event outcome: Status \& Intensity observations from the USA-NPN are
converted to interval bounds, linked to NASA POWER daily weather, and analyzed
with parametric accelerated failure time (AFT) models (Weibull and
log-logistic). To avoid outcome-dependent bias from aggregating weather up to
the event date, antecedent conditions are summarized in fixed pre-season
windows and standardized; quality-control filters ensure adequate within-window
data coverage. Applied to flowering and ripening of \textit{V.~vinifera}, the
framework yields interpretable time-ratio effects and publication-ready tables
and figures. Warmer pre-season conditions are associated with earlier ripening,
whereas flowering responses are modest and uncertain in these data;
precipitation plays, at most, a secondary role. The approach demonstrates how
interval-censored survival models with exogenous weather windows can extract
robust climate signals from citizen-science phenology while preserving
observation uncertainty, and it generalizes readily to other species and
networks.

</details>


### [2] [MS2toImg: A Framework for Direct Bioactivity Prediction from Raw LC-MS/MS Data](https://arxiv.org/abs/2510.09716)
*Hansol Hong,Sangwon Lee,Jang-Ho Ha,Sung-June Chu,So-Hee An,Woo-Hyun Paek,Gyuhwa Chung,Kyoung Tai No*

Main category: q-bio.QM

TL;DR: MS2toImg将LC-MS/MS数据转换为二维图像，使用CNN直接预测生物活性，无需代谢物鉴定或特征对齐，解决了代谢组学中的识别瓶颈和对齐误差问题。


<details>
  <summary>Details</summary>
Motivation: 解决非靶向代谢组学中的识别瓶颈问题（大部分检测到的特征无法通过现有光谱库注释）和LC-MS/MS仪器重现性低导致的特征对齐误差问题。

Method: 将原始LC-MS/MS数据转换为代表每个样品全局碎裂模式的二维图像，然后使用卷积神经网络进行端到端的生物活性预测，无需显式特征工程或对齐。

Result: 在野生大豆样品和多种生物活性测定中，MS2toImg-CNN模型优于传统机器学习基线（如随机森林、PCA），在不同任务中表现出稳健的分类准确性。

Conclusion: 这种基于图像的免识别方法为复杂生物系统中的高通量功能筛选提供了新的范式，能够从非靶向代谢组学数据中实现更稳健和可扩展的生物活性预测。

Abstract: Untargeted metabolomics using LC-MS/MS offers the potential to
comprehensively profile the chemical diversity of biological samples. However,
the process is fundamentally limited by the "identification bottleneck," where
only a small fraction of detected features can be annotated using existing
spectral libraries, leaving the majority of data uncharacterized and unused. In
addition, the inherently low reproducibility of LC-MS/MS instruments introduces
alignment errors between runs, making feature alignment across large datasets
both error-prone and challenging. To overcome these constraints, we developed a
deep learning method that eliminates the requirement for metabolite
identification and reduces the influence of alignment inaccuracies. Here, we
propose MS2toImg, a method that converts raw LC-MS/MS data into a
two-dimensional images representing the global fragmentation pattern of each
sample. These images are then used as direct input for a convolutional neural
network (CNN), enabling end-to-end prediction of biological activity without
explicit feature engineering or alignment. Our approach was validated using
wild soybean samples and multiple bioactivity assays (e.g., DPPH, elastase
inhibition). The MS2toImg-CNN model outperformed conventional machine learning
baselines (e.g., Random Forest, PCA), demonstrating robust classification
accuracy across diverse tasks. By transforming raw spectral data into images,
our framework is inherently less sensitive to alignment errors caused by low
instrument reproducibility, as it leverages the overall fragmentation landscape
rather than relying on precise feature matching. This identification-free,
image-based approach enables more robust and scalable bioactivity prediction
from untargeted metabolomics data, offering a new paradigm for high-throughput
functional screening in complex biological systems.

</details>


### [3] [Domain Knowledge Infused Generative Models for Drug Discovery Synthetic Data](https://arxiv.org/abs/2510.09837)
*Bing Hu,Jong-Hoon Park,Helen Chen,Young-Rae Cho,Anita Layton*

Main category: q-bio.QM

TL;DR: 提出了xImagand-DKI模型，一种基于扩散的SMILES/蛋白质到药代动力学/药物-靶点相互作用生成模型，用于解决药物发现中数据重叠稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 药物发现AI面临的主要挑战是不同研究收集的药代动力学和药物-靶点相互作用数据集重叠有限，导致数据稀疏，影响下游研究。

Method: 使用SP2PKDTI扩散模型，基于SMILES和蛋白质输入生成PK和DTI属性，并融入基因本体和分子指纹的领域知识来提升性能。

Result: 生成的合成PK数据在单变量和双变量分布上与真实数据相似，能够有效填补PK和DTI数据集之间的空白。

Conclusion: xImagand-DKI是解决数据重叠稀疏性的有前景方案，可改善下游药物发现研究任务的性能。

Abstract: The role of Artificial Intelligence (AI) is growing in every stage of drug
development. Nevertheless, a major challenge in drug discovery AI remains: Drug
pharmacokinetic (PK) and Drug-Target Interaction (DTI) datasets collected in
different studies often exhibit limited overlap, creating data overlap
sparsity. Thus, data curation becomes difficult, negatively impacting
downstream research investigations in high-throughput screening, polypharmacy,
and drug combination. We propose xImagand-DKI, a novel
SMILES/Protein-to-Pharmacokinetic/DTI (SP2PKDTI) diffusion model capable of
generating an array of PK and DTI target properties conditioned on SMILES and
protein inputs that exhibit data overlap sparsity. We infuse additional
molecular and genomic domain knowledge from the Gene Ontology (GO) and
molecular fingerprints to further improve our model performance. We show that
xImagand-DKI-generated synthetic PK data closely resemble real data univariate
and bivariate distributions, and can adequately fill in gaps among PK and DTI
datasets. As such, xImagand-DKI is a promising solution for data overlap
sparsity and may improve performance for downstream drug discovery research
tasks. Code available at:
https://github.com/GenerativeDrugDiscovery/xImagand-DKI

</details>


### [4] [SeFEF: A Seizure Forecasting Evaluation Framework](https://arxiv.org/abs/2510.11275)
*Ana Sofia Carmo,Lourenço Abrunhosa Rodrigues,Ana Rita Peralta,Ana Fred,Carla Bentes,Hugo Plácido da Silva*

Main category: q-bio.QM

TL;DR: 提出了一个Python框架，用于标准化癫痫发作预测算法的开发、评估和文档化，旨在推动该领域的进展和临床转化。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作预测领域缺乏标准化阻碍了进展并限制了预测模型的临床转化，需要统一的开发评估框架。

Method: 开发了自动化数据处理、交叉验证、预测后处理、性能评估和报告的Python框架，支持多种预测时间范围，包含模型卡片记录实现细节和性能指标。

Result: 成功实现了三个概念验证模型，展示了框架的灵活性，并强调了由于概率缩放、校准和个体差异等因素需要仔细解释模型结果的重要性。

Conclusion: 该框架作为概念验证，旨在促进社区参与和实验，最终目标是为癫痫发作预测算法的开发和验证建立标准化方法共识。

Abstract: The lack of standardization in seizure forecasting slows progress in the
field and limits the clinical translation of forecasting models. In this work,
we introduce a Python-based framework aimed at streamlining the development,
assessment, and documentation of individualized seizure forecasting algorithms.
  The framework automates data labeling, cross-validation splitting, forecast
post-processing, performance evaluation, and reporting. It supports various
forecasting horizons and includes a model card that documents implementation
details, training and evaluation settings, and performance metrics. Three
different models were implemented as a proof-of-concept. The models leveraged
features extracted from time series data and seizure periodicity. Model
performance was assessed using time series cross-validation and key
deterministic and probabilistic metrics.
  Implementation of the three models was successful, demonstrating the
flexibility of the framework. The results also emphasize the importance of
careful model interpretation due to variations in probability scaling,
calibration, and subject-specific differences. Although formal usability
metrics were not recorded, empirical observations suggest reduced development
time and methodological consistency, minimizing unintentional variations that
could affect the comparability of different approaches.
  As a proof-of-concept, this validation is inherently limited, relying on a
single-user experiment without statistical analyses or replication across
independent datasets. At this stage, our objective is to make the framework
publicly available to foster community engagement, facilitate experimentation,
and gather feedback. In the long term, we aim to contribute to the
establishment of a consensus on a standardized methodology for the development
and validation of seizure forecasting algorithms in people with epilepsy.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [5] [A clustering algorithm for the single cell analysis of mixtures](https://arxiv.org/abs/2510.10614)
*Robert G. Cowell*

Main category: stat.AP

TL;DR: 提出了一种用于法医DNA混合物分析的概率聚类算法，通过分离单个细胞并使用PCR扩增短串联重复序列来生成单细胞电泳图，利用峰高信息将细胞按贡献者分组。


<details>
  <summary>Details</summary>
Motivation: 解决法医DNA混合物分析中单个细胞分离和分组的挑战，提高对贡献者识别的准确性。

Method: 使用概率聚类算法分析单细胞电泳图中的峰高信息，通过模拟实验验证算法性能。

Result: 模拟实验显示该聚类算法在正确分组单细胞和为已知基因型人员分配似然比方面表现优异。

Conclusion: 该概率聚类算法能有效分析法医DNA混合物，准确分组细胞并为相关人员提供可靠的似然比评估。

Abstract: A probabilistic clustering algorithm is proposed for the analysis of forensic
DNA mixtures in which individual cells are isolated and short tandem repeats
are amplified using the polymerase chain reaction to generate single cell
electropherograms. The task of the algorithm is to use the peak height
information in the electropherograms to group the cells according to their
contributors. Using a recently developed experimental set of individual cell
electropherograms, a large set of simulations shows that the proposed
clustering algorithm has excellent performance in correctly grouping single
cells, and for assigning likelihood ratios for persons of interest (of known
genotype).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [Combined Representation and Generation with Diffusive State Predictive Information Bottleneck](https://arxiv.org/abs/2510.09784)
*Richard John,Yunrui Qiu,Lukas Herron,Pratyush Tiwary*

Main category: cs.LG

TL;DR: 提出D-SPIB方法，结合时间延迟信息瓶颈和扩散模型，在分子科学中平衡表示学习和生成目标，能够从不同温度模拟轨迹中学习一致的热力学表示。


<details>
  <summary>Details</summary>
Motivation: 分子科学中数据收集昂贵且重要事件罕见，高维空间生成建模需要压缩到低维流形，这对下游任务（包括生成）尤为重要。

Method: 结合时间延迟信息瓶颈（用于表征分子重要表示）和扩散模型，在一个联合训练目标中实现。

Result: 在多个分子任务上进行了基准测试，展示了探索训练集外物理条件的潜力。

Conclusion: D-SPIB提供了一个灵活的架构，能够平衡表示学习和生成目标，并能从不同温度模拟中学习一致有用的热力学内部表示。

Abstract: Generative modeling becomes increasingly data-intensive in high-dimensional
spaces. In molecular science, where data collection is expensive and important
events are rare, compression to lower-dimensional manifolds is especially
important for various downstream tasks, including generation. We combine a
time-lagged information bottleneck designed to characterize molecular important
representations and a diffusion model in one joint training objective. The
resulting protocol, which we term Diffusive State Predictive Information
Bottleneck (D-SPIB), enables the balancing of representation learning and
generation aims in one flexible architecture. Additionally, the model is
capable of combining temperature information from different molecular
simulation trajectories to learn a coherent and useful internal representation
of thermodynamics. We benchmark D-SPIB on multiple molecular tasks and showcase
its potential for exploring physical conditions outside the training set.

</details>


### [7] [Augmenting generative models with biomedical knowledge graphs improves targeted drug discovery](https://arxiv.org/abs/2510.09914)
*Aditya Malusare,Vineet Punyamoorty,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: K-DREAM是一个利用知识图谱增强扩散生成模型的新框架，通过整合生物医学知识来生成更具生物相关性和治疗适用性的分子。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在分子生成方面表现出色，但缺乏对全面生物医学知识的整合，限制了生成分子的生物相关性和治疗潜力。

Method: 通过嵌入大规模知识图谱的结构化信息，将知识图谱与扩散生成模型相结合，引导分子生成朝向具有更高生物相关性的候选分子。

Result: 在靶向药物设计任务中，K-DREAM生成的候选药物具有更好的结合亲和力和预测疗效，超越了当前最先进的生成模型，并能针对多个靶点设计分子。

Conclusion: 知识增强的生成模型在理性药物设计中具有重要价值，对实际治疗开发具有相关性。

Abstract: Recent breakthroughs in generative modeling have demonstrated remarkable
capabilities in molecular generation, yet the integration of comprehensive
biomedical knowledge into these models has remained an untapped frontier. In
this study, we introduce K-DREAM (Knowledge-Driven Embedding-Augmented Model),
a novel framework that leverages knowledge graphs to augment diffusion-based
generative models for drug discovery. By embedding structured information from
large-scale knowledge graphs, K-DREAM directs molecular generation toward
candidates with higher biological relevance and therapeutic suitability. This
integration ensures that the generated molecules are aligned with specific
therapeutic targets, moving beyond traditional heuristic-driven approaches. In
targeted drug design tasks, K-DREAM generates drug candidates with improved
binding affinities and predicted efficacy, surpassing current state-of-the-art
generative models. It also demonstrates flexibility by producing molecules
designed for multiple targets, enabling applications to complex disease
mechanisms. These results highlight the utility of knowledge-enhanced
generative models in rational drug design and their relevance to practical
therapeutic development.

</details>


### [8] [MIEO: encoding clinical data to enhance cardiovascular event prediction](https://arxiv.org/abs/2510.11257)
*Davide Borghini,Davide Marchi,Angelo Nardone,Giordano Scerra,Silvia Giulia Galfrè,Alessandro Pingitore,Giuseppe Prencipe,Corrado Priami,Alina Sîrbu*

Main category: cs.LG

TL;DR: 使用自监督自动编码器处理临床数据中的标签稀缺和异构性问题，通过无标签数据构建潜在空间来预测心血管死亡，相比直接在原始数据上训练分类器获得了更好的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 临床数据存在两个主要问题：标签数据稀缺和数据异构性导致的缺失值。这些限制了机器学习方法在临床事件预测中的应用效果。

Method: 采用自监督自动编码器，利用无标签数据构建潜在空间，然后在该潜在空间上训练神经网络分类器来预测心血管死亡。

Result: 相比直接在原始数据上应用分类器，该方法在平衡准确率方面表现更好，特别是在无标签数据可用性可能增加的条件下。

Conclusion: 自监督自动编码器是解决临床数据标签稀缺和异构性问题的有前景方案，能够有效提升心血管死亡预测的准确性。

Abstract: As clinical data are becoming increasingly available, machine learning
methods have been employed to extract knowledge from them and predict clinical
events. While promising, approaches suffer from at least two main issues: low
availability of labelled data and data heterogeneity leading to missing values.
This work proposes the use of self-supervised auto-encoders to efficiently
address these challenges. We apply our methodology to a clinical dataset from
patients with ischaemic heart disease. Patient data is embedded in a latent
space, built using unlabelled data, which is then used to train a neural
network classifier to predict cardiovascular death. Results show improved
balanced accuracy compared to applying the classifier directly to the raw data,
demonstrating that this solution is promising, especially in conditions where
availability of unlabelled data could increase.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [9] [Does Re-referencing Matter? Large Laplacian Filter Optimizes Single-Trial P300 BCI Performance](https://arxiv.org/abs/2510.10733)
*Eva Guttmann-Flury,Jian Zhao,Mohamad Sawan*

Main category: q-bio.NC

TL;DR: 本研究系统评估了四种EEG重参考策略对P300分类准确率的影响，发现大拉普拉斯方法在保持P3a特异性的同时能解析分布式P3b网络，获得最佳分类性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号质量限制了脑机接口性能，其中重参考策略是一个普遍但常被忽视的预处理偏差。本研究旨在解决关于重参考必要性和最佳选择的争议。

Method: 采用量化方法评估四种重参考策略（无重参考、共同平均参考、小拉普拉斯、大拉普拉斯），使用62通道EEG数据（31名受试者，2520次试验），通过控制流程分离重参考对源空间重建和分类的影响。

Result: 大拉普拉斯方法获得最佳P300峰值分类准确率（混合方法81.57%；主要感兴趣区域75.97%）。性能呈现一致的统计显著层次：大拉普拉斯 > 共同平均参考 > 无重参考 > 小拉普拉斯。

Conclusion: 大拉普拉斯重参考策略在P300分类中表现最优，为统一方法学评估提供了基础，解决了重参考策略选择的争议。

Abstract: Electroencephalography (EEG) provides a non-invasive window into brain
activity, enabling Brain-Computer Interfaces (BCIs) for communication and
control. However, their performance is limited by signal fidelity issues, among
which the choice of re-referencing strategy is a pervasive but often overlooked
preprocessing bias. Addressing controversies about its necessity and optimal
choice, we adopted a quantified approach to evaluate four strategies - no
re-referencing, Common Average Reference (CAR), small Laplacian, and large
Laplacian - using 62-channels EEG (31 subjects, 2,520 trials). To our
knowledge, this is the first study systematically quantifying their impact on
single-trial P300 classification accuracy. Our controlled pipeline isolated
re-referencing effects for source-space reconstruction (eLORETA with Phase Lag
Index) and anatomically constrained classification. The large Laplacian
resolves distributed P3b networks while maintaining P3a specificity, achieving
the best P300 peak classification accuracy (81.57% hybrid method; 75.97%
majority regions of interest). Performance follows a consistent and
statistically significant hierarchy: large Laplacian > CAR > no re-reference >
small Laplacian, providing a foundation for unified methodological evaluation.

</details>


### [10] [The Cost of Simplicity: How Reducing EEG Electrodes Affects Source Localization and BCI Accuracy](https://arxiv.org/abs/2510.10770)
*Eva Guttmann-Flury,Yanyan Wei,Shan Zhao,Jian Zhao,Mohamad Sawan*

Main category: q-bio.NC

TL;DR: 该研究系统评估了62、32和16通道EEG配置对P300电位源定位和源连接性的影响，发现电极密度降低会导致源重建质量下降，并提出了sqrt(Re)缩放定律来描述电极减少比与定位精度之间的关系。


<details>
  <summary>Details</summary>
Motivation: EEG电极密度优化需要在实用性和信号保真度之间取得平衡，特别是对于源定位。减少电极可提高便携性，但其对神经源重建质量和源连接性的影响尚未得到充分研究。

Method: 使用固定、全自动处理流程系统评估62、32和16通道配置，分析Eye-BCI数据集的63个会话（31名受试者），采用严格的伪影校正和通道验证。

Result: 1）稀疏配置导致源重建质量逐渐下降，包括深部神经发生器模糊和时空畸变；2）发现sqrt(Re)缩放定律，量化电极减少比与定位精度的关系；3）减少配置可保留基本P300地形图，但高密度通道对可靠的深部源重建至关重要。

Conclusion: 本研究为电极选择建立了定量基准，对需要解剖精度的临床BCI应用具有重要影响。sqrt(Re)缩放定律可能提供了首个基于可接受误差范围或预期效应大小来确定最小电极密度的原则性方法。

Abstract: Electrode density optimization in electroencephalography (EEG)-based
Brain-Computer Interfaces (BCIs) requires balancing practical usability against
signal fidelity, particularly for source localization. Reducing electrodes
enhances portability but its effects on neural source reconstruction quality
and source connectivity - treated as proxies to BCI performance - remain
understudied. We address this gap through systematic evaluation of 62-, 32-,
and 16-channel configurations using a fixed, fully automated processing
pipeline applied to the well-characterized P300 potential. This approach's
rationale is to minimize variability and bias inherent to EEG analysis by
leveraging the P300's stimulus-locked reproducibility and pipeline
standardization. Analyzing 63 sessions (31 subjects) from the Eye-BCI dataset
with rigorous artifact correction and channel validation, we demonstrate: (1)
Progressive degradation in source reconstruction quality with sparser
configurations, including obscured deep neural generators and spatiotemporal
distortions; (2) A novel sqrt(Re) scaling law linking electrode reduction ratio
(Re) to localization accuracy - a previously unquantified relationship to the
best of our knowledge; (3) While reduced configurations preserve basic P300
topography and may suffice for communicative BCIs, higher-density channels are
essential for reliable deep source reconstruction. Overall, this study
establishes a first step towards quantitative benchmarks for electrode
selection, with critical implications for clinical BCIs requiring anatomical
precision in applications like neurodegenerative disease monitoring, where
compromised spatial resolution could mask pathological signatures. Most
importantly, the sqrt(Re) scaling law may provide the first principled method
to determine the minimal electrode density required based on acceptable error
margins or expected effect sizes.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [11] [An uncertainty-aware framework for data-efficient multi-view animal pose estimation](https://arxiv.org/abs/2510.09903)
*Lenny Aharon,Keemin Lee,Karan Sikka,Selmaan Chettih,Cole Hurwitz,Liam Paninski,Matthew R Whiteway*

Main category: cs.CV

TL;DR: 提出了一个综合的多视角姿态估计框架，结合了新颖的训练和后处理技术，通过模型蒸馏提高效率，在有限标注数据下实现准确跟踪和更好的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 解决当前多视角动物姿态估计方法在有限标注数据下准确跟踪困难和不确定性估计不足的问题，为科学研究中的动物行为量化提供可靠工具。

Method: 使用多视角transformer（MVT）同时处理所有视角信息，采用补丁掩码方案学习跨视角对应关系；结合几何一致性的3D增强和三角化损失；扩展非线性集成卡尔曼平滑器（EKS）并改进不确定性量化；设计蒸馏过程利用改进的预测生成高质量伪标签。

Result: 在三种不同动物物种（苍蝇、小鼠、山雀）上一致优于现有方法，每个组件都提供互补优势，实现了实用、不确定性感知的可靠姿态估计系统。

Conclusion: 该框架为现实世界数据约束下的下游行为分析提供了实用的不确定性感知系统，显著减少了对人工标注的依赖。

Abstract: Multi-view pose estimation is essential for quantifying animal behavior in
scientific research, yet current methods struggle to achieve accurate tracking
with limited labeled data and suffer from poor uncertainty estimates. We
address these challenges with a comprehensive framework combining novel
training and post-processing techniques, and a model distillation procedure
that leverages the strengths of these techniques to produce a more efficient
and effective pose estimator. Our multi-view transformer (MVT) utilizes
pretrained backbones and enables simultaneous processing of information across
all views, while a novel patch masking scheme learns robust cross-view
correspondences without camera calibration. For calibrated setups, we
incorporate geometric consistency through 3D augmentation and a triangulation
loss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to
the nonlinear case and enhance uncertainty quantification via a variance
inflation technique. Finally, to leverage the scaling properties of the MVT, we
design a distillation procedure that exploits improved EKS predictions and
uncertainty estimates to generate high-quality pseudo-labels, thereby reducing
dependence on manual labels. Our framework components consistently outperform
existing methods across three diverse animal species (flies, mice, chickadees),
with each component contributing complementary benefits. The result is a
practical, uncertainty-aware system for reliable pose estimation that enables
downstream behavioral analyses under real-world data constraints.

</details>
