{"id": "2510.13886", "pdf": "https://arxiv.org/pdf/2510.13886", "abs": "https://arxiv.org/abs/2510.13886", "authors": ["Pierre Fayolle", "Alexandre Bône", "Noëlie Debs", "Mathieu Naudin", "Pascal Bourdon", "Remy Guillevin", "David Helbert"], "title": "Physics-Informed autoencoder for DSC-MRI Perfusion post-processing: application to glioma grading", "categories": ["q-bio.QM", "cs.AI", "eess.IV", "eess.SP"], "comment": "5 pages, 5 figures, IEEE ISBI 2025, Houston, Tx, USA", "summary": "DSC-MRI perfusion is a medical imaging technique for diagnosing and\nprognosing brain tumors and strokes. Its analysis relies on mathematical\ndeconvolution, but noise or motion artifacts in a clinical environment can\ndisrupt this process, leading to incorrect estimate of perfusion parameters.\nAlthough deep learning approaches have shown promising results, their\ncalibration typically rely on third-party deconvolution algorithms to generate\nreference outputs and are bound to reproduce their limitations.\n  To adress this problem, we propose a physics-informed autoencoder that\nleverages an analytical model to decode the perfusion parameters and guide the\nlearning of the encoding network. This autoencoder is trained in a\nself-supervised fashion without any third-party software and its performance is\nevaluated on a database with glioma patients. Our method shows reliable results\nfor glioma grading in accordance with other well-known deconvolution algorithms\ndespite a lower computation time. It also achieved competitive performance even\nin the presence of high noise which is critical in a medical environment."}
{"id": "2510.13896", "pdf": "https://arxiv.org/pdf/2510.13896", "abs": "https://arxiv.org/abs/2510.13896", "authors": ["Xi Yu", "Yang Yang", "Qun Liu", "Yonghua Du", "Sean McSweeney", "Yuewei Lin"], "title": "GenCellAgent: Generalizable, Training-Free Cellular Image Segmentation via Large Language Model Agents", "categories": ["q-bio.QM", "cs.AI", "cs.CV", "cs.MA"], "comment": "43 pages", "summary": "Cellular image segmentation is essential for quantitative biology yet remains\ndifficult due to heterogeneous modalities, morphological variability, and\nlimited annotations. We present GenCellAgent, a training-free multi-agent\nframework that orchestrates specialist segmenters and generalist\nvision-language models via a planner-executor-evaluator loop (choose tool\n$\\rightarrow$ run $\\rightarrow$ quality-check) with long-term memory. The\nsystem (i) automatically routes images to the best tool, (ii) adapts on the fly\nusing a few reference images when imaging conditions differ from what a tool\nexpects, (iii) supports text-guided segmentation of organelles not covered by\nexisting models, and (iv) commits expert edits to memory, enabling\nself-evolution and personalized workflows. Across four cell-segmentation\nbenchmarks, this routing yields a 15.7\\% mean accuracy gain over\nstate-of-the-art baselines. On endoplasmic reticulum and mitochondria from new\ndatasets, GenCellAgent improves average IoU by 37.6\\% over specialist models.\nIt also segments novel objects such as the Golgi apparatus via iterative\ntext-guided refinement, with light human correction further boosting\nperformance. Together, these capabilities provide a practical path to robust,\nadaptable cellular image segmentation without retraining, while reducing\nannotation burden and matching user preferences."}
{"id": "2510.13897", "pdf": "https://arxiv.org/pdf/2510.13897", "abs": "https://arxiv.org/abs/2510.13897", "authors": ["Naomi Fridman", "Anat Goldstein"], "title": "Dual-attention ResNet outperforms transformers in HER2 prediction on DCE-MRI", "categories": ["q-bio.QM", "cs.AI"], "comment": null, "summary": "Breast cancer is the most diagnosed cancer in women, with HER2 status\ncritically guiding treatment decisions. Noninvasive prediction of HER2 status\nfrom dynamic contrast-enhanced MRI (DCE-MRI) could streamline diagnostics and\nreduce reliance on biopsy. However, preprocessing high-dynamic-range DCE-MRI\ninto standardized 8-bit RGB format for pretrained neural networks is\nnontrivial, and normalization strategy significantly affects model performance.\nWe benchmarked intensity normalization strategies using a Triple-Head\nDual-Attention ResNet that processes RGB-fused temporal sequences from three\nDCE phases. Trained on a multicenter cohort (n=1,149) from the I-SPY trials and\nexternally validated on BreastDCEDL_AMBL (n=43 lesions), our model outperformed\ntransformer-based architectures, achieving 0.75 accuracy and 0.74 AUC on I-SPY\ntest data. N4 bias field correction slightly degraded performance. Without\nfine-tuning, external validation yielded 0.66 AUC, demonstrating\ncross-institutional generalizability. These findings highlight the\neffectiveness of dual-attention mechanisms in capturing transferable\nspatiotemporal features for HER2 stratification, advancing reproducible deep\nlearning biomarkers in breast cancer imaging."}
{"id": "2510.13911", "pdf": "https://arxiv.org/pdf/2510.13911", "abs": "https://arxiv.org/abs/2510.13911", "authors": ["Jia Zhang", "Bodong Du", "Yitong Miao", "Dongwei Sun", "Xiangyong Cao"], "title": "OralGPT: A Two-Stage Vision-Language Model for Oral Mucosal Disease Diagnosis and Description", "categories": ["q-bio.QM"], "comment": null, "summary": "Oral mucosal diseases such as leukoplakia, oral lichen planus, and recurrent\n  aphthous ulcers exhibit diverse and overlapping visual features,\n  making diagnosis challenging for non-specialists. While vision-language\n  models (VLMs) have shown promise in medical image interpretation,\n  their application in oral healthcare remains underexplored due to\n  the lack of large-scale, well-annotated datasets. In this work, we present\n  \\textbf{OralGPT}, the first domain-specific two-stage vision-language\n  framework designed for oral mucosal disease diagnosis and captioning.\n  In Stage 1, OralGPT learns visual representations and disease-related\n  concepts from classification labels. In Stage 2, it enhances its language\n  generation ability using long-form expert-authored captions. To\n  overcome the annotation bottleneck, we propose a novel similarity-guided\n  data augmentation strategy that propagates descriptive knowledge from\n  expert-labeled images to weakly labeled ones. We also construct the\n  first benchmark dataset for oral mucosal diseases, integrating multi-source\n  image data with both structured and unstructured textual annotations.\n  Experimental results on four common oral conditions demonstrate that\n  OralGPT achieves competitive diagnostic performance while generating\n  fluent, clinically meaningful image descriptions. This study\n  provides a foundation for language-assisted diagnostic tools in oral\n  healthcare."}
{"id": "2510.13932", "pdf": "https://arxiv.org/pdf/2510.13932", "abs": "https://arxiv.org/abs/2510.13932", "authors": ["Henrik Podéus", "Gustav Magnusson", "Sasan Keshmiri", "Kajsa Tunedal", "Nicolas Sundqvist", "William Lövfors", "Gunnar Cedersund"], "title": "SUND: simulation using nonlinear dynamic models - a toolbox for simulating multi-level, time-dynamic systems in a modular way", "categories": ["q-bio.QM", "65L05 (Primary) 37M05, 92C42 (Secondary)"], "comment": "6 pages, 1 figure, software paper. The last two listed authors\n  contributed equally to this work. Gunnar Cedersund is the corresponding\n  author", "summary": "When modeling complex, hierarchical, and time-dynamic systems, such as\nbiological systems, good computational tools are essential. Current tools,\nwhile powerful, often lack comprehensive frameworks for modular model\ncomposition, hierarchical system building, and time-dependent input handling,\nparticularly within the Python ecosystem. We present SUND (Simulation Using\nNonlinear Dynamic models), a Python toolbox designed to address these\nchallenges. SUND provides a unified framework for defining, combining, and\nsimulating multi-level time-dynamic systems. The toolbox enables users to\ndefine models with interconnectable inputs and outputs, facilitating the\nconstruction of complex systems from simpler, reusable components. It supports\ntime-dependent functions and piecewise constant inputs, enabling intuitive\nsimulation of various experimental conditions such as multiple dosing schemes.\nWe demonstrate the toolbox's capabilities through simulation of a multi-level\nhuman glucose-insulin system model, showcasing its flexibility in handling\nmultiple temporal scales, and levels of biological detail. SUND is open-source,\neasily extensible, and available at PyPI (https://pypi.org/project/sund/) and\nat Gitlab (https://gitlab.liu.se/ISBgroup/projects/sund/)."}
{"id": "2510.14143", "pdf": "https://arxiv.org/pdf/2510.14143", "abs": "https://arxiv.org/abs/2510.14143", "authors": ["Alexandr A. Kalinin", "Anne E. Carpenter", "Shantanu Singh", "Matthew J. O'Meara"], "title": "cubic: CUDA-accelerated 3D Bioimage Computing", "categories": ["cs.CV", "q-bio.QM", "92C55, 68U10", "I.4.0; J.3"], "comment": "accepted to BioImage Computing workshop @ ICCV 2025", "summary": "Quantitative analysis of multidimensional biological images is useful for\nunderstanding complex cellular phenotypes and accelerating advances in\nbiomedical research. As modern microscopy generates ever-larger 2D and 3D\ndatasets, existing computational approaches are increasingly limited by their\nscalability, efficiency, and integration with modern scientific computing\nworkflows. Existing bioimage analysis tools often lack application programmable\ninterfaces (APIs), do not support graphics processing unit (GPU) acceleration,\nlack broad 3D image processing capabilities, and/or have poor interoperability\nfor compute-heavy workflows. Here, we introduce cubic, an open-source Python\nlibrary that addresses these challenges by augmenting widely used SciPy and\nscikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.\ncubic's API is device-agnostic and dispatches operations to GPU when data\nreside on the device and otherwise executes on CPU, seamlessly accelerating a\nbroad range of image processing routines. This approach enables GPU\nacceleration of existing bioimage analysis workflows, from preprocessing to\nsegmentation and feature extraction for 2D and 3D data. We evaluate cubic both\nby benchmarking individual operations and by reproducing existing deconvolution\nand segmentation pipelines, achieving substantial speedups while maintaining\nalgorithmic fidelity. These advances establish a robust foundation for\nscalable, reproducible bioimage analysis that integrates with the broader\nPython scientific computing ecosystem, including other GPU-accelerated methods,\nenabling both interactive exploration and automated high-throughput analysis\nworkflows. cubic is openly available at\nhttps://github$.$com/alxndrkalinin/cubic"}
{"id": "2510.14188", "pdf": "https://arxiv.org/pdf/2510.14188", "abs": "https://arxiv.org/abs/2510.14188", "authors": ["Eric Albers", "Paul Marriott", "Masami Tatsuno"], "title": "Using Information Geometry to Characterize Higher-Order Interactions in EEG", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "In neuroscience, methods from information geometry (IG) have been\nsuccessfully applied in the modelling of binary vectors from spike train data,\nusing the orthogonal decomposition of the Kullback-Leibler divergence and\nmutual information to isolate different orders of interaction between neurons.\nWhile spike train data is well-approximated with a binary model, here we apply\nthese IG methods to data from electroencephalography (EEG), a continuous signal\nrequiring appropriate discretization strategies. We developed and compared\nthree different binarization methods and used them to identify third-order\ninteractions in an experiment involving imagined motor movements. The\nstatistical significance of these interactions was assessed using\nphase-randomized surrogate data that eliminated higher-order dependencies while\npreserving the spectral characteristics of the original signals. We validated\nour approach by implementing known second- and third-order dependencies in a\nforward model and quantified information attenuation at different steps of the\nanalysis. This revealed that the greatest loss in information occurred when\ngoing from the idealized binary case to enforcing these dependencies using\noscillatory signals. When applied to the real EEG dataset, our analysis\ndetected statistically significant third-order interactions during the task\ncondition despite the relatively sparse data (45 trials per condition). This\nwork demonstrates that IG methods can successfully extract genuine higher-order\ndependencies from continuous neural recordings when paired with appropriate\nbinarization schemes."}
{"id": "2510.14481", "pdf": "https://arxiv.org/pdf/2510.14481", "abs": "https://arxiv.org/abs/2510.14481", "authors": ["Seong Jun Park"], "title": "Viral population dynamics at the cellular level, considering the replication cycle", "categories": ["q-bio.PE", "q-bio.QM"], "comment": null, "summary": "Viruses are microscopic infectious agents that require a host cell for\nreplication. Viral replication occurs in several stages, and the completion\ntime for each stage varies due to differences in the cellular environment.\nThus, the time to complete each stage in viral replication is a random\nvariable. However, no analytic expression exists for the viral population at\nthe cellular level when the completion time for each process constituting viral\nreplication is a random variable. This paper presents a simplified model of\nviral replication, treating each stage as a renewal process with independently\nand identically distributed completion times. Using the proposed model, we\nderive an analytical formula for viral populations at the cellular level, based\non viewing viral replication as a birth-death process. The mean viral count is\nexpressed via probability density functions representing the completion time\nfor each step in the replication process. This work validates the results with\nstochastic simulations. This study provides a new quantitative framework for\nunderstanding viral infection dynamics."}
