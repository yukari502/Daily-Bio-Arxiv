{"id": "2510.20866", "pdf": "https://arxiv.org/pdf/2510.20866", "abs": "https://arxiv.org/abs/2510.20866", "authors": ["Christopher S. Bird", "Antonio P. L. Bo", "Wei Lu", "Taylor J. M. Dick"], "title": "Ultra-wideband radar to measure in vivo musculoskeletal forces", "categories": ["q-bio.QM"], "comment": "25 pages, 6 main figures, 5 extended data figures", "summary": "Accurate measures of musculoskeletal forces are critical for clinicians,\nbiomechanists, and engineers, yet direct measurement is highly invasive and\ncurrent estimation methods remain limited in accuracy. Here, we demonstrate the\napplication of ultra-wideband radar to non-invasively estimate musculoskeletal\nforces by measuring changes in the electromagnetic properties of contracting\nmuscles, in muscles with different structural properties, during various static\nand dynamic conditions, and in the presence of fatigue. First, we show that\nultra-wideband radar scans of muscle can reliably track isometric force in a\nunipennate knee extensor (vastus lateralis) and a bipennate ankle dorsiflexor\n(tibialis anterior). Next, we integrate radar signals within machine-learning\nand linear models to estimate musculoskeletal forces during fatiguing\nisometric, and dynamic knee extension contractions, with exceptional accuracy\n(test R2>0.984; errors<3.3%). Finally, we identify frequency-dependent effects\nof musculoskeletal forces on ultra-wideband radar signals, that are independent\nof physiological and structural features known to influence muscle force.\nTogether, these findings establish ultra-wideband radar as a powerful,\nnon-invasive approach for quantifying in vivo musculoskeletal forces, with\ntransformative potential for wearable assistive technologies, biomechanics, and\nrehabilitation."}
{"id": "2510.21281", "pdf": "https://arxiv.org/pdf/2510.21281", "abs": "https://arxiv.org/abs/2510.21281", "authors": ["Christian Salomonsen", "Kristoffer K. Wickstrøm", "Samuel Kuttner", "Elisabeth Wetzer"], "title": "Physics-Informed Deep Learning for Improved Input Function Estimation in Motion-Blurred Dynamic [${}^{18}$F]FDG PET Images", "categories": ["q-bio.QM", "cs.CV"], "comment": "12 pages, 4 figures, 1 table. Preprint: Accepted to PRIME @ MICCAI\n  2025. This is the submitted (pre-review) version (url:\n  https://openreview.net/forum?id=twg1nba5ep)", "summary": "Kinetic modeling enables \\textit{in vivo} quantification of tracer uptake and\nglucose metabolism in [${}^{18}$F]Fluorodeoxyglucose ([${}^{18}$F]FDG) dynamic\npositron emission tomography (dPET) imaging of mice. However, kinetic modeling\nrequires the accurate determination of the arterial input function (AIF) during\nimaging, which is time-consuming and invasive. Recent studies have shown the\nefficacy of using deep learning to directly predict the input function,\nsurpassing established methods such as the image-derived input function (IDIF).\nIn this work, we trained a physics-informed deep learning-based input function\nprediction model (PIDLIF) to estimate the AIF directly from the PET images,\nincorporating a kinetic modeling loss during training. The proposed method uses\na two-tissue compartment model over two regions, the myocardium and brain of\nthe mice, and is trained on a dataset of 70 [${}^{18}$F]FDG dPET images of mice\naccompanied by the measured AIF during imaging. The proposed method had\ncomparable performance to the network without a physics-informed loss, and when\nsudden movement causing blurring in the images was simulated, the PIDLIF model\nmaintained high performance in severe cases of image degradation. The proposed\nphysics-informed method exhibits an improved robustness that is promoted by\nphysically constraining the problem, enforcing consistency for\nout-of-distribution samples. In conclusion, the PIDLIF model offers insight\ninto the effects of leveraging physiological distribution mechanics in mice to\nguide a deep learning-based AIF prediction network in images with severe\ndegradation as a result of blurring due to movement during imaging."}
{"id": "2510.21484", "pdf": "https://arxiv.org/pdf/2510.21484", "abs": "https://arxiv.org/abs/2510.21484", "authors": ["Uwe Schmitt", "Jethro L. Hemmann", "Nicola Zamboni", "Julia A. Vorholt", "Patrick Kiefer"], "title": "eMZed 3: flexible and interactive development of scalable LC-MS/MS data analysis workflows in Python", "categories": ["q-bio.QM"], "comment": "7 pages, 1 figure", "summary": "Liquid chromatography-mass spectrometry (LC-MS/MS) data analysis requires\nadaptable software solutions to meet diverse analytical needs. We present eMZed\n3, a modern Python framework for flexible and interactive analysis of LC-MS/MS\ndata. eMZed 3 enables users to develop scalable workflows tailored to their\nspecific requirements while leveraging Python's extensive ecosystem of\nlibraries. Building on its predecessor, eMZed 3 is now Python 3-based and\nincludes substantial enhancements, including support for chromatogram-based\nLC-MS data, a new SQLite-based backend supporting optional out-of-memory\nprocessing, and rich interactive visualization tools. Compared to the previous\nversion, eMZed 3 is now split into three packages: emzed (core\nfunctionalities), emzed-gui (interactive data visualization), and emzed-spyder\n(an integrated development environment). This modular architecture allows\nstraightforward integration of the emzed core library into headless Python\nenvironments, including computational notebooks (such as Jupyter) or\nhigh-performance computing clusters. eMZed 3 incorporates well-established\nlibraries such as OpenMS, and is highly suited for both targeted and untargeted\nmetabolomics. Overall, eMZed 3 supports the efficient development of scalable\nand reproducible LC-MS data analysis and is accessible to both novice and\nadvanced programmers.\n  Availability and Implementation: eMZed 3 and its documentation are freely\navailable at https://emzed.ethz.ch, the source code is hosted at\nhttps://gitlab.com/groups/emzed3. An online-executable example workflow is\navailable on Binder at:\nhttps://mybinder.org/v2/gl/emzed3%2Femzed-example-workflow/HEAD?labpath=example.ipynb."}
{"id": "2510.21280", "pdf": "https://arxiv.org/pdf/2510.21280", "abs": "https://arxiv.org/abs/2510.21280", "authors": ["Christiaan M. Geldenhuys", "Günther Tonitz", "Thomas R. Niesler"], "title": "WhaleVAD-BPN: Improving Baleen Whale Call Detection with Boundary Proposal Networks and Post-processing Optimisation", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "q-bio.QM"], "comment": null, "summary": "While recent sound event detection (SED) systems can identify baleen whale\ncalls in marine audio, challenges related to false positive and minority-class\ndetection persist. We propose the boundary proposal network (BPN), which\nextends an existing lightweight SED system. The BPN is inspired by work in\nimage object detection and aims to reduce the number of false positive\ndetections. It achieves this by using intermediate latent representations\ncomputed within the backbone classification model to gate the final output.\nWhen added to an existing SED system, the BPN achieves a 16.8 % absolute\nincrease in precision, as well as 21.3 % and 9.4 % improvements in the F1-score\nfor minority-class d-calls and bp-calls, respectively. We further consider two\napproaches to the selection of post-processing hyperparameters: a\nforward-search and a backward-search. By separately optimising event-level and\nframe-level hyperparameters, these two approaches lead to considerable\nperformance improvements over parameters selected using empirical methods. The\ncomplete WhaleVAD-BPN system achieves a cross-validated development F1-score of\n0.475, which is a 9.8 % absolute improvement over the baseline."}
{"id": "2510.21664", "pdf": "https://arxiv.org/pdf/2510.21664", "abs": "https://arxiv.org/abs/2510.21664", "authors": ["Riya Gupta", "Yiwei Zong", "Dennis H. Murphree"], "title": "Foundation Models in Dermatopathology: Skin Tissue Classification", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "The rapid generation of whole-slide images (WSIs) in dermatopathology\nnecessitates automated methods for efficient processing and accurate\nclassification. This study evaluates the performance of two foundation models,\nUNI and Virchow2, as feature extractors for classifying WSIs into three\ndiagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-level\nembeddings were aggregated into slide-level features using a mean-aggregation\nstrategy and subsequently used to train multiple machine learning classifiers,\nincluding logistic regression, gradient-boosted trees, and random forest\nmodels. Performance was assessed using precision, recall, true positive rate,\nfalse positive rate, and the area under the receiver operating characteristic\ncurve (AUROC) on the test set. Results demonstrate that patch-level features\nextracted using Virchow2 outperformed those extracted via UNI across most\nslide-level classifiers, with logistic regression achieving the highest\naccuracy (90%) for Virchow2, though the difference was not statistically\nsignificant. The study also explored data augmentation techniques and image\nnormalization to enhance model robustness and generalizability. The\nmean-aggregation approach provided reliable slide-level feature\nrepresentations. All experimental results and metrics were tracked and\nvisualized using WandB.ai, facilitating reproducibility and interpretability.\nThis research highlights the potential of foundation models for automated WSI\nclassification, providing a scalable and effective approach for\ndermatopathological diagnosis while paving the way for future advancements in\nslide-level representation learning."}
