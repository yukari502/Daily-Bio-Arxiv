{"id": "2511.04637", "pdf": "https://arxiv.org/pdf/2511.04637", "abs": "https://arxiv.org/abs/2511.04637", "authors": ["Madison Caballero", "Behrang Mahjani"], "title": "Advancing Risk Gene Discovery Across the Allele Frequency Spectrum", "categories": ["q-bio.GN"], "comment": "Review; 31 pages", "summary": "The discovery of genetic risk factors has transformed human genetics, yet the\npace of new gene identification has slowed despite the exponential expansion of\nsequencing and biobank resources. Current approaches are optimized for the\nextremes of the allele frequency spectrum: rare, high-penetrance variants\nidentified through burden testing, and common, low-effect variants mapped by\ngenome-wide association studies. Between these extremes lies variants of\nintermediate frequency and effect size where statistical power is limited,\npathogenicity is often misclassified, and gene discovery lags behind empirical\nevidence of heritable contribution. This 'missing middle' represents a critical\nblind spot across disease areas, from neurodevelopmental and psychiatric\ndisorders to cancer and aging. In this review, we organize strategies for risk\ngene identification by variant frequency class, highlighting methodological\nstrengths and constraints at each scale. We draw on lessons across fields to\nillustrate how innovations in variant annotation, joint modeling, phenotype\nrefinement, and network-based inference can extend discovery into the\nintermediate range. By framing the frequency spectrum as a unifying axis, we\nprovide a conceptual map of current capabilities, their limitations, and\nemerging directions toward more comprehensive risk gene discovery.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u9057\u4f20\u98ce\u9669\u57fa\u56e0\u53d1\u73b0\u4e2d\u7684\"\u7f3a\u5931\u4e2d\u95f4\"\u95ee\u9898\uff0c\u5373\u4e2d\u7b49\u9891\u7387\u548c\u4e2d\u7b49\u6548\u5e94\u5927\u5c0f\u7684\u53d8\u5f02\u5728\u7edf\u8ba1\u529f\u6548\u3001\u81f4\u75c5\u6027\u5206\u7c7b\u548c\u57fa\u56e0\u53d1\u73b0\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5e76\u63d0\u51fa\u4e86\u8de8\u9886\u57df\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5f53\u524d\u9057\u4f20\u98ce\u9669\u56e0\u7d20\u53d1\u73b0\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u7f55\u89c1\u9ad8\u5916\u663e\u7387\u53d8\u5f02\u548c\u5e38\u89c1\u4f4e\u6548\u5e94\u53d8\u5f02\uff0c\u800c\u4e2d\u7b49\u9891\u7387\u548c\u6548\u5e94\u5927\u5c0f\u7684\u53d8\u5f02\u6210\u4e3a\u57fa\u56e0\u53d1\u73b0\u7684\u76f2\u70b9\uff0c\u8fd9\u9650\u5236\u4e86\u4eba\u7c7b\u9057\u4f20\u5b66\u7684\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u6309\u53d8\u5f02\u9891\u7387\u7c7b\u522b\u7ec4\u7ec7\u98ce\u9669\u57fa\u56e0\u8bc6\u522b\u7b56\u7565\uff0c\u5206\u6790\u5404\u5c3a\u5ea6\u7684\u65b9\u6cd5\u5b66\u4f18\u52bf\u548c\u7ea6\u675f\uff0c\u5e76\u6574\u5408\u53d8\u5f02\u6ce8\u91ca\u3001\u8054\u5408\u5efa\u6a21\u3001\u8868\u578b\u7ec6\u5316\u548c\u57fa\u4e8e\u7f51\u7edc\u7684\u63a8\u65ad\u7b49\u521b\u65b0\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u5c06\u9891\u7387\u8c31\u4f5c\u4e3a\u7edf\u4e00\u8f74\u7ebf\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5f53\u524d\u80fd\u529b\u3001\u5c40\u9650\u6027\u548c\u65b0\u5174\u65b9\u5411\uff0c\u4e3a\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u98ce\u9669\u57fa\u56e0\u53d1\u73b0\u63d0\u4f9b\u8def\u7ebf\u56fe\u3002", "conclusion": "\u901a\u8fc7\u8de8\u9886\u57df\u6574\u5408\u521b\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6269\u5c55\u53d1\u73b0\u5230\u4e2d\u7b49\u9891\u7387\u53d8\u5f02\u8303\u56f4\uff0c\u586b\u8865\u9057\u4f20\u98ce\u9669\u57fa\u56e0\u53d1\u73b0\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u63a8\u52a8\u66f4\u5168\u9762\u7684\u75be\u75c5\u9057\u4f20\u673a\u5236\u7406\u89e3\u3002"}}
{"id": "2511.03976", "pdf": "https://arxiv.org/pdf/2511.03976", "abs": "https://arxiv.org/abs/2511.03976", "authors": ["Xu Zou"], "title": "PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "comment": "preprint", "summary": "Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable\nevolutionary trajectory, characterized by the continual emergence of\nimmune-evasive variants. This poses persistent challenges to public health and\nvaccine development.\n  While large-scale generative pre-trained transformers (GPTs) have\nrevolutionized the modeling of sequential data, their direct applications to\nnoisy viral genomic sequences are limited. In this paper, we introduce\nPETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based\non evolutionary trajectories derived from phylogenetic trees rather than raw\nRNA sequences. This method effectively mitigates sequencing noise and captures\nthe hierarchical structure of viral evolution.\n  With a weighted training framework to address substantial geographical and\ntemporal imbalances in global sequence data, PETRA excels in predicting future\nSARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide\nmutations and 17.10\\% for spike amino-acid mutations, compared to 0.49% and\n6.64% respectively for the best baseline. PETRA also demonstrates its ability\nto aid in the real-time mutation prediction of major clades like 24F(XEC) and\n25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra", "AI": {"tldr": "PETRA\u662f\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u8f68\u8ff9\u800c\u975e\u539f\u59cbRNA\u5e8f\u5217\u7684Transformer\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u53d1\u80b2\u6811\u6784\u5efa\u8fdb\u5316\u8f68\u8ff9\uff0c\u6709\u6548\u51cf\u8f7b\u6d4b\u5e8f\u566a\u58f0\u5e76\u6355\u6349\u75c5\u6bd2\u8fdb\u5316\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5728\u9884\u6d4bSARS-CoV-2\u672a\u6765\u7a81\u53d8\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "SARS-CoV-2\u5177\u6709\u5feb\u901f\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u8fdb\u5316\u8f68\u8ff9\uff0c\u6301\u7eed\u51fa\u73b0\u514d\u75ab\u9003\u9038\u53d8\u4f53\uff0c\u5bf9\u516c\u5171\u536b\u751f\u548c\u75ab\u82d7\u5f00\u53d1\u6784\u6210\u6311\u6218\u3002\u867d\u7136\u5927\u578b\u751f\u6210\u9884\u8bad\u7ec3Transformer\u5728\u5e8f\u5217\u6570\u636e\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u5608\u6742\u7684\u75c5\u6bd2\u57fa\u56e0\u7ec4\u5e8f\u5217\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faPETRA\u65b9\u6cd5\uff0c\u57fa\u4e8e\u7cfb\u7edf\u53d1\u80b2\u6811\u63d0\u53d6\u7684\u8fdb\u5316\u8f68\u8ff9\u800c\u975e\u539f\u59cbRNA\u5e8f\u5217\uff0c\u91c7\u7528\u52a0\u6743\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u5168\u7403\u5e8f\u5217\u6570\u636e\u7684\u5730\u7406\u548c\u65f6\u95f4\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "PETRA\u5728\u9884\u6d4bSARS-CoV-2\u672a\u6765\u7a81\u53d8\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u6838\u82f7\u9178\u7a81\u53d8\u7684\u52a0\u6743recall@1\u8fbe\u52309.45%\uff0c\u523a\u7a81\u86cb\u767d\u6c28\u57fa\u9178\u7a81\u53d8\u4e3a17.10%\uff0c\u800c\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u5206\u522b\u4e3a0.49%\u548c6.64%\u3002\u8be5\u65b9\u6cd5\u8fd8\u80fd\u5b9e\u65f6\u9884\u6d4b\u4e3b\u8981\u652f\u7cfb\u7684\u7a81\u53d8\u3002", "conclusion": "PETRA\u901a\u8fc7\u5229\u7528\u8fdb\u5316\u8f68\u8ff9\u800c\u975e\u539f\u59cb\u5e8f\u5217\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u6d4b\u5e8f\u566a\u58f0\u5e76\u6355\u6349\u4e86\u75c5\u6bd2\u8fdb\u5316\u7684\u5c42\u6b21\u7ed3\u6784\uff0c\u5728\u7a81\u53d8\u9884\u6d4b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u65f6\u76d1\u6d4b\u75c5\u6bd2\u8fdb\u5316\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2511.03751", "pdf": "https://arxiv.org/pdf/2511.03751", "abs": "https://arxiv.org/abs/2511.03751", "authors": ["Hossein Fathollahian", "Siyuan Zhao", "Nafiul Nipu", "G. Elisabeta Marai"], "title": "Attention-based ROI Discovery in 3D Tissue Images", "categories": ["q-bio.QM"], "comment": "2 pages, 3 figures;", "summary": "High-dimensional tissue imaging generates highly complex 3D data containing\nmultiple biomarkers, making it challenging to identify biologically relevant\nregions without an expert user specifying manual labels for regions of\ninterest. We introduce an approach to automatically identifying regions of\ninterest (ROIs) in the 3D microscopy data. Our approach is based on a novel\nself-supervised multi-layer graph attention network (SSGAT), coupled with a\nReact interactive interface wrapped around Vitessce. SSGAT employs an\nadversarial self-supervised learning objective to identify meaningful immune\nmicroenvironments through marker interactions. Our method reveals complex\nspatial bioreactions that can be visually assessed to assess their distribution\nacross tissue. Index Terms: Biomedical visualization, graph attention\nnetworks,self-supervised learning, spatial interaction analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u76d1\u7763\u591a\u5c42\u56fe\u6ce8\u610f\u529b\u7f51\u7edc(SSGAT)\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408React\u4ea4\u4e92\u754c\u9762\uff0c\u81ea\u52a8\u8bc6\u522b3D\u663e\u5fae\u955c\u6570\u636e\u4e2d\u7684\u611f\u5174\u8da3\u533a\u57df\uff0c\u65e0\u9700\u4e13\u5bb6\u624b\u52a8\u6807\u6ce8\u3002", "motivation": "\u9ad8\u7ef4\u7ec4\u7ec7\u6210\u50cf\u751f\u6210\u5305\u542b\u591a\u4e2a\u751f\u7269\u6807\u5fd7\u7269\u7684\u590d\u67423D\u6570\u636e\uff0c\u5728\u6ca1\u6709\u4e13\u5bb6\u624b\u52a8\u6807\u6ce8\u611f\u5174\u8da3\u533a\u57df\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc6\u522b\u751f\u7269\u5b66\u76f8\u5173\u533a\u57df\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u81ea\u76d1\u7763\u591a\u5c42\u56fe\u6ce8\u610f\u529b\u7f51\u7edc(SSGAT)\uff0c\u7ed3\u5408\u5bf9\u6297\u6027\u81ea\u76d1\u7763\u5b66\u4e60\u76ee\u6807\uff0c\u901a\u8fc7\u6807\u8bb0\u7269\u76f8\u4e92\u4f5c\u7528\u8bc6\u522b\u6709\u610f\u4e49\u7684\u514d\u75ab\u5fae\u73af\u5883\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63ed\u793a\u590d\u6742\u7684\u7a7a\u95f4\u751f\u7269\u53cd\u5e94\uff0c\u53ef\u89c6\u89c9\u8bc4\u4f30\u5176\u5728\u7ec4\u7ec7\u4e2d\u7684\u5206\u5e03\u3002", "conclusion": "SSGAT\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u8bc6\u522b3D\u663e\u5fae\u955c\u6570\u636e\u4e2d\u7684\u611f\u5174\u8da3\u533a\u57df\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u53ef\u89c6\u5316\u63d0\u4f9b\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.03767", "pdf": "https://arxiv.org/pdf/2511.03767", "abs": "https://arxiv.org/abs/2511.03767", "authors": ["Adam M. Saunders", "Michael E. Kim", "Gaurav Rudravaram", "Lucas W. Remedios", "Chloe Cho", "Elyssa M. McMaster", "Daniel R. Gillis", "Yihao Liu", "Lianrui Zuo", "Bennett A. Landman", "Tonia S. Rex"], "title": "Phenotype discovery of traumatic brain injury segmentations from heterogeneous multi-site data", "categories": ["q-bio.QM", "eess.IV"], "comment": "13 pages, 7 figures. Accepted to SPIE Medical Imaging 2026: Image\n  Processing", "summary": "Traumatic brain injury (TBI) is intrinsically heterogeneous, and typical\nclinical outcome measures like the Glasgow Coma Scale complicate this\ndiversity. The large variability in severity and patient outcomes render it\ndifficult to link structural damage to functional deficits. The Federal\nInteragency Traumatic Brain Injury Research (FITBIR) repository contains\nlarge-scale multi-site magnetic resonance imaging data of varying resolutions\nand acquisition parameters (25 shared studies with 7,693 sessions that have\nage, sex and TBI status defined - 5,811 TBI and 1,882 controls). To reveal\nshared pathways of injury of TBI through imaging, we analyzed T1-weighted\nimages from these sessions by first harmonizing to a local dataset and\nsegmenting 132 regions of interest (ROIs) in the brain. After running quality\nassurance, calculating the volumes of the ROIs, and removing outliers, we\ncalculated the z-scores of volumes for all participants relative to the mean\nand standard deviation of the controls. We regressed out sex, age, and total\nbrain volume with a multivariate linear regression, and we found significant\ndifferences in 37 ROIs between subjects with TBI and controls (p < 0.05 with\nindependent t-tests with false discovery rate correction). We found that\ndifferences originated in 1) the brainstem, occipital pole and structures\nposterior to the orbit, 2) subcortical gray matter and insular cortex, and 3)\ncerebral and cerebellar white matter using independent component analysis and\nclustering the component loadings of those with TBI.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790FITBIR\u6570\u636e\u5e93\u4e2d\u7684\u5927\u89c4\u6a21MRI\u6570\u636e\uff0c\u8bc6\u522b\u4e86\u521b\u4f24\u6027\u8111\u635f\u4f24\u60a3\u8005\u4e0e\u5bf9\u7167\u7ec4\u572837\u4e2a\u8111\u533a\u4f53\u79ef\u4e0a\u7684\u663e\u8457\u5dee\u5f02\uff0c\u63ed\u793a\u4e86TBI\u635f\u4f24\u7684\u5171\u4eab\u8def\u5f84\u4e3b\u8981\u6d89\u53ca\u8111\u5e72\u3001\u6795\u6781\u3001\u76ae\u5c42\u4e0b\u7070\u8d28\u3001\u5c9b\u53f6\u76ae\u5c42\u4ee5\u53ca\u8111\u767d\u8d28\u533a\u57df\u3002", "motivation": "\u521b\u4f24\u6027\u8111\u635f\u4f24\u5177\u6709\u9ad8\u5ea6\u5f02\u8d28\u6027\uff0c\u4e34\u5e8a\u7ed3\u679c\u6d4b\u91cf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8fd9\u79cd\u591a\u6837\u6027\u3002\u60a3\u8005\u4e25\u91cd\u7a0b\u5ea6\u548c\u9884\u540e\u7684\u5de8\u5927\u53d8\u5f02\u6027\u4f7f\u5f97\u96be\u4ee5\u5c06\u7ed3\u6784\u6027\u635f\u4f24\u4e0e\u529f\u80fd\u6027\u7f3a\u9677\u8054\u7cfb\u8d77\u6765\u3002", "method": "\u4f7f\u7528FITBIR\u6570\u636e\u5e93\u4e2d\u7684T1\u52a0\u6743\u56fe\u50cf\uff0c\u9996\u5148\u8fdb\u884c\u6570\u636e\u534f\u8c03\u548c132\u4e2a\u611f\u5174\u8da3\u533a\u57df\u7684\u5206\u5272\u3002\u7ecf\u8fc7\u8d28\u91cf\u4fdd\u8bc1\u3001\u4f53\u79ef\u8ba1\u7b97\u548c\u5f02\u5e38\u503c\u53bb\u9664\u540e\uff0c\u8ba1\u7b97\u76f8\u5bf9\u4e8e\u5bf9\u7167\u7ec4\u7684z\u5206\u6570\uff0c\u5e76\u901a\u8fc7\u591a\u5143\u7ebf\u6027\u56de\u5f52\u6d88\u9664\u6027\u522b\u3001\u5e74\u9f84\u548c\u603b\u8111\u5bb9\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0TBI\u60a3\u8005\u4e0e\u5bf9\u7167\u7ec4\u572837\u4e2aROI\u4e2d\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08p < 0.05\uff0c\u7ecfFDR\u6821\u6b63\uff09\u3002\u901a\u8fc7\u72ec\u7acb\u6210\u5206\u5206\u6790\u548c\u805a\u7c7b\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u5dee\u5f02\u4e3b\u8981\u6765\u6e90\u4e8e\u4e09\u4e2a\u533a\u57df\u7fa4\uff1a\u8111\u5e72\u3001\u6795\u6781\u53ca\u7736\u540e\u7ed3\u6784\uff1b\u76ae\u5c42\u4e0b\u7070\u8d28\u548c\u5c9b\u53f6\u76ae\u5c42\uff1b\u5927\u8111\u548c\u5c0f\u8111\u767d\u8d28\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u63ed\u793a\u4e86\u521b\u4f24\u6027\u8111\u635f\u4f24\u7684\u5171\u4eab\u635f\u4f24\u8def\u5f84\uff0c\u4e3a\u7406\u89e3TBI\u7684\u795e\u7ecf\u89e3\u5256\u5b66\u57fa\u7840\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u8bca\u65ad\u548c\u9884\u540e\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2511.03771", "pdf": "https://arxiv.org/pdf/2511.03771", "abs": "https://arxiv.org/abs/2511.03771", "authors": ["Alif Elham Khan"], "title": "Climbing the label tree: Hierarchy-preserving contrastive learning for medical imaging", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": null, "summary": "Medical image labels are often organized by taxonomies (e.g., organ - tissue\n- subtype), yet standard self-supervised learning (SSL) ignores this structure.\nWe present a hierarchy-preserving contrastive framework that makes the label\ntree a first-class training signal and an evaluation target. Our approach\nintroduces two plug-in objectives: Hierarchy-Weighted Contrastive (HWC), which\nscales positive/negative pair strengths by shared ancestors to promote\nwithin-parent coherence, and Level-Aware Margin (LAM), a prototype margin that\nseparates ancestor groups across levels. The formulation is geometry-agnostic\nand applies to Euclidean and hyperbolic embeddings without architectural\nchanges. Across several benchmarks, including breast histopathology, the\nproposed objectives consistently improve representation quality over strong SSL\nbaselines while better respecting the taxonomy. We evaluate with metrics\ntailored to hierarchy faithfulness: HF1 (hierarchical F1), H-Acc\n(tree-distance-weighted accuracy), and parent-distance violation rate. We also\nreport top-1 accuracy for completeness. Ablations show that HWC and LAM are\neffective even without curvature, and combining them yields the most\ntaxonomy-aligned representations. Taken together, these results provide a\nsimple, general recipe for learning medical image representations that respect\nthe label tree and advance both performance and interpretability in\nhierarchy-rich domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fdd\u6301\u5c42\u6b21\u7ed3\u6784\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u533b\u5b66\u56fe\u50cf\u6807\u7b7e\u7684\u6811\u72b6\u7ed3\u6784\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\u548c\u8bc4\u4f30\u76ee\u6807\uff0c\u901a\u8fc7\u5c42\u6b21\u52a0\u6743\u5bf9\u6bd4\u548c\u5c42\u7ea7\u611f\u77e5\u8fb9\u754c\u4e24\u4e2a\u76ee\u6807\u51fd\u6570\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u8868\u793a\u8d28\u91cf\u5e76\u66f4\u597d\u5730\u4fdd\u6301\u5206\u7c7b\u5c42\u6b21\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u6807\u7b7e\u901a\u5e38\u6309\u5c42\u6b21\u7ed3\u6784\u7ec4\u7ec7\uff08\u5982\u5668\u5b98-\u7ec4\u7ec7-\u4e9a\u578b\uff09\uff0c\u4f46\u6807\u51c6\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5ffd\u7565\u4e86\u8fd9\u79cd\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u5f15\u5165\u4e24\u4e2a\u63d2\u4ef6\u76ee\u6807\uff1a\u5c42\u6b21\u52a0\u6743\u5bf9\u6bd4\uff08HWC\uff09\u6839\u636e\u5171\u4eab\u7956\u5148\u7f29\u653e\u6b63\u8d1f\u5bf9\u5f3a\u5ea6\uff0c\u5c42\u7ea7\u611f\u77e5\u8fb9\u754c\uff08LAM\uff09\u901a\u8fc7\u539f\u578b\u8fb9\u754c\u5728\u4e0d\u540c\u5c42\u7ea7\u5206\u79bb\u7956\u5148\u7ec4\u3002\u8be5\u6846\u67b6\u662f\u51e0\u4f55\u65e0\u5173\u7684\uff0c\u9002\u7528\u4e8e\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u5d4c\u5165\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u76ee\u6807\u6301\u7eed\u4f18\u4e8e\u5f3a\u81ea\u76d1\u7763\u5b66\u4e60\u57fa\u7ebf\uff0c\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u6301\u4e86\u5206\u7c7b\u5c42\u6b21\u3002\u4f7f\u7528\u5c42\u6b21\u5fe0\u5b9e\u5ea6\u6307\u6807\uff08HF1\u3001H-Acc\u3001\u7236\u8ddd\u79bb\u8fdd\u53cd\u7387\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u62a5\u544atop-1\u51c6\u786e\u7387\u3002", "conclusion": "HWC\u548cLAM\u5373\u4f7f\u5728\u6ca1\u6709\u66f2\u7387\u7684\u60c5\u51b5\u4e0b\u4e5f\u6709\u6548\uff0c\u7ec4\u5408\u4f7f\u7528\u53ef\u83b7\u5f97\u6700\u7b26\u5408\u5206\u7c7b\u5c42\u6b21\u7684\u8868\u793a\u3002\u8fd9\u4e3a\u5b66\u4e60\u5c0a\u91cd\u6807\u7b7e\u6811\u7684\u533b\u5b66\u56fe\u50cf\u8868\u793a\u63d0\u4f9b\u4e86\u7b80\u5355\u901a\u7528\u7684\u65b9\u6cd5\uff0c\u5728\u5c42\u6b21\u4e30\u5bcc\u7684\u9886\u57df\u63d0\u5347\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.03826", "pdf": "https://arxiv.org/pdf/2511.03826", "abs": "https://arxiv.org/abs/2511.03826", "authors": ["Esha Sadia Nasir", "Behnaz Elhaminia", "Mark Eastwood", "Catherine King", "Owen Cain", "Lorraine Harper", "Paul Moss", "Dimitrios Chanouzas", "David Snead", "Nasir Rajpoot", "Adam Shephard", "Shan E Ahmed Raza"], "title": "CORE - A Cell-Level Coarse-to-Fine Image Registration Engine for Multi-stain Image Alignment", "categories": ["q-bio.QM", "cs.AI"], "comment": null, "summary": "Accurate and efficient registration of whole slide images (WSIs) is essential\nfor high-resolution, nuclei-level analysis in multi-stained tissue slides. We\npropose a novel coarse-to-fine framework CORE for accurate nuclei-level\nregistration across diverse multimodal whole-slide image (WSI) datasets. The\ncoarse registration stage leverages prompt-based tissue mask extraction to\neffectively filter out artefacts and non-tissue regions, followed by global\nalignment using tissue morphology and ac- celerated dense feature matching with\na pre-trained feature extractor. From the coarsely aligned slides, nuclei\ncentroids are detected and subjected to fine-grained rigid registration using a\ncustom, shape-aware point-set registration model. Finally, non-rigid alignment\nat the cellular level is achieved by estimating a non-linear dis- placement\nfield using Coherent Point Drift (CPD). Our approach benefits from\nautomatically generated nuclei that enhance the accuracy of deformable\nregistra- tion and ensure precise nuclei-level correspondence across\nmodalities. The pro- posed model is evaluated on three publicly available WSI\nregistration datasets, and two private datasets. We show that CORE outperforms\ncurrent state-of-the-art methods in terms of generalisability, precision, and\nrobustness in bright-field and immunofluorescence microscopy WSIs", "AI": {"tldr": "\u63d0\u51faCORE\u6846\u67b6\uff0c\u4e00\u79cd\u4ece\u7c97\u5230\u7ec6\u7684\u591a\u6a21\u6001\u5168\u5207\u7247\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec4\u7ec7\u63a9\u6a21\u63d0\u53d6\u3001\u5168\u5c40\u5bf9\u9f50\u3001\u7ec6\u7c92\u5ea6\u521a\u6027\u914d\u51c6\u548c\u7ec6\u80de\u7ea7\u975e\u521a\u6027\u5bf9\u9f50\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u7ec6\u80de\u6838\u7ea7\u914d\u51c6\u3002", "motivation": "\u591a\u67d3\u8272\u7ec4\u7ec7\u5207\u7247\u4e2d\u5168\u5207\u7247\u56fe\u50cf\u7684\u51c6\u786e\u9ad8\u6548\u914d\u51c6\u5bf9\u4e8e\u9ad8\u5206\u8fa8\u7387\u7ec6\u80de\u6838\u7ea7\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u89e3\u51b3\u591a\u6a21\u6001\u56fe\u50cf\u914d\u51c6\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4ece\u7c97\u5230\u7ec6\u7684\u6846\u67b6\uff1a1)\u57fa\u4e8e\u63d0\u793a\u7684\u7ec4\u7ec7\u63a9\u6a21\u63d0\u53d6\u8fc7\u6ee4\u4f2a\u5f71\uff1b2)\u5229\u7528\u7ec4\u7ec7\u5f62\u6001\u5b66\u548c\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u5668\u8fdb\u884c\u5168\u5c40\u5bf9\u9f50\uff1b3)\u68c0\u6d4b\u7ec6\u80de\u6838\u8d28\u5fc3\u5e76\u4f7f\u7528\u5f62\u72b6\u611f\u77e5\u70b9\u96c6\u914d\u51c6\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u521a\u6027\u914d\u51c6\uff1b4)\u4f7f\u7528\u76f8\u5e72\u70b9\u6f02\u79fb\u4f30\u8ba1\u975e\u7ebf\u6027\u4f4d\u79fb\u573a\u5b9e\u73b0\u7ec6\u80de\u7ea7\u975e\u521a\u6027\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00WSI\u914d\u51c6\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cCORE\u5728\u660e\u573a\u548c\u514d\u75ab\u8367\u5149\u663e\u5fae\u955cWSI\u4e2d\uff0c\u5728\u6cdb\u5316\u6027\u3001\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "CORE\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7684\u7ec6\u80de\u6838\u589e\u5f3a\u4e86\u53ef\u53d8\u5f62\u914d\u51c6\u7684\u51c6\u786e\u6027\uff0c\u786e\u4fdd\u8de8\u6a21\u6001\u7684\u7cbe\u786e\u7ec6\u80de\u6838\u7ea7\u5bf9\u5e94\uff0c\u4e3a\u591a\u6a21\u6001\u7ec4\u7ec7\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.04143", "pdf": "https://arxiv.org/pdf/2511.04143", "abs": "https://arxiv.org/abs/2511.04143", "authors": ["Luca Quaroni"], "title": "Infrared Microscopy of Biochemistry and Metabolism in Single Living Eukaryotic Cells", "categories": ["q-bio.QM"], "comment": null, "summary": "The turn of the millennium has seen a growing interest in the study of live\ncells by infrared (IR) spectroscopy, driven by the versatility, wealth of\nmolecular information, and potential for high-throughput screening of the\ntechnique. Measurements on individual cells, either isolated or within a\nmulti-cellular structure, provide information that is not available from\nensemble samples. The present review discusses the use of infrared (IR)\nmicroscopy to analyse live single cells from a biochemical perspective, seeking\ninformation on real-time processes. The emphasis is on the use of the technique\nto quantify metabolic turnover, with the aim of providing a complementary\nmethod for metabolomics, and for toxicological and pharmacological studies. The\npresent work highlights the methodological advances and proof-of-concept\nexperiments that took place over the past few years in this direction. It\ndiscusses current advantages and limitations of the technique, including the\npossibility of detecting specific biomolecules and their reactivity, and it\nconcludes with a brief outline of future perspectives.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u7ea2\u5916\u663e\u5fae\u955c\u5728\u6d3b\u7ec6\u80de\u5206\u6790\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u4ee3\u8c22\u5468\u8f6c\u7684\u5b9a\u91cf\u7814\u7a76\uff0c\u4e3a\u4ee3\u8c22\u7ec4\u5b66\u3001\u6bd2\u7406\u5b66\u548c\u836f\u7406\u5b66\u7814\u7a76\u63d0\u4f9b\u8865\u5145\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5bf9\u6d3b\u7ec6\u80de\u7814\u7a76\u7684\u5174\u8da3\u589e\u957f\uff0c\u7ea2\u5916\u5149\u8c31\u6280\u672f\u56e0\u5176\u591a\u529f\u80fd\u6027\u3001\u4e30\u5bcc\u7684\u5206\u5b50\u4fe1\u606f\u548c\u9ad8\u901a\u91cf\u7b5b\u9009\u6f5c\u529b\u800c\u53d7\u5230\u5173\u6ce8\uff0c\u80fd\u591f\u63d0\u4f9b\u4f20\u7edf\u7fa4\u4f53\u6837\u672c\u65e0\u6cd5\u83b7\u5f97\u7684\u5355\u4e2a\u7ec6\u80de\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u7ea2\u5916\u663e\u5fae\u955c\u5206\u6790\u6d3b\u4f53\u5355\u7ec6\u80de\uff0c\u4ece\u751f\u5316\u89d2\u5ea6\u7814\u7a76\u5b9e\u65f6\u8fc7\u7a0b\uff0c\u91cd\u70b9\u5173\u6ce8\u4ee3\u8c22\u5468\u8f6c\u7684\u5b9a\u91cf\u6d4b\u91cf\u3002", "result": "\u7efc\u8ff0\u4e86\u8fc7\u53bb\u51e0\u5e74\u5728\u8be5\u65b9\u5411\u4e0a\u7684\u65b9\u6cd5\u5b66\u8fdb\u5c55\u548c\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\uff0c\u8ba8\u8bba\u4e86\u68c0\u6d4b\u7279\u5b9a\u751f\u7269\u5206\u5b50\u53ca\u5176\u53cd\u5e94\u6027\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u603b\u7ed3\u4e86\u8be5\u6280\u672f\u7684\u5f53\u524d\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u7b80\u8981\u6982\u8ff0\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2511.03819", "pdf": "https://arxiv.org/pdf/2511.03819", "abs": "https://arxiv.org/abs/2511.03819", "authors": ["Ozan Kanbertay", "Richard Vogg", "Elif Karakoc", "Peter M. Kappeler", "Claudia Fichtel", "Alexander S. Ecker"], "title": "SILVI: Simple Interface for Labeling Video Interactions", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Computer vision methods are increasingly used for the automated analysis of\nlarge volumes of video data collected through camera traps, drones, or direct\nobservations of animals in the wild. While recent advances have focused\nprimarily on detecting individual actions, much less work has addressed the\ndetection and annotation of interactions -- a crucial aspect for understanding\nsocial and individualized animal behavior. Existing open-source annotation\ntools support either behavioral labeling without localization of individuals,\nor localization without the capacity to capture interactions. To bridge this\ngap, we present SILVI, an open-source labeling software that integrates both\nfunctionalities. SILVI enables researchers to annotate behaviors and\ninteractions directly within video data, generating structured outputs suitable\nfor training and validating computer vision models. By linking behavioral\necology with computer vision, SILVI facilitates the development of automated\napproaches for fine-grained behavioral analyses. Although developed primarily\nin the context of animal behavior, SILVI could be useful more broadly to\nannotate human interactions in other videos that require extracting dynamic\nscene graphs. The software, along with documentation and download instructions,\nis available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.", "AI": {"tldr": "SILVI\u662f\u4e00\u4e2a\u5f00\u6e90\u6807\u6ce8\u8f6f\u4ef6\uff0c\u7528\u4e8e\u5728\u89c6\u9891\u6570\u636e\u4e2d\u6807\u6ce8\u884c\u4e3a\u548c\u4ea4\u4e92\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u5728\u4ea4\u4e92\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u9762\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6807\u6ce8\u5de5\u5177\u8981\u4e48\u652f\u6301\u884c\u4e3a\u6807\u6ce8\u4f46\u4e0d\u5b9a\u4f4d\u4e2a\u4f53\uff0c\u8981\u4e48\u652f\u6301\u5b9a\u4f4d\u4f46\u4e0d\u6355\u6349\u4ea4\u4e92\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7406\u89e3\u793e\u4ea4\u548c\u4e2a\u4f53\u5316\u52a8\u7269\u884c\u4e3a\u7684\u9700\u6c42\u3002", "method": "\u5f00\u53d1SILVI\u8f6f\u4ef6\uff0c\u96c6\u6210\u884c\u4e3a\u548c\u4ea4\u4e92\u6807\u6ce8\u529f\u80fd\uff0c\u76f4\u63a5\u5728\u89c6\u9891\u6570\u636e\u4e2d\u6807\u6ce8\uff0c\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u3002", "result": "SILVI\u80fd\u591f\u6807\u6ce8\u884c\u4e3a\u548c\u4ea4\u4e92\uff0c\u94fe\u63a5\u884c\u4e3a\u751f\u6001\u5b66\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\uff0c\u4fc3\u8fdb\u7ec6\u7c92\u5ea6\u884c\u4e3a\u5206\u6790\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\u5f00\u53d1\u3002", "conclusion": "SILVI\u586b\u8865\u4e86\u4ea4\u4e92\u6807\u6ce8\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u867d\u7136\u4e3b\u8981\u9488\u5bf9\u52a8\u7269\u884c\u4e3a\u5f00\u53d1\uff0c\u4f46\u4e5f\u53ef\u7528\u4e8e\u6807\u6ce8\u5176\u4ed6\u89c6\u9891\u4e2d\u7684\u4eba\u7c7b\u4ea4\u4e92\u548c\u52a8\u6001\u573a\u666f\u56fe\u63d0\u53d6\u3002"}}
{"id": "2511.03986", "pdf": "https://arxiv.org/pdf/2511.03986", "abs": "https://arxiv.org/abs/2511.03986", "authors": ["Ahmed A. Metwally", "Heyjun Park", "Yue Wu", "Tracey McLaughlin", "Michael P. Snyder"], "title": "Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes", "categories": ["cs.LG", "q-bio.QM"], "comment": "18 pages, 8 figures", "summary": "The classification of diabetes and prediabetes by static glucose thresholds\nobscures the pathophysiological dysglycemia heterogeneity, primarily driven by\ninsulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This\nreview demonstrates that continuous glucose monitoring and wearable\ntechnologies enable a paradigm shift towards non-invasive, dynamic metabolic\nphenotyping. We show evidence that machine learning models can leverage\nhigh-resolution glucose data from at-home, CGM-enabled oral glucose tolerance\ntests to accurately predict gold-standard measures of muscle IR and beta-cell\nfunction. This personalized characterization extends to real-world nutrition,\nwhere an individual's unique postprandial glycemic response (PPGR) to\nstandardized meals, such as the relative glucose spike to potatoes versus\ngrapes, could serve as a biomarker for their metabolic subtype. Moreover,\nintegrating wearable data reveals that habitual diet, sleep, and physical\nactivity patterns, particularly their timing, are uniquely associated with\nspecific metabolic dysfunctions, informing precision lifestyle interventions.\nThe efficacy of dietary mitigators in attenuating PPGR is also shown to be\nphenotype-dependent. Collectively, this evidence demonstrates that CGM can\ndeconstruct the complexity of early dysglycemia into distinct, actionable\nsubphenotypes. This approach moves beyond simple glycemic control, paving the\nway for targeted nutritional, behavioral, and pharmacological strategies\ntailored to an individual's core metabolic defects, thereby paving the way for\na new era of precision diabetes prevention.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5982\u4f55\u5229\u7528\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u548c\u53ef\u7a7f\u6234\u6280\u672f\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ece\u9759\u6001\u8840\u7cd6\u9608\u503c\u8f6c\u5411\u52a8\u6001\u4ee3\u8c22\u8868\u578b\u5206\u6790\uff0c\u5b9e\u73b0\u7cd6\u5c3f\u75c5\u548c\u524d\u9a71\u7cd6\u5c3f\u75c5\u7684\u7cbe\u51c6\u5206\u578b\u548c\u4e2a\u6027\u5316\u5e72\u9884\u3002", "motivation": "\u4f20\u7edf\u7684\u7cd6\u5c3f\u75c5\u548c\u524d\u9a71\u7cd6\u5c3f\u75c5\u5206\u7c7b\u57fa\u4e8e\u9759\u6001\u8840\u7cd6\u9608\u503c\uff0c\u63a9\u76d6\u4e86\u7531\u80f0\u5c9b\u7d20\u62b5\u6297\u3001\u03b2\u7ec6\u80de\u529f\u80fd\u969c\u788d\u548c\u80a0\u4fc3\u80f0\u5c9b\u7d20\u7f3a\u4e4f\u5f15\u8d77\u7684\u75c5\u7406\u751f\u7406\u5f02\u8d28\u6027\u3002\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u4e0d\u540c\u7684\u4ee3\u8c22\u4e9a\u578b\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u548c\u53ef\u7a7f\u6234\u6280\u672f\u6536\u96c6\u9ad8\u5206\u8fa8\u7387\u8840\u7cd6\u6570\u636e\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5206\u6790\u5bb6\u5ead\u53e3\u670d\u8461\u8404\u7cd6\u8010\u91cf\u6d4b\u8bd5\u6570\u636e\uff0c\u9884\u6d4b\u808c\u8089\u80f0\u5c9b\u7d20\u62b5\u6297\u548c\u03b2\u7ec6\u80de\u529f\u80fd\u7684\u91d1\u6807\u51c6\u6307\u6807\u3002\u6574\u5408\u996e\u98df\u3001\u7761\u7720\u548c\u8eab\u4f53\u6d3b\u52a8\u6a21\u5f0f\u6570\u636e\u3002", "result": "\u7814\u7a76\u8868\u660e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u51c6\u786e\u9884\u6d4b\u80f0\u5c9b\u7d20\u62b5\u6297\u548c\u03b2\u7ec6\u80de\u529f\u80fd\uff1b\u4e2a\u4f53\u5bf9\u6807\u51c6\u5316\u9910\u98df\u7684\u9910\u540e\u8840\u7cd6\u53cd\u5e94\u53ef\u4f5c\u4e3a\u4ee3\u8c22\u4e9a\u578b\u7684\u751f\u7269\u6807\u5fd7\u7269\uff1b\u751f\u6d3b\u4e60\u60ef\u6a21\u5f0f\u4e0e\u7279\u5b9a\u4ee3\u8c22\u529f\u80fd\u969c\u788d\u76f8\u5173\uff1b\u996e\u98df\u7f13\u89e3\u5242\u7684\u6548\u679c\u5177\u6709\u8868\u578b\u4f9d\u8d56\u6027\u3002", "conclusion": "\u8fde\u7eed\u8840\u7cd6\u76d1\u6d4b\u80fd\u591f\u5c06\u65e9\u671f\u8840\u7cd6\u5f02\u5e38\u7684\u590d\u6742\u6027\u5206\u89e3\u4e3a\u4e0d\u540c\u7684\u3001\u53ef\u64cd\u4f5c\u7684\u4e9a\u8868\u578b\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u8840\u7cd6\u63a7\u5236\uff0c\u4e3a\u9488\u5bf9\u4e2a\u4f53\u6838\u5fc3\u4ee3\u8c22\u7f3a\u9677\u7684\u7cbe\u51c6\u8425\u517b\u3001\u884c\u4e3a\u548c\u836f\u7269\u7b56\u7565\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
