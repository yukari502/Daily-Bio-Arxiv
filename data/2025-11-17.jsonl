{"id": "2511.10708", "pdf": "https://arxiv.org/pdf/2511.10708", "abs": "https://arxiv.org/abs/2511.10708", "authors": ["Yoonho Jeong", "Chengcheng Yang", "Jihoo Kim", "Eok Kyun Lee", "Younghoon Lee", "Won June Kim", "Seung Seo Lee", "Insung S. Choi"], "title": "MOSAIC: Codon Harmonization of Monte Carlo-Based Simulated Annealing for Linked Codons in Heterologous Protein Expression", "categories": ["q-bio.QM"], "comment": "31 pages, 3 figures. Submitted to ACS Synthetic Biology", "summary": "Codon usage bias has a crucial impact on the translation efficiency and co-translational folding of proteins, necessitating the algorithmic development of codon optimization/harmonization methods, particularly for heterologous recombinant protein expression. Codon harmonization is especially valuable for proteins sensitive to translation rates, because it can potentially replicate native translation speeds, preserving proper folding and maintaining protein activity. This work proposes a Monte Carlo-based codon harmonization algorithm, MOSAIC (Monte Carlo-based Simulated Annealing for Linked Codons), for the harmonization of a set of linked codons, which differs from conventional codon harmonization, by focusing on the codon sets rather than individual ones. Our MOSAIC demonstrates state-of-the-art performance on ribosomal proteins (S18, S15, S10, and L11) as model systems. Among them, the harmonized gene of RP S18 was expressed and compared with the expression of the wild-type gene. The harmonized gene clearly yielded a larger quantity of the protein, from which the amount of the soluble protein was also significant. These results underscore the potential of the linked codon harmonization approach to enhance the expression and functionality of sensitive proteins, setting the stage for more efficient production of recombinant proteins in various biotechnological and pharmaceutical applications."}
{"id": "2511.11452", "pdf": "https://arxiv.org/pdf/2511.11452", "abs": "https://arxiv.org/abs/2511.11452", "authors": ["Seth Alain Chang", "Muhammad Mueez Amjad", "Noorul Wahab", "Ethar Alzaid", "Nasir Rajpoot", "Adam Shephard"], "title": "Synergy vs. Noise: Performance-Guided Multimodal Fusion For Biochemical Recurrence-Free Survival in Prostate Cancer", "categories": ["q-bio.QM", "cs.CV", "cs.LG", "eess.IV"], "comment": "5 pages, 1 figure, 4 tables", "summary": "Multimodal deep learning (MDL) has emerged as a transformative approach in computational pathology. By integrating complementary information from multiple data sources, MDL models have demonstrated superior predictive performance across diverse clinical tasks compared to unimodal models. However, the assumption that combining modalities inherently improves performance remains largely unexamined. We hypothesise that multimodal gains depend critically on the predictive quality of individual modalities, and that integrating weak modalities may introduce noise rather than complementary information. We test this hypothesis on a prostate cancer dataset with histopathology, radiology, and clinical data to predict time-to-biochemical recurrence. Our results confirm that combining high-performing modalities yield superior performance compared to unimodal approaches. However, integrating a poor-performing modality with other higher-performing modalities degrades predictive accuracy. These findings demonstrate that multimodal benefit requires selective, performance-guided integration rather than indiscriminate modality combination, with implications for MDL design across computational pathology and medical imaging."}
{"id": "2511.11515", "pdf": "https://arxiv.org/pdf/2511.11515", "abs": "https://arxiv.org/abs/2511.11515", "authors": ["Paul Jonas Jost", "Frank T Bergmann", "Daniel Weindl", "Jan Hasenauer"], "title": "PEtab-GUI: A graphical user interface to create, edit and inspect PEtab parameter estimation problems", "categories": ["q-bio.QM"], "comment": null, "summary": "Motivation: Parameter estimation is a cornerstone of data-driven modeling in systems biology. Yet, constructing such problems in a reproducible and accessible manner remains challenging. The PEtab format has established itself as a powerful community standard to encode parameter estimation problems, promoting interoperability and reusability. However, its reliance on multiple interlinked files - often edited manually - can introduce inconsistencies, and new users often struggle to navigate them. Here, we present PEtab-GUI, an open-source Python application designed to streamline the creation, editing, and validation of PEtab problems through an intuitive graphical user interface. PEtab-GUI integrates all PEtab components, including SBML models and tabular files, into a single environment with live error-checking and customizable defaults. Interactive visualization and simulation capabilities enable users to inspect the relationship between the model and the data. PEtab-GUI lowers the barrier to entry for specifying standardized parameter estimation problems, making dynamic modeling more accessible, especially in educational and interdisciplinary settings.\n  Availability and Implementation: PEtab-GUI is implemented in Python, open-source under a 3-Clause BSD license. The code, designed to be modular and extensible, is hosted on https://github.com/PEtab-dev/PEtab-GUI and can be installed from PyPI.\n  Key words: Parameter Estimation, Python, Graphical User Interface, Systems Biology"}
{"id": "2511.11080", "pdf": "https://arxiv.org/pdf/2511.11080", "abs": "https://arxiv.org/abs/2511.11080", "authors": ["M. Benfatto", "L. De Paolis", "L. Tonello", "P. Grigolini"], "title": "Advanced Data Analysis of Spontaneous Biophoton Emission: A Multi-Method Approach", "categories": ["physics.bio-ph", "nlin.AO", "physics.data-an", "q-bio.QM"], "comment": "35 pages, 8 figures, methodological work on possible methods of analysis of experimental data from biophoton experiments", "summary": "Ultra-weak photon emission (UPE) from living systems is widely hypothesized to reflect un-derlying self-organization and long-range coordination in biological dynamics. However, distin-guishing biologically driven correlations from trivial stochastic or instrumental effects requires a robust, multi-method framework. In this work, we establish and benchmark a comprehensive anal-ysis pipeline for photon-count time series, combining Distribution Entropy Analysis, Rényi entro-py, Detrended Fluctuation Analysis, its generalization Multifractal Detrended Fluctuation Analysis, and tail-statistics characterization. Surrogate signals constructed from Poisson processes, Fractional Gaussian Noise, and Renewal Processes with power-law waiting times are used to validate sensitivity to memory, intermittency, and multifractality. Across all methods, a coherent hierarchy of dynamical regimes is recovered, demonstrating internal methodological consistency. Application to experimental dark-count data and attenuated coherent-laser emission confirm Poisson-like behavior, establishing an essential statistical baseline for UPE studies. The combined results show that this multi-resolution approach reliably separates trivial photon-counting statistics from struc-tured long-range organization, providing a validated methodological foundation for future biological UPE measurements and their interpretation in the context of non-equilibrium statistical physics, information dynamics, and prospective markers of biological coherence."}
{"id": "2511.11293", "pdf": "https://arxiv.org/pdf/2511.11293", "abs": "https://arxiv.org/abs/2511.11293", "authors": ["Jiheum Park", "Chao Pang", "Tristan Y. Lee", "Jeong Yun Yang", "Jacob Berkowitz", "Alexander Z. Wei", "Nicholas Tatonetti"], "title": "Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies."}
{"id": "2511.11486", "pdf": "https://arxiv.org/pdf/2511.11486", "abs": "https://arxiv.org/abs/2511.11486", "authors": ["Roman Kinakh", "Gonzalo R. Ríos-Muñoz", "Arrate Muñoz-Barrutia"], "title": "Multimodal Posterior Sampling-based Uncertainty in PD-L1 Segmentation from H&E Images", "categories": ["cs.CV", "q-bio.QM"], "comment": "Preprint (pre-review). Accepted for publication in Lecture Notes in Bioinformatics (Springer, 2025). The final authenticated version will be available on SpringerLink once published", "summary": "Accurate assessment of PD-L1 expression is critical for guiding immunotherapy, yet current immunohistochemistry (IHC) based methods are resource-intensive. We present nnUNet-B: a Bayesian segmentation framework that infers PD-L1 expression directly from H&E-stained histology images using Multimodal Posterior Sampling (MPS). Built upon nnUNet-v2, our method samples diverse model checkpoints during cyclic training to approximate the posterior, enabling both accurate segmentation and epistemic uncertainty estimation via entropy and standard deviation. Evaluated on a dataset of lung squamous cell carcinoma, our approach achieves competitive performance against established baselines with mean Dice Score and mean IoU of 0.805 and 0.709, respectively, while providing pixel-wise uncertainty maps. Uncertainty estimates show strong correlation with segmentation error, though calibration remains imperfect. These results suggest that uncertainty-aware H&E-based PD-L1 prediction is a promising step toward scalable, interpretable biomarker assessment in clinical workflows."}
