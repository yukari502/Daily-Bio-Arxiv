{"id": "2511.12205", "pdf": "https://arxiv.org/pdf/2511.12205", "abs": "https://arxiv.org/abs/2511.12205", "authors": ["Akmuhammet Ashyralyyev", "Zülal Bingöl", "Begüm Filiz Öz", "Salem Malikic", "Uzi Vishkin", "S. Cenk Sahinalp", "Can Alkan"], "title": "LCPan: efficient variation graph construction using Locally Consistent Parsing", "categories": ["q-bio.GN"], "comment": null, "summary": "Efficient and consistent string processing is critical in the exponentially growing genomic data era. Locally Consistent Parsing (LCP) addresses this need by partitioning an input genome string into short, exactly matching substrings (e.g., \"cores\"), ensuring consistency across partitions. Labeling the cores of an input string consistently not only provides a compact representation of the input but also enables the reapplication of LCP to refine the cores over multiple iterations, providing a progressively longer and more informative set of substrings for downstream analyses.\n  We present the first iterative implementation of LCP with Lcptools and demonstrate its effectiveness in identifying cores with minimal collisions. Experimental results show that the number of cores at the i^th iteration is O(n/c^i) for c ~ 2.34, while the average length and the average distance between consecutive cores are O(c^i). Compared to the popular sketching techniques, LCP produces significantly fewer cores, enabling a more compact representation and faster analyses. To demonstrate the advantages of LCP in genomic string processing in terms of computation and memory efficiency, we also introduce LCPan, an efficient variation graph constructor. We show that LCPan generates variation graphs >10x faster than vg, while using >13x less memory."}
{"id": "2511.11717", "pdf": "https://arxiv.org/pdf/2511.11717", "abs": "https://arxiv.org/abs/2511.11717", "authors": ["Xiang Xiang Wang", "Sean Cottrell", "Guo-Wei Wei"], "title": "Multiscale Grassmann Manifolds for Single-Cell Data Analysis", "categories": ["cs.LG", "q-bio.GN"], "comment": null, "summary": "Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data."}
{"id": "2511.12797", "pdf": "https://arxiv.org/pdf/2511.12797", "abs": "https://arxiv.org/abs/2511.12797", "authors": ["Nathan Breslow", "Aayush Mishra", "Mahler Revsine", "Michael C. Schatz", "Anqi Liu", "Daniel Khashabi"], "title": "Genomic Next-Token Predictors are In-Context Learners", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "comment": null, "summary": "In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?\n  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning."}
{"id": "2511.13705", "pdf": "https://arxiv.org/pdf/2511.13705", "abs": "https://arxiv.org/abs/2511.13705", "authors": ["Alaa Mezghiche"], "title": "Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering", "categories": ["cs.LG", "q-bio.GN"], "comment": "16 pages", "summary": "Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI \"Gene Expression Cancer RNA-Seq\" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype."}
{"id": "2511.12805", "pdf": "https://arxiv.org/pdf/2511.12805", "abs": "https://arxiv.org/abs/2511.12805", "authors": ["Noriaki Sato", "Marco Scutari", "Shuichi Kawano", "Rui Yamaguchi", "Seiya Imoto"], "title": "Practical Causal Evaluation Metrics for Biological Networks", "categories": ["q-bio.MN", "cs.LG"], "comment": "15 pages, 1 figure", "summary": "Estimating causal networks from biological data is a critical step in systems biology. When evaluating the inferred network, assessing the networks based on their intervention effects is particularly important for downstream probabilistic reasoning and the identification of potential drug targets. In the context of gene regulatory network inference, biological databases are often used as reference sources. These databases typically describe relationships in a qualitative rather than quantitative manner. However, few evaluation metrics have been developed that take this qualitative nature into account. To address this, we developed a metric, the sign-augmented Structural Intervention Distance (sSID), and a weighted sSID that incorporates the net effects of the intervention. Through simulations and analyses of real transcriptomic datasets, we found that our proposed metrics could identify a different algorithm as optimal compared to conventional metrics, and the network selected by sSID had a superior performance in the classification task of clinical covariates using transcriptomic data. This suggests that sSID can distinguish networks that are structurally correct but functionally incorrect, highlighting its potential as a more biologically meaningful and practical evaluation metric."}
{"id": "2511.11758", "pdf": "https://arxiv.org/pdf/2511.11758", "abs": "https://arxiv.org/abs/2511.11758", "authors": ["Michael Sun", "Weize Yuan", "Gang Liu", "Wojciech Matusik", "Marinka Zitnik"], "title": "Protein Structure Tokenization via Geometric Byte Pair Encoding", "categories": ["q-bio.QM", "cs.AI"], "comment": null, "summary": "Protein structure is central to biological function, and enabling multimodal protein models requires joint reasoning over sequence, structure, and function. A key barrier is the lack of principled protein structure tokenizers (PSTs): existing approaches fix token size or rely on continuous vector codebooks, limiting interpretability, multi-scale control, and transfer across architectures. We introduce GeoBPE, a geometry-grounded PST that transforms continuous, noisy, multi-scale backbone conformations into discrete ``sentences'' of geometry while enforcing global constraints. Analogous to byte-pair encoding, GeoBPE generates a hierarchical vocabulary of geometric primitives by iteratively (i) clustering Geo-Pair occurrences with k-medoids to yield a resolution-controllable vocabulary; (ii) quantizing each Geo-Pair to its closest medoid prototype; and (iii) reducing drift through differentiable inverse kinematics that optimizes boundary glue angles under an $\\mathrm{SE}(3)$ end-frame loss. GeoBPE offers compression ($>$10x reduction in bits-per-residue at similar distortion rate), data efficiency ($>$10x less training data), and generalization (maintains test/train distortion ratio of $1.0-1.1$). It is architecture-agnostic: (a) its hierarchical vocabulary provides a strong inductive bias for coarsening residue-level embeddings from large PLMs into motif- and protein-level representations, consistently outperforming leading PSTs across $12$ tasks and $24$ test splits; (b) paired with a transformer, GeoBPE supports unconditional backbone generation via language modeling; and (c) tokens align with CATH functional families and support expert-interpretable case studies, offering functional meaning absent in prior PSTs. Code is available at https://github.com/shiningsunnyday/PT-BPE/."}
{"id": "2511.11913", "pdf": "https://arxiv.org/pdf/2511.11913", "abs": "https://arxiv.org/abs/2511.11913", "authors": ["Saeed Mohammadzadeh", "Yao-Chang Tsan", "Aaron Renberg", "Hiba Kobeissi", "Adam Helms", "Emma Lejeune"], "title": "SarcGraph for High-Throughput Regional Analysis of Sarcomere Organization and Contractile Function in 2D Cardiac Muscle Bundles", "categories": ["q-bio.QM"], "comment": "10 pages, 1 figure", "summary": "Timelapse images of human induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) provide rich information on cell structure and contractile function. However, it is challenging to reproducibly generate tissue samples and conduct scalable experiments with these cells. The two-dimensional cardiac muscle bundle (2DMB) platform helps address these limitations by standardizing tissue geometry, resulting in physiologic, uniaxial contractions of discrete tissues on an elastomeric substrate with stiffness similar to the heart. 2DMBs are highly conducive to sarcomere imaging using fluorescent reporters, but, due to their larger and more physiologic sarcomere displacements and velocities, prior sarcomere-tracking pipelines have been unreliable. Here, we present adaptations to SarcGraph, an open-source Python package for sarcomere detection and tracking, that enable automated analysis of high-frame-rate 2DMB recordings. Key modifications to the pipeline include: 1) switching to a frame-by-frame sarcomere detection approach and automating tissue segmentation with spatial partitioning, 2) performing Gaussian Process Regression for signal denoising, and 3) incorporating an automatic contractile phase detection pipeline. These enhancements enable the extraction of structural organization and functional contractility metrics for both the whole 2DMB tissue and distinct tissue regions, both in a fully automated manner. We complement this software release with a dataset of 130 example movies of baseline and drug-treated samples disseminated through the Harvard Dataverse. By providing open-source tools and datasets, we aim to enable high-throughput analysis of engineered cardiac tissues and advance collective progress within the hiPSC-CM research community."}
{"id": "2511.12463", "pdf": "https://arxiv.org/pdf/2511.12463", "abs": "https://arxiv.org/abs/2511.12463", "authors": ["Adham M. Alkhadrawi", "Kyungsu Kim", "Arif M. Rahman"], "title": "Explainable deep learning framework for cancer therapeutic target prioritization leveraging PPI centrality and node embeddings", "categories": ["q-bio.QM"], "comment": null, "summary": "We developed an explainable deep learning framework integrating protein-protein interaction (PPI) network centrality metrics with node embeddings for cancer therapeutic target prioritization. A high-confidence PPI network was constructed from STRING database interactions, computing six centrality metrics: degree, strength, betweenness, closeness, eigenvector centrality, and clustering coefficient. Node2Vec embeddings captured latent network topology. Combined features trained XGBoost and neural network classifiers using DepMap CRISPR essentiality scores as ground truth. Model interpretability was assessed through GradientSHAP analysis quantifying feature contributions. We developed a novel blended scoring approach combining model probability predictions with SHAP attribution magnitudes for enhanced gene prioritization. Our framework achieved state-of-the-art performance with AUROC of 0.930 and AUPRC of 0.656 for identifying the top 10\\% most essential genes. GradientSHAP analysis revealed centrality measures contributed significantly to predictions, with degree centrality showing strongest correlation ($ρ$ = -0.357) with gene essentiality. The blended scoring approach created robust gene prioritization rankings, successfully identifying known essential genes including ribosomal proteins (RPS27A, RPS17, RPS6) and oncogenes (MYC). This study presents a human-based, combinatorial \\textit{in silico} framework successfully integrating network biology with explainable AI for therapeutic target discovery. The framework provides mechanistic transparency through feature attribution analysis while maintaining state-of-the-art predictive performance. Its reproducible design and reliance on human molecular datasets demonstrate a reduction-to-practice example of next-generation, animal-free modeling for cancer therapeutic target discovery and prioritization."}
{"id": "2511.12990", "pdf": "https://arxiv.org/pdf/2511.12990", "abs": "https://arxiv.org/abs/2511.12990", "authors": ["Sixtus Dakurah"], "title": "Brain Networks Flow-Topology via Variance Minimization in the Wasserstein Space", "categories": ["q-bio.QM"], "comment": null, "summary": "This work introduces a novel framework for testing topological variability in weighted networks by combining Hodge decomposition with Wasserstein variance minimization. Traditional approaches that analyze raw edge weights are susceptible to noise driven perturbations, limiting their ability to detect meaningful structural differences between network populations. Network signals are decomposed into various components using combinatorial Hodge theory, then topological disparity is quantified via the 2-Wasserstein distance between persistence diagrams. The test statistic measures variance reduction when comparing within group to between group dispersions in the Wasserstein space. Simulations demonstrate that the proposed method suppresses small random perturbations while maintaining sensitivity to genuine topological differences, particularly when applied to Hodge decomposed flows rather than raw edge weights. The framework is applied to functional brain networks from the Multimodal Treatment of ADHD dataset, comparing cannabis users and non-users"}
{"id": "2511.13141", "pdf": "https://arxiv.org/pdf/2511.13141", "abs": "https://arxiv.org/abs/2511.13141", "authors": ["Yangfan Liu", "Xiong Xiong", "Yong Liao", "Mingli Qin", "Zhen Huang", "Shilin Zhu", "Lilin Yin", "Yuhua Fu", "Haohao Zhang", "Jingya Xu", "Dong Yin", "Xin Huang", "Yuan Quan", "Xuan Li", "Tengfei Jiang", "Wanneng Yang", "Xiaohui Yuan", "Laurent Frantz", "Xinyun Li", "Xiaolei Liu", "Shuhong Zhao"], "title": "Bridging the genotype-phenotype gap with generative artificial intelligence", "categories": ["q-bio.QM"], "comment": null, "summary": "The genotype-phenotype gap is a persistent barrier to complex trait genetic dissection, worsened by the explosive growth of genomic data (1.5 billion variants identified in the UK Biobank WGS study) alongside persistently scarce and subjective human-defined phenotypes. Digital phenotyping offers a potential solution, yet existing tools fail to balance scalable non-manual phenotype generation and biological interpretability of these quantitative traits. Here we report AIPheno, the first generative AI-driven \"phenotype sequencer\" that bridges this gap. It enables high-throughput, unsupervised extraction of digital phenotypes from imaging data and unlocks their biological meaning via generative network analysis. AIPheno transforms imaging modalities into a rich source of quantitative traits, dramatically enhancing cross-species genetic discovery, including novel loci such as CCBE1 (humans), KITLG-TMTC3 (domestic pigeons), and SOD2-IGF2R (swine). Critically, its generative module decodes AI-derived phenotypes by synthesizing variant-specific images to yield actionable biological insights. For example, it clarifies how the OCA2-HERC2 locus pleiotropically links pigmentation to retinal vascular traits via vascular visibility modulation. Integrating scalable non-manual phenotyping, enhanced genetic discovery power, and generative mechanistic decoding, AIPheno establishes a transformative closed-loop paradigm. This work addresses the longstanding genotype-phenotype imbalance, redefines digital phenotype utility, and accelerates translation of genetic associations into actionable understanding with profound implications for human health and agriculture."}
{"id": "2511.13295", "pdf": "https://arxiv.org/pdf/2511.13295", "abs": "https://arxiv.org/abs/2511.13295", "authors": ["Chaowang Lan", "Jingxin Wu", "Yulong Yuan", "Chuxun Liu", "Huangyi Kang", "Caihua Liu"], "title": "Causal Inference, Biomarker Discovery, Graph Neural Network, Feature Selection", "categories": ["q-bio.QM", "cs.LG"], "comment": null, "summary": "Biomarker discovery from high-throughput transcriptomic data is crucial for advancing precision medicine. However, existing methods often neglect gene-gene regulatory relationships and lack stability across datasets, leading to conflation of spurious correlations with genuine causal effects. To address these issues, we develop a causal graph neural network (Causal-GNN) method that integrates causal inference with multi-layer graph neural networks (GNNs). The key innovation is the incorporation of causal effect estimation for identifying stable biomarkers, coupled with a GNN-based propensity scoring mechanism that leverages cross-gene regulatory networks. Experimental results demonstrate that our method achieves consistently high predictive accuracy across four distinct datasets and four independent classifiers. Moreover, it enables the identification of more stable biomarkers compared to traditional methods. Our work provides a robust, efficient, and biologically interpretable tool for biomarker discovery, demonstrating strong potential for broad application across medical disciplines."}
{"id": "2511.11769", "pdf": "https://arxiv.org/pdf/2511.11769", "abs": "https://arxiv.org/abs/2511.11769", "authors": ["Xiangru Wang", "Zekun Jiang", "Heng Yang", "Cheng Tan", "Xingying Lan", "Chunming Xu", "Tianhang Zhou"], "title": "Socrates-Mol: Self-Oriented Cognitive Reasoning through Autonomous Trial-and-Error with Empirical-Bayesian Screening for Molecules", "categories": ["physics.chem-ph", "cs.LG", "q-bio.QM", "stat.ME"], "comment": null, "summary": "Molecular property prediction is fundamental to chemical engineering applications such as solvent screening. We present Socrates-Mol, a framework that transforms language models into empirical Bayesian reasoners through context engineering, addressing cold start problems without model fine-tuning. The system implements a reflective-prediction cycle where initial outputs serve as priors, retrieved molecular cases provide evidence, and refined predictions form posteriors, extracting reusable chemical rules from sparse data. We introduce ranking tasks aligned with industrial screening priorities and employ cross-model self-consistency across five language models to reduce variance. Experiments on amine solvent LogP prediction reveal task-dependent patterns: regression achieves 72% MAE reduction and 112% R-squared improvement through self-consistency, while ranking tasks show limited gains due to systematic multi-model biases. The framework reduces deployment costs by over 70% compared to full fine-tuning, providing a scalable solution for molecular property prediction while elucidating the task-adaptive nature of self-consistency mechanisms."}
{"id": "2511.12459", "pdf": "https://arxiv.org/pdf/2511.12459", "abs": "https://arxiv.org/abs/2511.12459", "authors": ["Marco Pollanen"], "title": "The Probabilistic Foundations of Surveillance Failure: From False Alerts to Structural Bias", "categories": ["stat.ME", "cs.CY", "math.PR", "q-bio.QM", "stat.AP"], "comment": "24 pages, 1 figure", "summary": "For decades, forensic statisticians have debated whether searching large DNA databases undermines the evidential value of a match. Modern surveillance faces an exponentially harder problem: screening populations across thousands of attributes using threshold rules rather than exact matching. Intuition suggests that requiring many coincidental matches should make false alerts astronomically unlikely. This intuition fails.\n  Consider a system that monitors 1,000 attributes, each with a 0.5 percent innocent match rate. Matching 15 pre-specified attributes has probability \\(10^{-35}\\), one in 30 decillion, effectively impossible. But operational systems require no such specificity. They might flag anyone who matches \\emph{any} 15 of the 1,000. In a city of one million innocent people, this produces about 226 false alerts. A seemingly impossible event becomes all but guaranteed. This is not an implementation flaw but a mathematical consequence of high-dimensional screening.\n  We identify fundamental probabilistic limits on screening reliability. Systems undergo sharp transitions from reliable to unreliable with small increases in data scale, a fragility worsened by data growth and correlations. As data accumulate and correlation collapses effective dimensionality, systems enter regimes where alerts lose evidential value even when individual coincidences remain vanishingly rare. This framework reframes the DNA database controversy as a shift between operational regimes. Unequal surveillance exposures magnify failure, making ``structural bias'' mathematically inevitable. These limits are structural: beyond a critical scale, failure cannot be prevented through threshold adjustment or algorithmic refinement."}
{"id": "2511.13124", "pdf": "https://arxiv.org/pdf/2511.13124", "abs": "https://arxiv.org/abs/2511.13124", "authors": ["Changxi Chi", "Yufei Huang", "Jun Xia", "Jiangbin Zheng", "Yunfan Liu", "Zelin Zang", "Stan Z. Li"], "title": "Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance."}
{"id": "2511.13611", "pdf": "https://arxiv.org/pdf/2511.13611", "abs": "https://arxiv.org/abs/2511.13611", "authors": ["Torec T. Luik", "Joost de Folter", "Rodrigo Rosas-Bertolini", "Eric A. J. Reits", "Ron A. Hoebe", "Przemek M. Krawczyk"], "title": "BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance", "categories": ["cs.SE", "q-bio.QM"], "comment": "16 pages, 2 figures, 25 pages supplemental information; for software, see https://github.com/Cellular-Imaging-Amsterdam-UMC/NL-BIOMERO", "summary": "We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing."}
