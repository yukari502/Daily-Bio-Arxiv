<div id=toc></div>

# Table of Contents

- [q-bio.GN](#q-bio.GN) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 6]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [1] [LCPan: efficient variation graph construction using Locally Consistent Parsing](https://arxiv.org/abs/2511.12205)
*Akmuhammet Ashyralyyev,Zülal Bingöl,Begüm Filiz Öz,Salem Malikic,Uzi Vishkin,S. Cenk Sahinalp,Can Alkan*

Main category: q-bio.GN

TL;DR: 提出了首个迭代式局部一致性解析(LCP)实现Lcptools，通过多轮迭代将基因组字符串分割为匹配子串，相比传统方法产生更少的核心子串，实现更紧凑的表示和更快的分析。


<details>
  <summary>Details</summary>
Motivation: 在基因组数据指数级增长的时代，需要高效一致的字符串处理方法。局部一致性解析通过将输入基因组分割为短且精确匹配的子串，确保跨分区的一致性。

Method: 开发了Lcptools工具实现迭代式LCP，通过多轮迭代精炼核心子串，每轮迭代产生更长、信息更丰富的子串用于下游分析。

Result: 实验显示第i轮迭代的核心数量为O(n/c^i)，c≈2.34，平均长度和核心间距为O(c^i)。相比流行草图技术，LCP产生显著更少的核心，实现更紧凑表示和更快分析。LCPan构建变异图比vg快10倍以上，内存使用减少13倍以上。

Conclusion: 迭代式LCP提供了一种高效的基因组字符串处理方法，在计算和内存效率方面具有显著优势，特别适用于大规模基因组数据分析。

Abstract: Efficient and consistent string processing is critical in the exponentially growing genomic data era. Locally Consistent Parsing (LCP) addresses this need by partitioning an input genome string into short, exactly matching substrings (e.g., "cores"), ensuring consistency across partitions. Labeling the cores of an input string consistently not only provides a compact representation of the input but also enables the reapplication of LCP to refine the cores over multiple iterations, providing a progressively longer and more informative set of substrings for downstream analyses.
  We present the first iterative implementation of LCP with Lcptools and demonstrate its effectiveness in identifying cores with minimal collisions. Experimental results show that the number of cores at the i^th iteration is O(n/c^i) for c ~ 2.34, while the average length and the average distance between consecutive cores are O(c^i). Compared to the popular sketching techniques, LCP produces significantly fewer cores, enabling a more compact representation and faster analyses. To demonstrate the advantages of LCP in genomic string processing in terms of computation and memory efficiency, we also introduce LCPan, an efficient variation graph constructor. We show that LCPan generates variation graphs >10x faster than vg, while using >13x less memory.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [2] [Protein Structure Tokenization via Geometric Byte Pair Encoding](https://arxiv.org/abs/2511.11758)
*Michael Sun,Weize Yuan,Gang Liu,Wojciech Matusik,Marinka Zitnik*

Main category: q-bio.QM

TL;DR: GeoBPE是一种基于几何的蛋白质结构标记化方法，将连续的多尺度蛋白质结构转化为离散的几何"句子"，提供压缩、数据效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质结构标记化方法存在局限性：固定标记大小或依赖连续向量码本，限制了可解释性、多尺度控制和跨架构迁移。需要一种原则性的蛋白质结构标记化方法。

Method: GeoBPE通过迭代过程生成几何基元的分层词汇表：(i)使用k-medoids聚类Geo-Pair出现；(ii)将每个Geo-Pair量化到最近的质心原型；(iii)通过可微分逆运动学优化边界胶合角度来减少漂移。

Result: GeoBPE提供超过10倍的压缩率（在相似失真率下每残基比特数减少）、超过10倍的数据效率，并保持1.0-1.1的测试/训练失真比。在12个任务和24个测试分割中始终优于领先的PST方法。

Conclusion: GeoBPE是一种架构无关的蛋白质结构标记化方法，支持无条件骨架生成，标记与CATH功能家族对齐，提供先前PST中缺乏的功能意义。

Abstract: Protein structure is central to biological function, and enabling multimodal protein models requires joint reasoning over sequence, structure, and function. A key barrier is the lack of principled protein structure tokenizers (PSTs): existing approaches fix token size or rely on continuous vector codebooks, limiting interpretability, multi-scale control, and transfer across architectures. We introduce GeoBPE, a geometry-grounded PST that transforms continuous, noisy, multi-scale backbone conformations into discrete ``sentences'' of geometry while enforcing global constraints. Analogous to byte-pair encoding, GeoBPE generates a hierarchical vocabulary of geometric primitives by iteratively (i) clustering Geo-Pair occurrences with k-medoids to yield a resolution-controllable vocabulary; (ii) quantizing each Geo-Pair to its closest medoid prototype; and (iii) reducing drift through differentiable inverse kinematics that optimizes boundary glue angles under an $\mathrm{SE}(3)$ end-frame loss. GeoBPE offers compression ($>$10x reduction in bits-per-residue at similar distortion rate), data efficiency ($>$10x less training data), and generalization (maintains test/train distortion ratio of $1.0-1.1$). It is architecture-agnostic: (a) its hierarchical vocabulary provides a strong inductive bias for coarsening residue-level embeddings from large PLMs into motif- and protein-level representations, consistently outperforming leading PSTs across $12$ tasks and $24$ test splits; (b) paired with a transformer, GeoBPE supports unconditional backbone generation via language modeling; and (c) tokens align with CATH functional families and support expert-interpretable case studies, offering functional meaning absent in prior PSTs. Code is available at https://github.com/shiningsunnyday/PT-BPE/.

</details>


### [3] [SarcGraph for High-Throughput Regional Analysis of Sarcomere Organization and Contractile Function in 2D Cardiac Muscle Bundles](https://arxiv.org/abs/2511.11913)
*Saeed Mohammadzadeh,Yao-Chang Tsan,Aaron Renberg,Hiba Kobeissi,Adam Helms,Emma Lejeune*

Main category: q-bio.QM

TL;DR: 开发了改进的SarcGraph工具包，用于自动分析hiPSC-CMs二维心肌束的高帧率图像，解决了传统方法在更大生理位移下的追踪不可靠问题。


<details>
  <summary>Details</summary>
Motivation: hiPSC-CMs的延时图像提供了丰富的细胞结构和收缩功能信息，但组织样本的可重复生成和规模化实验具有挑战性。二维心肌束平台虽然标准化了组织几何形状，但由于其更大的生理性肌节位移和速度，现有的肌节追踪方法不可靠。

Method: 对SarcGraph开源Python包进行改进：1）切换到逐帧肌节检测方法，使用空间分区自动组织分割；2）执行高斯过程回归进行信号去噪；3）加入自动收缩相位检测流程。

Result: 实现了对完整2DMB组织和不同组织区域的结构组织和功能收缩指标的自动提取，并发布了包含130个示例视频的数据集。

Conclusion: 通过提供开源工具和数据集，旨在实现对工程化心脏组织的高通量分析，推动hiPSC-CM研究社区的集体进步。

Abstract: Timelapse images of human induced pluripotent stem cell-derived cardiomyocytes (hiPSC-CMs) provide rich information on cell structure and contractile function. However, it is challenging to reproducibly generate tissue samples and conduct scalable experiments with these cells. The two-dimensional cardiac muscle bundle (2DMB) platform helps address these limitations by standardizing tissue geometry, resulting in physiologic, uniaxial contractions of discrete tissues on an elastomeric substrate with stiffness similar to the heart. 2DMBs are highly conducive to sarcomere imaging using fluorescent reporters, but, due to their larger and more physiologic sarcomere displacements and velocities, prior sarcomere-tracking pipelines have been unreliable. Here, we present adaptations to SarcGraph, an open-source Python package for sarcomere detection and tracking, that enable automated analysis of high-frame-rate 2DMB recordings. Key modifications to the pipeline include: 1) switching to a frame-by-frame sarcomere detection approach and automating tissue segmentation with spatial partitioning, 2) performing Gaussian Process Regression for signal denoising, and 3) incorporating an automatic contractile phase detection pipeline. These enhancements enable the extraction of structural organization and functional contractility metrics for both the whole 2DMB tissue and distinct tissue regions, both in a fully automated manner. We complement this software release with a dataset of 130 example movies of baseline and drug-treated samples disseminated through the Harvard Dataverse. By providing open-source tools and datasets, we aim to enable high-throughput analysis of engineered cardiac tissues and advance collective progress within the hiPSC-CM research community.

</details>


### [4] [Explainable deep learning framework for cancer therapeutic target prioritization leveraging PPI centrality and node embeddings](https://arxiv.org/abs/2511.12463)
*Adham M. Alkhadrawi,Kyungsu Kim,Arif M. Rahman*

Main category: q-bio.QM

TL;DR: 开发了一个可解释的深度学习框架，整合蛋白质相互作用网络中心性指标和节点嵌入，用于癌症治疗靶点优先排序，在识别关键基因方面达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 整合网络生物学与可解释AI，为癌症治疗靶点发现提供机制透明性，同时保持最先进的预测性能，展示无动物建模的实践案例。

Method: 构建高置信度PPI网络，计算6种中心性指标，使用Node2Vec嵌入捕获网络拓扑，结合XGBoost和神经网络分类器，采用GradientSHAP分析特征贡献，开发混合评分方法。

Result: AUROC达0.930，AUPRC达0.656，成功识别已知关键基因如核糖体蛋白和癌基因，中心性指标特别是度中心性与基因必要性显著相关。

Conclusion: 该框架成功整合网络生物学与可解释AI，为癌症治疗靶点发现提供机制透明性和最先进预测性能，是可重复、无动物建模的下一代方法范例。

Abstract: We developed an explainable deep learning framework integrating protein-protein interaction (PPI) network centrality metrics with node embeddings for cancer therapeutic target prioritization. A high-confidence PPI network was constructed from STRING database interactions, computing six centrality metrics: degree, strength, betweenness, closeness, eigenvector centrality, and clustering coefficient. Node2Vec embeddings captured latent network topology. Combined features trained XGBoost and neural network classifiers using DepMap CRISPR essentiality scores as ground truth. Model interpretability was assessed through GradientSHAP analysis quantifying feature contributions. We developed a novel blended scoring approach combining model probability predictions with SHAP attribution magnitudes for enhanced gene prioritization. Our framework achieved state-of-the-art performance with AUROC of 0.930 and AUPRC of 0.656 for identifying the top 10\% most essential genes. GradientSHAP analysis revealed centrality measures contributed significantly to predictions, with degree centrality showing strongest correlation ($ρ$ = -0.357) with gene essentiality. The blended scoring approach created robust gene prioritization rankings, successfully identifying known essential genes including ribosomal proteins (RPS27A, RPS17, RPS6) and oncogenes (MYC). This study presents a human-based, combinatorial \textit{in silico} framework successfully integrating network biology with explainable AI for therapeutic target discovery. The framework provides mechanistic transparency through feature attribution analysis while maintaining state-of-the-art predictive performance. Its reproducible design and reliance on human molecular datasets demonstrate a reduction-to-practice example of next-generation, animal-free modeling for cancer therapeutic target discovery and prioritization.

</details>


### [5] [Brain Networks Flow-Topology via Variance Minimization in the Wasserstein Space](https://arxiv.org/abs/2511.12990)
*Sixtus Dakurah*

Main category: q-bio.QM

TL;DR: 提出了一种结合Hodge分解和Wasserstein方差最小化的新框架，用于测试加权网络的拓扑变异性，相比传统方法能更好地抑制噪声扰动并检测真实拓扑差异。


<details>
  <summary>Details</summary>
Motivation: 传统分析原始边权重的方法容易受到噪声扰动影响，限制了检测网络群体间有意义结构差异的能力。

Method: 使用组合Hodge理论将网络信号分解为不同分量，通过持久图之间的2-Wasserstein距离量化拓扑差异，测试统计量比较组内与组间在Wasserstein空间中的离散度方差减少。

Result: 模拟显示该方法能抑制小随机扰动，同时保持对真实拓扑差异的敏感性，特别是在应用于Hodge分解流而非原始边权重时。

Conclusion: 该框架成功应用于ADHD多模式治疗数据集中的功能性脑网络，比较了大麻使用者和非使用者。

Abstract: This work introduces a novel framework for testing topological variability in weighted networks by combining Hodge decomposition with Wasserstein variance minimization. Traditional approaches that analyze raw edge weights are susceptible to noise driven perturbations, limiting their ability to detect meaningful structural differences between network populations. Network signals are decomposed into various components using combinatorial Hodge theory, then topological disparity is quantified via the 2-Wasserstein distance between persistence diagrams. The test statistic measures variance reduction when comparing within group to between group dispersions in the Wasserstein space. Simulations demonstrate that the proposed method suppresses small random perturbations while maintaining sensitivity to genuine topological differences, particularly when applied to Hodge decomposed flows rather than raw edge weights. The framework is applied to functional brain networks from the Multimodal Treatment of ADHD dataset, comparing cannabis users and non-users

</details>


### [6] [Bridging the genotype-phenotype gap with generative artificial intelligence](https://arxiv.org/abs/2511.13141)
*Yangfan Liu,Xiong Xiong,Yong Liao,Mingli Qin,Zhen Huang,Shilin Zhu,Lilin Yin,Yuhua Fu,Haohao Zhang,Jingya Xu,Dong Yin,Xin Huang,Yuan Quan,Xuan Li,Tengfei Jiang,Wanneng Yang,Xiaohui Yuan,Laurent Frantz,Xinyun Li,Xiaolei Liu,Shuhong Zhao*

Main category: q-bio.QM

TL;DR: AIPheno是一个生成式AI驱动的表型测序器，通过无监督提取影像数据的数字表型并进行生成网络分析，解决基因型-表型差距问题，显著增强跨物种遗传发现能力。


<details>
  <summary>Details</summary>
Motivation: 解决基因型-表型差距问题，传统方法面临基因组数据爆炸增长（如UK Biobank WGS研究识别15亿变异）与稀缺主观人工定义表型之间的矛盾，现有数字表型工具无法平衡可扩展性和生物学可解释性。

Method: 开发AIPheno生成式AI系统，将影像模态转换为丰富的定量性状，通过生成网络分析解码AI衍生表型的生物学意义，合成变异特异性图像提供可操作的生物学见解。

Result: 成功识别多个新基因位点：CCBE1（人类）、KITLG-TMTC3（家鸽）、SOD2-IGF2R（猪），并阐明OCA2-HERC2位点通过血管可见度调节将色素沉着与视网膜血管特征多效性关联的机制。

Conclusion: AIPheno建立了变革性的闭环范式，解决了长期存在的基因型-表型不平衡问题，重新定义了数字表型效用，加速了遗传关联向可操作理解的转化，对人类健康和农业具有深远影响。

Abstract: The genotype-phenotype gap is a persistent barrier to complex trait genetic dissection, worsened by the explosive growth of genomic data (1.5 billion variants identified in the UK Biobank WGS study) alongside persistently scarce and subjective human-defined phenotypes. Digital phenotyping offers a potential solution, yet existing tools fail to balance scalable non-manual phenotype generation and biological interpretability of these quantitative traits. Here we report AIPheno, the first generative AI-driven "phenotype sequencer" that bridges this gap. It enables high-throughput, unsupervised extraction of digital phenotypes from imaging data and unlocks their biological meaning via generative network analysis. AIPheno transforms imaging modalities into a rich source of quantitative traits, dramatically enhancing cross-species genetic discovery, including novel loci such as CCBE1 (humans), KITLG-TMTC3 (domestic pigeons), and SOD2-IGF2R (swine). Critically, its generative module decodes AI-derived phenotypes by synthesizing variant-specific images to yield actionable biological insights. For example, it clarifies how the OCA2-HERC2 locus pleiotropically links pigmentation to retinal vascular traits via vascular visibility modulation. Integrating scalable non-manual phenotyping, enhanced genetic discovery power, and generative mechanistic decoding, AIPheno establishes a transformative closed-loop paradigm. This work addresses the longstanding genotype-phenotype imbalance, redefines digital phenotype utility, and accelerates translation of genetic associations into actionable understanding with profound implications for human health and agriculture.

</details>


### [7] [Causal Inference, Biomarker Discovery, Graph Neural Network, Feature Selection](https://arxiv.org/abs/2511.13295)
*Chaowang Lan,Jingxin Wu,Yulong Yuan,Chuxun Liu,Huangyi Kang,Caihua Liu*

Main category: q-bio.QM

TL;DR: 提出了Causal-GNN方法，将因果推断与图神经网络结合，用于从高通量转录组数据中发现稳定可靠的生物标志物。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽略基因-基因调控关系，且在不同数据集间缺乏稳定性，容易将虚假相关误认为真实因果效应。

Method: 开发了因果图神经网络方法，整合因果推断与多层图神经网络，结合因果效应估计和基于GNN的倾向评分机制，利用跨基因调控网络。

Result: 在四个不同数据集和四个独立分类器上均获得一致的高预测准确率，相比传统方法能识别更稳定的生物标志物。

Conclusion: 该方法为生物标志物发现提供了稳健、高效且具有生物学可解释性的工具，在医学各领域具有广泛应用潜力。

Abstract: Biomarker discovery from high-throughput transcriptomic data is crucial for advancing precision medicine. However, existing methods often neglect gene-gene regulatory relationships and lack stability across datasets, leading to conflation of spurious correlations with genuine causal effects. To address these issues, we develop a causal graph neural network (Causal-GNN) method that integrates causal inference with multi-layer graph neural networks (GNNs). The key innovation is the incorporation of causal effect estimation for identifying stable biomarkers, coupled with a GNN-based propensity scoring mechanism that leverages cross-gene regulatory networks. Experimental results demonstrate that our method achieves consistently high predictive accuracy across four distinct datasets and four independent classifiers. Moreover, it enables the identification of more stable biomarkers compared to traditional methods. Our work provides a robust, efficient, and biologically interpretable tool for biomarker discovery, demonstrating strong potential for broad application across medical disciplines.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [8] [Practical Causal Evaluation Metrics for Biological Networks](https://arxiv.org/abs/2511.12805)
*Noriaki Sato,Marco Scutari,Shuichi Kawano,Rui Yamaguchi,Seiya Imoto*

Main category: q-bio.MN

TL;DR: 提出了sSID（符号增强结构干预距离）及其加权版本，用于评估基因调控网络推断结果，考虑了生物数据库中关系的定性性质，能识别出在功能上更正确的网络。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标很少考虑生物数据库中关系的定性性质，而基于干预效应的网络评估对于下游概率推理和药物靶点识别至关重要。

Method: 开发了sSID和加权sSID指标，通过模拟和真实转录组数据集分析验证其有效性。

Result: 与传统指标相比，sSID能识别出不同的最优算法，且sSID选出的网络在临床协变量分类任务中表现更优。

Conclusion: sSID能够区分结构正确但功能不正确的网络，具有更好的生物学意义和实用性。

Abstract: Estimating causal networks from biological data is a critical step in systems biology. When evaluating the inferred network, assessing the networks based on their intervention effects is particularly important for downstream probabilistic reasoning and the identification of potential drug targets. In the context of gene regulatory network inference, biological databases are often used as reference sources. These databases typically describe relationships in a qualitative rather than quantitative manner. However, few evaluation metrics have been developed that take this qualitative nature into account. To address this, we developed a metric, the sign-augmented Structural Intervention Distance (sSID), and a weighted sSID that incorporates the net effects of the intervention. Through simulations and analyses of real transcriptomic datasets, we found that our proposed metrics could identify a different algorithm as optimal compared to conventional metrics, and the network selected by sSID had a superior performance in the classification task of clinical covariates using transcriptomic data. This suggests that sSID can distinguish networks that are structurally correct but functionally incorrect, highlighting its potential as a more biologically meaningful and practical evaluation metric.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [9] [BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance](https://arxiv.org/abs/2511.13611)
*Torec T. Luik,Joost de Folter,Rodrigo Rosas-Bertolini,Eric A. J. Reits,Ron A. Hoebe,Przemek M. Krawczyk*

Main category: cs.SE

TL;DR: BIOMERO 2.0将OMERO升级为FAIR合规、可溯源生物成像平台，通过容器化组件集成数据导入、预处理、分析和流程监控，实现端到端的可追溯工作流。


<details>
  <summary>Details</summary>
Motivation: 解决生物成像数据在导入、预处理、分析和共享过程中缺乏标准化、可追溯性的问题，使OMERO成为FAIR合规的完整分析平台。

Method: 开发OMERO.web插件和容器化组件，包括导入子系统（容器化预处理和元数据丰富）和分析子系统（通过BIOMERO Python库协调HPC系统上的容器化分析），实时记录参数、版本和结果。

Result: 实现了实时可溯源性，通过集成仪表板可访问所有导入和分析记录，将OMERO置于生物成像分析过程的核心位置。

Conclusion: BIOMERO 2.0通过双层集成方法显著提升了OMERO的FAIR化程度，为图像分析提供了可追溯、可重用的工作流，弥合了数据导入、分析和共享之间的鸿沟。

Abstract: We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Multiscale Grassmann Manifolds for Single-Cell Data Analysis](https://arxiv.org/abs/2511.11717)
*Xiang Xiang Wang,Sean Cottrell,Guo-Wei Wei*

Main category: cs.LG

TL;DR: 提出基于Grassmann流形的多尺度框架，用于单细胞数据分析，通过多尺度嵌入整合不同几何视图，在基准数据集上表现出良好的结构保持和聚类稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统单细胞数据分析方法将细胞表示为欧几里得空间中的向量，难以捕捉内在相关性和多尺度几何结构。

Method: 基于Grassmann流形的多尺度框架，集成机器学习与子空间几何，通过多尺度表示生成嵌入，使用基于幂的尺度采样函数控制尺度选择。

Result: 在九个基准单细胞RNA-seq数据集上的实验表明，该方法能有效保持有意义的结构，提供稳定的聚类性能，尤其适用于中小型数据集。

Conclusion: Grassmann流形为单细胞数据分析提供了连贯且信息丰富的基础。

Abstract: Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.

</details>


### [11] [Genomic Next-Token Predictors are In-Context Learners](https://arxiv.org/abs/2511.12797)
*Nathan Breslow,Aayush Mishra,Mahler Revsine,Michael C. Schatz,Anqi Liu,Daniel Khashabi*

Main category: cs.LG

TL;DR: 研究发现基因组模型在上下文学习中表现出与语言模型类似的能力，表明上下文学习是大规模预测建模的普遍特性，而不仅限于语言领域。


<details>
  <summary>Details</summary>
Motivation: 探索上下文学习是否能在非语言序列领域（如基因组）通过大规模预测训练自然涌现，挑战上下文学习仅源于人类语言独特统计特性的观点。

Method: 开发受控实验框架，在语言和基因组形式中实例化符号推理任务，直接比较基因组模型和语言模型的上下文学习能力。使用Evo2基因组模型进行大规模核苷酸预测训练。

Result: 基因组模型与语言模型类似，随着上下文演示数量增加，在模式归纳方面表现出对数线性增益。这是首次在基因组序列中发现自然涌现的上下文学习证据。

Conclusion: 上下文学习是大规模预测建模在丰富数据上的结果，具有模态无关的统一特性，超越了语言领域的限制。

Abstract: In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?
  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.

</details>


### [12] [Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering](https://arxiv.org/abs/2511.13705)
*Alaa Mezghiche*

Main category: cs.LG

TL;DR: 通过自编码器和聚类分析，在KIRC癌症中发现了一个罕见但稳定的分子亚型，而泛癌分析主要受组织来源影响。


<details>
  <summary>Details</summary>
Motivation: 在RNA-seq数据中发现超越标准标签的分子亚型，特别是寻找罕见但可重复的基因组亚型。

Method: 结合自编码器表示与聚类和稳定性分析，选择高变异基因，训练前馈自编码器，运行k-means聚类，并使用预定义发现规则识别罕见稳定亚型。

Result: 泛癌分析显示聚类与组织来源高度一致（Cramer's V = 0.887），而在KIRC中发现了k=5的聚类方案，其中罕见簇C0（6.85%患者）高度稳定（Jaccard = 0.787）。

Conclusion: 泛癌聚类主要受组织来源主导，而基于稳定性的癌症内分析方法能够揭示罕见且可重复的KIRC亚型。

Abstract: Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.

</details>


### [13] [Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges](https://arxiv.org/abs/2511.13124)
*Changxi Chi,Yufei Huang,Jun Xia,Jiangbin Zheng,Yunfan Liu,Zelin Zang,Stan Z. Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于薛定谔桥近似的方法，用于直接对齐不同扰动条件下控制和扰动单细胞群体的分布，解决了单细胞扰动数据无配对的问题，并在遗传和药物扰动数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞扰动结果预测对基因功能分析和药物候选选择至关重要，但由于测序的破坏性，单细胞数据本质上是无配对的。现有的神经生成传输模型要么缺乏显式条件，要么依赖先验空间进行间接分布对齐，限制了精确的扰动建模。

Method: 通过近似薛定谔桥来直接对齐控制和扰动单细胞群体的分布，利用Minibatch-OT基于配对的方法避免双向推理，直接指导桥学习。近似了两个SB模型：一个建模离散基因激活状态，另一个建模连续表达分布。

Result: 在公共遗传和药物扰动数据集上的实验表明，该模型有效捕获了异质性单细胞响应，并实现了最先进的性能。

Conclusion: 该方法通过直接分布对齐和联合训练，能够准确建模扰动并捕获单细胞异质性，为单细胞扰动预测提供了有效的解决方案。

Abstract: Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [14] [Socrates-Mol: Self-Oriented Cognitive Reasoning through Autonomous Trial-and-Error with Empirical-Bayesian Screening for Molecules](https://arxiv.org/abs/2511.11769)
*Xiangru Wang,Zekun Jiang,Heng Yang,Cheng Tan,Xingying Lan,Chunming Xu,Tianhang Zhou*

Main category: physics.chem-ph

TL;DR: Socrates-Mol框架通过上下文工程将语言模型转化为经验贝叶斯推理器，解决分子属性预测中的冷启动问题，无需模型微调即可实现72%的MAE降低和112%的R平方提升，部署成本降低70%以上。


<details>
  <summary>Details</summary>
Motivation: 解决分子属性预测中的冷启动问题，为化学工程应用如溶剂筛选提供无需微调的可扩展解决方案。

Method: 通过上下文工程实现反思-预测循环：初始输出作为先验，检索分子案例提供证据，精炼预测形成后验，从稀疏数据中提取可重用化学规则；引入与工业筛选优先级对齐的排序任务，并采用跨模型自一致性来减少方差。

Result: 在胺溶剂LogP预测实验中，回归任务通过自一致性实现72% MAE降低和112% R平方提升，但排序任务由于系统性多模型偏差而增益有限。

Conclusion: 该框架为分子属性预测提供了可扩展解决方案，同时阐明了自一致性机制的任务适应性特性，显著降低了部署成本。

Abstract: Molecular property prediction is fundamental to chemical engineering applications such as solvent screening. We present Socrates-Mol, a framework that transforms language models into empirical Bayesian reasoners through context engineering, addressing cold start problems without model fine-tuning. The system implements a reflective-prediction cycle where initial outputs serve as priors, retrieved molecular cases provide evidence, and refined predictions form posteriors, extracting reusable chemical rules from sparse data. We introduce ranking tasks aligned with industrial screening priorities and employ cross-model self-consistency across five language models to reduce variance. Experiments on amine solvent LogP prediction reveal task-dependent patterns: regression achieves 72% MAE reduction and 112% R-squared improvement through self-consistency, while ranking tasks show limited gains due to systematic multi-model biases. The framework reduces deployment costs by over 70% compared to full fine-tuning, providing a scalable solution for molecular property prediction while elucidating the task-adaptive nature of self-consistency mechanisms.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [15] [The Probabilistic Foundations of Surveillance Failure: From False Alerts to Structural Bias](https://arxiv.org/abs/2511.12459)
*Marco Pollanen*

Main category: stat.ME

TL;DR: 论文揭示了大规模多属性筛查系统的根本局限性：即使单个巧合概率极低，系统仍会产生大量误报，这是高维筛查的数学必然结果。


<details>
  <summary>Details</summary>
Motivation: 解决现代监控系统中大规模多属性筛查的可靠性问题，传统直觉认为多个巧合匹配会极大降低误报率，但实际在高维空间中这一直觉失效。

Method: 通过概率论分析高维筛查系统的数学特性，识别系统可靠性的基本限制和临界点。

Result: 发现系统在数据规模小幅增加时会经历从可靠到不可靠的急剧转变，误报不可避免，且数据相关性和结构性偏差会加剧这一问题。

Conclusion: 大规模筛查系统存在结构性限制，超过临界规模后无法通过调整阈值或算法改进来防止误报，这对DNA数据库争议和监控系统设计有重要启示。

Abstract: For decades, forensic statisticians have debated whether searching large DNA databases undermines the evidential value of a match. Modern surveillance faces an exponentially harder problem: screening populations across thousands of attributes using threshold rules rather than exact matching. Intuition suggests that requiring many coincidental matches should make false alerts astronomically unlikely. This intuition fails.
  Consider a system that monitors 1,000 attributes, each with a 0.5 percent innocent match rate. Matching 15 pre-specified attributes has probability \(10^{-35}\), one in 30 decillion, effectively impossible. But operational systems require no such specificity. They might flag anyone who matches \emph{any} 15 of the 1,000. In a city of one million innocent people, this produces about 226 false alerts. A seemingly impossible event becomes all but guaranteed. This is not an implementation flaw but a mathematical consequence of high-dimensional screening.
  We identify fundamental probabilistic limits on screening reliability. Systems undergo sharp transitions from reliable to unreliable with small increases in data scale, a fragility worsened by data growth and correlations. As data accumulate and correlation collapses effective dimensionality, systems enter regimes where alerts lose evidential value even when individual coincidences remain vanishingly rare. This framework reframes the DNA database controversy as a shift between operational regimes. Unequal surveillance exposures magnify failure, making ``structural bias'' mathematically inevitable. These limits are structural: beyond a critical scale, failure cannot be prevented through threshold adjustment or algorithmic refinement.

</details>
