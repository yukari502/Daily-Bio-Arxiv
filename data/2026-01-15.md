<div id=toc></div>

# Table of Contents

- [stat.ME](#stat.ME) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1] [Semiparametric estimation of GLMs with interval-censored covariates via an augmented Turnbull estimator](https://arxiv.org/abs/2601.08996)
*Andrea Toloba,Klaus Langohr,Guadalupe Gómez Melis*

Main category: stat.ME

TL;DR: 提出GELc方法，用于处理广义线性模型中区间删失协变量的估计问题，基于Turnbull非参数估计器的增强版本，具有一致性和渐近正态性。


<details>
  <summary>Details</summary>
Motivation: 区间删失协变量在生物医学研究中常见（如时间-事件数据、检测限问题），但相关回归模型估计方法发展不足，需要更有效的统计方法。

Method: 提出基于似然的GELc方法，构建在Turnbull区间删失数据非参数估计器的增强版本上，用于估计广义线性模型中区间删失协变量。

Result: 证明GELc估计量在温和正则条件下具有一致性和渐近正态性，模拟研究显示良好的有限样本性能和置信区间覆盖率，并通过两个真实应用验证。

Conclusion: GELc为处理区间删失协变量的广义线性模型提供了有效的似然方法，已实现为R包，适用于生物医学研究中的实际数据分析。

Abstract: Interval-censored covariates are frequently encountered in biomedical studies, particularly in time-to-event data or when measurements are subject to detection or quantification limits. Yet, the estimation of regression models with interval-censored covariates remains methodologically underdeveloped. In this article, we address the estimation of generalized linear models when one covariate is subject to interval censoring. We propose a likelihood-based approach, GELc, that builds upon an augmented version of Turnbull's nonparametric estimator for interval-censored data. We prove that the GELc estimator is consistent and asymptotically normal under mild regularity conditions, with available standard errors. Simulation studies demonstrate favorable finite-sample performance of the estimator and satisfactory coverage of the confidence intervals. Finally, we illustrate the method using two real-world applications: the AIDS Clinical Trials Group Study 359 and an observational nutrition study on circulating carotenoids. The proposed methodology is available as an R package at github.com/atoloba/ICenCov.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [2] [Breaking the Bottlenecks: Scalable Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2601.08963)
*Adrita Das,Peiran Jiang,Dantong Zhu,Barnabas Poczos,Jose Lugo-Martinez*

Main category: cs.LG

TL;DR: 本文通过反向转移核(RTK)框架重新解释直接去噪扩散模型(DDDM)，为确定性去噪提供理论依据，解决了分子扩散模型中的采样效率、数值稳定性和结构保真度问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在分子设计中表现出强大能力，但存在采样轨迹长、反向过程随机方差大、去噪动态结构意识有限等问题。DDDM通过确定性去噪步骤提高了效率，但其理论基础不明确，需要建立统一的理论框架来解释确定性扩散过程。

Method: 采用Huang等人(2024)的反向转移核(RTK)框架，将DDDM的反向过程重新解释为近似核算子，表明直接去噪过程隐式优化了噪声样本与干净样本之间的结构化传输映射。该方法确保数值稳定性、消除随机方差，并实现可扩展且保持SE(3)等变性的去噪器。

Result: 在GEOM-DRUGS数据集上的实验表明，RTK引导的确定性去噪比随机扩散模型收敛更快、结构保真度更高，同时保持化学有效性。该方法解决了分子扩散中的多个长期瓶颈问题。

Conclusion: 通过RTK框架为DDDM提供了理论依据，统一了确定性和随机性扩散过程，显著提高了分子生成模型的效率、稳定性和结构保真度，为分子设计中的扩散模型应用提供了更可靠的理论基础。

Abstract: Diffusion models have emerged as a powerful class of generative models for molecular design, capable of capturing complex structural distributions and achieving high fidelity in 3D molecule generation. However, their widespread use remains constrained by long sampling trajectories, stochastic variance in the reverse process, and limited structural awareness in denoising dynamics. The Directly Denoising Diffusion Model (DDDM) mitigates these inefficiencies by replacing stochastic reverse MCMC updates with deterministic denoising step, substantially reducing inference time. Yet, the theoretical underpinnings of such deterministic updates have remained opaque. In this work, we provide a principled reinterpretation of DDDM through the lens of the Reverse Transition Kernel (RTK) framework by Huang et al. 2024, unifying deterministic and stochastic diffusion under a shared probabilistic formalism. By expressing the DDDM reverse process as an approximate kernel operator, we show that the direct denoising process implicitly optimizes a structured transport map between noisy and clean samples. This perspective elucidates why deterministic denoising achieves efficient inference. Beyond theoretical clarity, this reframing resolves several long-standing bottlenecks in molecular diffusion. The RTK view ensures numerical stability by enforcing well-conditioned reverse kernels, improves sample consistency by eliminating stochastic variance, and enables scalable and symmetry-preserving denoisers that respect SE(3) equivariance. Empirically, we demonstrate that RTK-guided deterministic denoising achieves faster convergence and higher structural fidelity than stochastic diffusion models, while preserving chemical validity across GEOM-DRUGS dataset. Code, models, and datasets are publicly available in our project repository.

</details>


### [3] [Geometric Stability: The Missing Axis of Representations](https://arxiv.org/abs/2601.09173)
*Prashant C. Raju*

Main category: cs.LG

TL;DR: 论文提出几何稳定性作为表示分析的新维度，与相似性正交，用于量化表示结构在扰动下的鲁棒性，并开发了Shesha框架进行测量。


<details>
  <summary>Details</summary>
Motivation: 现有表示分析主要关注相似性（表示与外部参考的对齐程度），但相似性只揭示表示了什么，无法评估表示结构的鲁棒性。需要一个新的维度来量化表示几何在扰动下的可靠性。

Method: 提出几何稳定性概念，开发Shesha框架进行测量。在7个领域的2463个配置中进行实验，对比稳定性与相似性的关系，分析它们在机制上的差异（如移除主成分后的表现）。

Result: 稳定性与相似性在经验上不相关（ρ≈0.01），机制上不同：相似性在移除主成分后崩溃，而稳定性对精细流形结构保持敏感。稳定性在安全监控中比CKA敏感2倍，能预测线性可控性（ρ=0.89-0.96），并揭示迁移优化带来的几何代价。

Conclusion: 几何稳定性是表示分析的必要补充，量化系统如何可靠地维持结构，为生物和计算系统的表示审计提供新视角，在安全监控、可控性预测和模型选择等方面具有实际应用价值。

Abstract: Analysis of learned representations has a blind spot: it focuses on $similarity$, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce $geometric$ $stability$, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present $Shesha$, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated ($ρ\approx 0.01$) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2$\times$ more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability ($ρ= 0.89$-$0.96$); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying $how$ $reliably$ systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.

</details>
