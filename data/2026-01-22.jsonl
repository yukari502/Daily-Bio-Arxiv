{"id": "2601.14624", "pdf": "https://arxiv.org/pdf/2601.14624", "abs": "https://arxiv.org/abs/2601.14624", "authors": ["Simeng Zhang", "Xinying Liu", "Jun Lou", "Mudi Jiang", "Quan Zou", "Zengyou He"], "title": "Biological Sequence Clustering: A Survey", "categories": ["q-bio.GN"], "comment": null, "summary": "The rapid development of high-throughput sequencing technologies has led to an explosive increase in biological sequence data, making sequence clustering a fundamental task in large-scale bioinformatics analyses. Unlike traditional clustering problems, biological sequence clustering faces unique challenges due to the lack of direct similarity measures, strict biological constraints, and demanding requirements for both scalability and accuracy. Over the past decades, a wide variety of methods have been developed, differing in how they model sequence similarity, construct clusters, and prioritize optimization objectives. In this review, we provide a comprehensive methodological overview of biological sequence clustering algorithms. We begin by summarizing the main strategies for modeling sequence similarity, which can be divided into three stages: sequence encoding, feature generation, and similarity measurement. Next, we discuss the major clustering paradigms, including greedy incremental, hierarchical, graph-based, model-based, partitional, and deep learning approaches, highlighting their methodological characteristics and practical trade-offs. We then discuss clustering objectives from three key perspectives: scalability and resource efficiency, biological interpretability, and robustness and clustering quality. Organizing existing methods along these dimensions allows us to explore the trade-offs in biological sequence clustering and clarify the contexts in which different approaches are most appropriate. Finally, we identify current limitations and challenges, providing guidance for researchers and directions for future method development."}
{"id": "2601.14969", "pdf": "https://arxiv.org/pdf/2601.14969", "abs": "https://arxiv.org/abs/2601.14969", "authors": ["Yiyao Yang"], "title": "Robust Machine Learning for Regulatory Sequence Modeling under Biological and Technical Distribution Shifts", "categories": ["q-bio.GN", "stat.ML"], "comment": "19 pages, 16 figures", "summary": "Robust machine learning for regulatory genomics is studied under biologically and technically induced distribution shifts. Deep convolutional and attention based models achieve strong in distribution performance on DNA regulatory sequence prediction tasks but are usually evaluated under i.i.d. assumptions, even though real applications involve cell type specific programs, evolutionary turnover, assay protocol changes, and sequencing artifacts. We introduce a robustness framework that combines a mechanistic simulation benchmark with real data analysis on a massively parallel reporter assay (MPRA) dataset to quantify performance degradation, calibration failures, and uncertainty based reliability. In simulation, motif driven regulatory outputs are generated with cell type specific programs, PWM perturbations, GC bias, depth variation, batch effects, and heteroscedastic noise, and CNN, BiLSTM, and transformer models are evaluated. Models remain accurate and reasonably calibrated under mild GC content shifts but show higher error, severe variance miscalibration, and coverage collapse under motif effect rewiring and noise dominated regimes, revealing robustness gaps invisible to standard i.i.d. evaluation. Adding simple biological structural priors motif derived features in simulation and global GC content in MPRA improves in distribution error and yields consistent robustness gains under biologically meaningful genomic shifts, while providing only limited protection against strong assay noise. Uncertainty-aware selective prediction offers an additional safety layer that risk coverage analyses on simulated and MPRA data show that filtering low confidence inputs recovers low risk subsets, including under GC-based out-of-distribution conditions, although reliability gains diminish when noise dominates."}
{"id": "2601.14536", "pdf": "https://arxiv.org/pdf/2601.14536", "abs": "https://arxiv.org/abs/2601.14536", "authors": ["Tiantian Yang", "Yuxuan Wang", "Zhenwei Zhou", "Ching-Ti Liu"], "title": "engGNN: A Dual-Graph Neural Network for Omics-Based Disease Classification and Feature Selection", "categories": ["cs.LG", "q-bio.GN", "stat.ML"], "comment": "21 pages, 14 figures, 5 tables", "summary": "Omics data, such as transcriptomics, proteomics, and metabolomics, provide critical insights into disease mechanisms and clinical outcomes. However, their high dimensionality, small sample sizes, and intricate biological networks pose major challenges for reliable prediction and meaningful interpretation. Graph Neural Networks (GNNs) offer a promising way to integrate prior knowledge by encoding feature relationships as graphs. Yet, existing methods typically rely solely on either an externally curated feature graph or a data-driven generated one, which limits their ability to capture complementary information. To address this, we propose the external and generated Graph Neural Network (engGNN), a dual-graph framework that jointly leverages both external known biological networks and data-driven generated graphs. Specifically, engGNN constructs a biologically informed undirected feature graph from established network databases and complements it with a directed feature graph derived from tree-ensemble models. This dual-graph design produces more comprehensive embeddings, thereby improving predictive performance and interpretability. Through extensive simulations and real-world applications to gene expression data, engGNN consistently outperforms state-of-the-art baselines. Beyond classification, engGNN provides interpretable feature importance scores that facilitate biologically meaningful discoveries, such as pathway enrichment analysis. Taken together, these results highlight engGNN as a robust, flexible, and interpretable framework for disease classification and biomarker discovery in high-dimensional omics contexts."}
{"id": "2601.14653", "pdf": "https://arxiv.org/pdf/2601.14653", "abs": "https://arxiv.org/abs/2601.14653", "authors": ["Yuyu Liu", "Jiannan Yang", "Ziyang Yu", "Weishen Pan", "Fei Wang", "Tengfei Ma"], "title": "Efficient Imputation for Patch-based Missing Single-cell Data via Cluster-regularized Optimal Transport", "categories": ["cs.LG", "q-bio.GN"], "comment": null, "summary": "Missing data in single-cell sequencing datasets poses significant challenges for extracting meaningful biological insights. However, existing imputation approaches, which often assume uniformity and data completeness, struggle to address cases with large patches of missing data. In this paper, we present CROT, an optimal transport-based imputation algorithm designed to handle patch-based missing data in tabular formats. Our approach effectively captures the underlying data structure in the presence of significant missingness. Notably, it achieves superior imputation accuracy while significantly reducing runtime, demonstrating its scalability and efficiency for large-scale datasets. This work introduces a robust solution for imputation in heterogeneous, high-dimensional datasets with structured data absence, addressing critical challenges in both biological and clinical data analysis. Our code is available at Anomalous Github."}
{"id": "2601.14314", "pdf": "https://arxiv.org/pdf/2601.14314", "abs": "https://arxiv.org/abs/2601.14314", "authors": ["Santiago Silva", "Ghiles Reguig", "Neil P Oxtoby", "Andre Altmann", "Marco Lorenzi"], "title": "Fed-ComBat: A Generalized Federated Framework for Batch Effect Harmonization in Collaborative Studies", "categories": ["q-bio.QM"], "comment": null, "summary": "The use of multi-centric analyses is crucial for obtaining sufficient sample sizes and representative clinical populations in experimental studies. In this setting, data harmonization techniques are typically employed to address systematic biases and ensure the interoperability of the data. State-of-the-art harmonisation approaches are based on the statistical theory of random effect modeling, allowing to account for either linear of non-linear biases and batch effects. However, optimizing these statistical methods generally requires data centralization at some point during the analysis pipeline, therefore introducing the risk of exposing individual patient information while posing significant data governance issues. To overcome this challenge, in this paper we present Fed-ComBat, a federated framework for batch effect harmonization on decentralized data. Fed-ComBat enables the preservation of nonlinear covariate effects without requiring centralization of data and without prior parametric hypothesis on the variables to account for. We demonstrate the effectiveness of Fed-ComBat against a comprehensive panel of existing approaches based on the state-of-the-art ComBat, along with distributed and nonlinear variants. Our experiments are based on extensive simulated data, and on the analysis of multiple cohorts based on 7 neuroimaging studies comprising healthy controls (CI) and subjects with various disorders such as Parkinson's disease (PD), Alzheimer's disease (AD), and autism spectrum disorder (ASD). Our results show that in a federated settings, Fed-ComBat harmonization exhibits comparable results to centralized methods for both linear and nonlinear cases. On real data, harmonized trajectories of the thickness ofthe right hippocampus across lifespan measured on a set of 7 public studies show comparable results between centralized and federated models and are consistent with the literature when using a nonlinear model. The code is publicly available at: https://gitlab.inria.fr/greguig/fedcombat"}
{"id": "2601.14577", "pdf": "https://arxiv.org/pdf/2601.14577", "abs": "https://arxiv.org/abs/2601.14577", "authors": ["Ariel Bruner", "Mona Singh"], "title": "FBApro: A fast, simple linear transformation for diverse metabolic modeling tasks", "categories": ["q-bio.QM"], "comment": "19 pages, 9 figures", "summary": "Constraint-based metabolic modeling is the predominant framework for simulating cellular metabolism. The central assumption of these models is that metabolism operates at a steady state, meaning that the production and consumption rates of each metabolite are balanced. This assumption imposes linear constraints on the fluxes of biochemical reactions. Flux Balance Analysis (FBA), a fundamental method in the field, is formulated as an optimization problem maximizing a cellular objective (e.g., growth) over the resulting linear subspace of steady state fluxes. Many other methods in the field are expressed either as a modification to FBA, or use FBA as a black box within an algorithm. Here, we propose a simple and general alternative to optimization that, for any flux vector, finds the closest flux distribution within the steady-state subspace. This operation corresponds to an orthogonal projection that enforces the steady-state assumption. We further introduce extensions to handle cases involving unknown or fixed fluxes through modified projections and tailored affine subspaces. The overall approach is computationally efficient, does not require a cellular objective, and is easy to implement. We validate our method and its variants on both synthetic and experimental datasets, demonstrating their speed and utility for denoising and imputing metabolic flux data, and for predicting steady-state fluxes from more readily available types of data.\n  Code availability: The code implementing FBApro is available at https://github.com/Singh-Lab/FBApro. All code required to reproduce the figures in the paper is available, although the data used must be sourced separately. The repository also contains toy models and examples."}
{"id": "2601.15273", "pdf": "https://arxiv.org/pdf/2601.15273", "abs": "https://arxiv.org/abs/2601.15273", "authors": ["Paul Van Liedekerke", "Jiří Pešek", "Kevin Alessandri", "Dirk Drasdo"], "title": "How high-resolution agent-based models can improve fundamental insights in tissue development and cell culturing methods", "categories": ["q-bio.QM"], "comment": null, "summary": "The fundamental understanding of how cells physically interact with each other and their environment is key to understanding their organisation in living tissues. Over the past decades several computational methods have been developed to decipher emergent multi-cellular behaviors. In particular agent-based (or cell-based) models that consider the individual cell as basic modeling unit tracked in space and time enjoy increasing interest across scientific communities. In this article we explore a particular class of cell-based models, so-called Deformable Cell Models (DCMs), that allow to simulate the biophysics of the cell with high realism. After situating this model among other model types, We give an overview of past and recent DCM developments and discuss new simulation results of several applications covering in-vitro and in-vivo systems. Our goal is to demonstrate how such models can generate quantitative added value in biological and biotechnological problems."}
