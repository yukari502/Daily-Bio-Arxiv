{"id": "2602.04058", "pdf": "https://arxiv.org/pdf/2602.04058", "abs": "https://arxiv.org/abs/2602.04058", "authors": ["Guantong Qi", "Jiasheng Wang", "Mei Ling Chong", "Zahid Shaik", "Shenglan Li", "Shinya Yamamoto", "Undiagnosed Diseases Network", "Pengfei Liu", "Hu Chen", "Zhandong Liu"], "title": "RareCollab -- An Agentic System Diagnosing Mendelian Disorders with Integrated Phenotypic and Molecular Evidence", "categories": ["q-bio.GN"], "comment": null, "summary": "Millions of children worldwide are affected by severe rare Mendelian disorders, yet exome and genome sequencing still fail to provide a definitive molecular diagnosis for a large fraction of patients, prolonging the diagnostic odyssey. Bridging this gap increasingly requires transitioning from DNA-only interpretation to multi-modal diagnostic reasoning that combines genomic data, transcriptomic sequencing (RNA-seq), and phenotype information; however, computational frameworks that coherently integrate these signals remain limited. Here we present RareCollab, an agentic diagnostic framework that pairs a stable quantitative Diagnostic Engine with Large Language Model (LLM)-based specialist modules that produce high-resolution, interpretable assessments from transcriptomic signals, phenotypes, variant databases, and the literature to prioritize potential diagnostic variants. In a rigorously curated benchmark of Undiagnosed Diseases Network (UDN) patients with paired genomic and transcriptomic data, RareCollab achieved 77% top-5 diagnostic accuracy and improved top-1 to top-5 accuracy by ~20% over widely used variant-prioritization approaches. RareCollab illustrates how modular artificial intelligence (AI) can operationalize multi-modal evidence for accurate, scalable rare disease diagnosis, offering a promising path toward reducing the diagnostic odyssey for affected families.", "AI": {"tldr": "RareCollab\u662f\u4e00\u4e2a\u57fa\u4e8eAI\u7684\u591a\u6a21\u6001\u7f55\u89c1\u75c5\u8bca\u65ad\u6846\u67b6\uff0c\u7ed3\u5408\u57fa\u56e0\u7ec4\u3001\u8f6c\u5f55\u7ec4\u548c\u8868\u578b\u6570\u636e\uff0c\u901a\u8fc7LLM\u4e13\u5bb6\u6a21\u5757\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5728UDN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523077%\u7684top-5\u8bca\u65ad\u51c6\u786e\u7387\u3002", "motivation": "\u76ee\u524d\u5168\u7403\u6570\u767e\u4e07\u513f\u7ae5\u60a3\u6709\u7f55\u89c1\u5b5f\u5fb7\u5c14\u75be\u75c5\uff0c\u4f46\u5916\u663e\u5b50\u7ec4\u548c\u57fa\u56e0\u7ec4\u6d4b\u5e8f\u4ecd\u65e0\u6cd5\u4e3a\u5927\u91cf\u60a3\u8005\u63d0\u4f9b\u660e\u786e\u5206\u5b50\u8bca\u65ad\uff0c\u5ef6\u957f\u4e86\u8bca\u65ad\u5386\u7a0b\u3002\u9700\u8981\u4ece\u4ec5\u57fa\u4e8eDNA\u7684\u89e3\u91ca\u8f6c\u5411\u7ed3\u5408\u57fa\u56e0\u7ec4\u3001\u8f6c\u5f55\u7ec4\u548c\u8868\u578b\u4fe1\u606f\u7684\u591a\u6a21\u6001\u8bca\u65ad\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u6846\u67b6\u6709\u9650\u3002", "method": "\u63d0\u51faRareCollab\u6846\u67b6\uff1a\u5305\u542b\u7a33\u5b9a\u7684\u5b9a\u91cf\u8bca\u65ad\u5f15\u64ce\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e13\u5bb6\u6a21\u5757\u3002\u8fd9\u4e9b\u6a21\u5757\u4ece\u8f6c\u5f55\u7ec4\u4fe1\u53f7\u3001\u8868\u578b\u3001\u53d8\u5f02\u6570\u636e\u5e93\u548c\u6587\u732e\u4e2d\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u3001\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\uff0c\u4ee5\u4f18\u5148\u8003\u8651\u6f5c\u5728\u8bca\u65ad\u53d8\u5f02\u3002", "result": "\u5728\u672a\u8bca\u65ad\u75be\u75c5\u7f51\u7edc\uff08UDN\uff09\u60a3\u8005\u57fa\u56e0\u7ec4\u548c\u8f6c\u5f55\u7ec4\u914d\u5bf9\u6570\u636e\u7684\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cRareCollab\u5b9e\u73b0\u4e8677%\u7684top-5\u8bca\u65ad\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684\u53d8\u5f02\u4f18\u5148\u6392\u5e8f\u65b9\u6cd5\uff0ctop-1\u5230top-5\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u7ea620%\u3002", "conclusion": "RareCollab\u5c55\u793a\u4e86\u6a21\u5757\u5316\u4eba\u5de5\u667a\u80fd\u5982\u4f55\u64cd\u4f5c\u591a\u6a21\u6001\u8bc1\u636e\u8fdb\u884c\u51c6\u786e\u3001\u53ef\u6269\u5c55\u7684\u7f55\u89c1\u75c5\u8bca\u65ad\uff0c\u4e3a\u51cf\u5c11\u53d7\u5f71\u54cd\u5bb6\u5ead\u7684\u8bca\u65ad\u5386\u7a0b\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002"}}
{"id": "2602.03886", "pdf": "https://arxiv.org/pdf/2602.03886", "abs": "https://arxiv.org/abs/2602.03886", "authors": ["Martin G. Frasch", "Marlene J. E. Mayer", "Clara Becker", "Peter Zimmermann", "Camilla Zelgert", "Marta C. Antonelli", "Silvia M. Lobmaier"], "title": "Prenatal Stress Detection from Electrocardiography Using Self-Supervised Deep Learning: Development and External Validation", "categories": ["q-bio.QM", "cs.LG"], "comment": "22 pages, 5 figures", "summary": "Prenatal psychological stress affects 15-25% of pregnancies and increases risks of preterm birth, low birth weight, and adverse neurodevelopmental outcomes. Current screening relies on subjective questionnaires (PSS-10), limiting continuous monitoring. We developed deep learning models for stress detection from electrocardiography (ECG) using the FELICITy 1 cohort (151 pregnant women, 32-38 weeks gestation). A ResNet-34 encoder was pretrained via SimCLR contrastive learning on 40,692 ECG segments per subject. Multi-layer feature extraction enabled binary classification and continuous PSS prediction across maternal (mECG), fetal (fECG), and abdominal ECG (aECG). External validation used the FELICITy 2 RCT (28 subjects, different ECG device, yoga intervention vs. control). On FELICITy 1 (5-fold CV): mECG 98.6% accuracy (R2=0.88, MAE=1.90), fECG 99.8% (R2=0.95, MAE=1.19), aECG 95.5% (R2=0.75, MAE=2.80). External validation on FELICITy 2: mECG 77.3% accuracy (R2=0.62, MAE=3.54, AUC=0.826), aECG 63.6% (R2=0.29, AUC=0.705). Signal quality-based channel selection outperformed all-channel averaging (+12% R2 improvement). Mixed-effects models detected a significant intervention response (p=0.041). Self-supervised deep learning on pregnancy ECG enables accurate, objective stress assessment, with multi-layer feature extraction substantially outperforming single embedding approaches.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u5fc3\u7535\u56fe\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u68c0\u6d4b\u5b55\u671f\u5fc3\u7406\u538b\u529b\uff0c\u5728\u5185\u90e8\u9a8c\u8bc1\u4e2d\u8fbe\u523099.8%\u51c6\u786e\u7387\uff0c\u5916\u90e8\u9a8c\u8bc1\u4e2d77.3%\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u95ee\u5377\u65b9\u6cd5\u3002", "motivation": "\u5b55\u671f\u5fc3\u7406\u538b\u529b\u5f71\u54cd15-25%\u7684\u5b55\u5987\uff0c\u589e\u52a0\u65e9\u4ea7\u3001\u4f4e\u51fa\u751f\u4f53\u91cd\u548c\u795e\u7ecf\u53d1\u80b2\u4e0d\u826f\u98ce\u9669\u3002\u76ee\u524d\u4f9d\u8d56\u4e3b\u89c2\u95ee\u5377\uff08PSS-10\uff09\u7b5b\u67e5\uff0c\u7f3a\u4e4f\u8fde\u7eed\u76d1\u6d4b\u80fd\u529b\uff0c\u9700\u8981\u5ba2\u89c2\u3001\u8fde\u7eed\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528FELICITy 1\u961f\u5217\uff08151\u540d\u5b55\u5987\uff0c32-38\u5468\uff09\u7684\u5fc3\u7535\u56fe\u6570\u636e\uff0c\u901a\u8fc7SimCLR\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3ResNet-34\u7f16\u7801\u5668\uff0c\u5bf9\u6bcf\u4f4d\u53d7\u8bd5\u8005\u768440,692\u4e2aECG\u7247\u6bb5\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u3002\u91c7\u7528\u591a\u5c42\u7279\u5f81\u63d0\u53d6\u5b9e\u73b0\u4e8c\u5143\u5206\u7c7b\u548c\u8fde\u7eedPSS\u9884\u6d4b\uff0c\u8986\u76d6\u6bcd\u4f53ECG\u3001\u80ce\u513fECG\u548c\u8179\u90e8ECG\u3002\u5916\u90e8\u9a8c\u8bc1\u4f7f\u7528FELICITy 2 RCT\uff0828\u540d\u53d7\u8bd5\u8005\uff0c\u4e0d\u540cECG\u8bbe\u5907\uff0c\u745c\u4f3d\u5e72\u9884vs\u5bf9\u7167\u7ec4\uff09\u3002", "result": "FELICITy 1\uff085\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff09\uff1a\u6bcd\u4f53ECG 98.6%\u51c6\u786e\u7387\uff08R2=0.88\uff0cMAE=1.90\uff09\uff0c\u80ce\u513fECG 99.8%\uff08R2=0.95\uff0cMAE=1.19\uff09\uff0c\u8179\u90e8ECG 95.5%\uff08R2=0.75\uff0cMAE=2.80\uff09\u3002\u5916\u90e8\u9a8c\u8bc1\uff1a\u6bcd\u4f53ECG 77.3%\u51c6\u786e\u7387\uff08R2=0.62\uff0cMAE=3.54\uff0cAUC=0.826\uff09\uff0c\u8179\u90e8ECG 63.6%\uff08R2=0.29\uff0cAUC=0.705\uff09\u3002\u57fa\u4e8e\u4fe1\u53f7\u8d28\u91cf\u7684\u901a\u9053\u9009\u62e9\u4f18\u4e8e\u5168\u901a\u9053\u5e73\u5747\uff08R2\u63d0\u534712%\uff09\u3002\u6df7\u5408\u6548\u5e94\u6a21\u578b\u68c0\u6d4b\u5230\u663e\u8457\u7684\u5e72\u9884\u54cd\u5e94\uff08p=0.041\uff09\u3002", "conclusion": "\u57fa\u4e8e\u5fc3\u7535\u56fe\u7684\u81ea\u6211\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u3001\u5ba2\u89c2\u7684\u5b55\u671f\u538b\u529b\u8bc4\u4f30\uff0c\u591a\u5c42\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u5d4c\u5165\u65b9\u6cd5\uff0c\u4e3a\u8fde\u7eed\u76d1\u6d4b\u5b55\u671f\u5fc3\u7406\u538b\u529b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2602.03902", "pdf": "https://arxiv.org/pdf/2602.03902", "abs": "https://arxiv.org/abs/2602.03902", "authors": ["Jiying Zhang", "Shuhao Zhang", "Pierre Vandergheynst", "Patrick Barth"], "title": "All-Atom GPCR-Ligand Simulation via Residual Isometric Latent Flow", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "36 pages", "summary": "G-protein-coupled receptors (GPCRs), primary targets for over one-third of approved therapeutics, rely on intricate conformational transitions to transduce signals. While Molecular Dynamics (MD) is essential for elucidating this transduction process, particularly within ligand-bound complexes, conventional all-atom MD simulation is computationally prohibitive. In this paper, we introduce GPCRLMD, a deep generative framework for efficient all-atom GPCR-ligand simulation.GPCRLMD employs a Harmonic-Prior Variational Autoencoder (HP-VAE) to first map the complex into a regularized isometric latent space, preserving geometric topology via physics-informed constraints. Within this latent space, a Residual Latent Flow samples evolution trajectories, which are subsequently decoded back to atomic coordinates. By capturing temporal dynamics via relative displacements anchored to the initial structure, this residual mechanism effectively decouples static topology from dynamic fluctuations. Experimental results demonstrate that GPCRLMD achieves state-of-the-art performance in GPCR-ligand dynamics simulation, faithfully reproducing thermodynamic observables and critical ligand-receptor interactions.", "AI": {"tldr": "GPCRLMD\uff1a\u57fa\u4e8e\u6df1\u5ea6\u751f\u6210\u6846\u67b6\u7684\u9ad8\u6548GPCR-\u914d\u4f53\u5168\u539f\u5b50\u6a21\u62df\u65b9\u6cd5\uff0c\u901a\u8fc7HP-VAE\u548c\u6b8b\u5dee\u6f5c\u5728\u6d41\u5b9e\u73b0\u8ba1\u7b97\u9ad8\u6548\u7684\u6784\u8c61\u52a8\u529b\u5b66\u6a21\u62df", "motivation": "GPCR\u662f\u4e09\u5206\u4e4b\u4e00\u4ee5\u4e0a\u5df2\u6279\u51c6\u836f\u7269\u7684\u4e3b\u8981\u9776\u70b9\uff0c\u5176\u4fe1\u53f7\u8f6c\u5bfc\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u6784\u8c61\u8f6c\u53d8\u3002\u4f20\u7edf\u5168\u539f\u5b50\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86GPCR-\u914d\u4f53\u590d\u5408\u7269\u52a8\u529b\u5b66\u7684\u7814\u7a76\u3002", "method": "\u63d0\u51faGPCRLMD\u6846\u67b6\uff1a1) \u4f7f\u7528\u8c10\u6ce2\u5148\u9a8c\u53d8\u5206\u81ea\u7f16\u7801\u5668(HP-VAE)\u5c06\u590d\u5408\u7269\u6620\u5c04\u5230\u6b63\u5219\u5316\u7b49\u8ddd\u6f5c\u5728\u7a7a\u95f4\uff0c\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u7ea6\u675f\u4fdd\u6301\u51e0\u4f55\u62d3\u6251\uff1b2) \u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f7f\u7528\u6b8b\u5dee\u6f5c\u5728\u6d41\u91c7\u6837\u6f14\u5316\u8f68\u8ff9\uff1b3) \u901a\u8fc7\u76f8\u5bf9\u4f4d\u79fb\u673a\u5236\u5c06\u9759\u6001\u62d3\u6251\u4e0e\u52a8\u6001\u6da8\u843d\u89e3\u8026\uff0c\u6700\u540e\u89e3\u7801\u56de\u539f\u5b50\u5750\u6807\u3002", "result": "GPCRLMD\u5728GPCR-\u914d\u4f53\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u80fd\u591f\u51c6\u786e\u590d\u73b0\u70ed\u529b\u5b66\u53ef\u89c2\u6d4b\u91cf\u548c\u5173\u952e\u7684\u914d\u4f53-\u53d7\u4f53\u76f8\u4e92\u4f5c\u7528\u3002", "conclusion": "GPCRLMD\u4e3a\u9ad8\u6548\u7684\u5168\u539f\u5b50GPCR-\u914d\u4f53\u6a21\u62df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6df1\u5ea6\u751f\u6210\u6846\u67b6\uff0c\u514b\u670d\u4e86\u4f20\u7edfMD\u6a21\u62df\u7684\u8ba1\u7b97\u9650\u5236\uff0c\u6709\u671b\u4fc3\u8fdbGPCR\u4fe1\u53f7\u8f6c\u5bfc\u673a\u5236\u7684\u7814\u7a76\u548c\u836f\u7269\u53d1\u73b0\u3002"}}
{"id": "2602.03875", "pdf": "https://arxiv.org/pdf/2602.03875", "abs": "https://arxiv.org/abs/2602.03875", "authors": ["Stefan Kuhn", "Vandana Dwarka", "Przemyslaw Karol Grenda", "Eero Vainikko"], "title": "Reversible Deep Learning for 13C NMR in Chemoinformatics: On Structures and Spectra", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "10 pages, 4 figures, 4 tables", "summary": "We introduce a reversible deep learning model for 13C NMR that uses a single conditional invertible neural network for both directions between molecular structures and spectra. The network is built from i-RevNet style bijective blocks, so the forward map and its inverse are available by construction. We train the model to predict a 128-bit binned spectrum code from a graph-based structure encoding, while the remaining latent dimensions capture residual variability. At inference time, we invert the same trained network to generate structure candidates from a spectrum code, which explicitly represents the one-to-many nature of spectrum-to-structure inference. On a filtered subset, the model is numerically invertible on trained examples, achieves spectrum-code prediction above chance, and produces coarse but meaningful structural signals when inverted on validation spectra. These results demonstrate that invertible architectures can unify spectrum prediction and uncertainty-aware candidate generation within one end-to-end model.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u9006\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u5355\u4e00\u6761\u4ef6\u53ef\u9006\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u5206\u5b50\u7ed3\u6784\u4e0e13C NMR\u8c31\u4e4b\u95f4\u7684\u53cc\u5411\u9884\u6d4b", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5206\u522b\u8bad\u7ec3\u6b63\u5411\uff08\u7ed3\u6784\u5230\u8c31\uff09\u548c\u9006\u5411\uff08\u8c31\u5230\u7ed3\u6784\uff09\u6a21\u578b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u6765\u5904\u7406\u8c31\u5230\u7ed3\u6784\u63a8\u65ad\u4e2d\u7684\u4e00\u5bf9\u591a\u4e0d\u786e\u5b9a\u6027", "method": "\u57fa\u4e8ei-RevNet\u98ce\u683c\u7684\u53cc\u5c04\u5757\u6784\u5efa\u6761\u4ef6\u53ef\u9006\u795e\u7ecf\u7f51\u7edc\uff0c\u8bad\u7ec3\u7f51\u7edc\u4ece\u56fe\u7ed3\u6784\u7f16\u7801\u9884\u6d4b128\u4f4d\u5206\u7bb1\u8c31\u7801\uff0c\u5269\u4f59\u6f5c\u5728\u7ef4\u5ea6\u6355\u6349\u6b8b\u5dee\u53d8\u5f02\u6027", "result": "\u6a21\u578b\u5728\u8bad\u7ec3\u793a\u4f8b\u4e0a\u6570\u503c\u53ef\u9006\uff0c\u8c31\u7801\u9884\u6d4b\u4f18\u4e8e\u968f\u673a\uff0c\u5728\u9a8c\u8bc1\u8c31\u4e0a\u9006\u5411\u751f\u6210\u5177\u6709\u610f\u4e49\u7684\u7ed3\u6784\u4fe1\u53f7\uff0c\u80fd\u663e\u5f0f\u8868\u793a\u8c31\u5230\u7ed3\u6784\u7684\u4e00\u5bf9\u591a\u5173\u7cfb", "conclusion": "\u53ef\u9006\u67b6\u6784\u80fd\u5728\u5355\u4e00\u7aef\u5230\u7aef\u6a21\u578b\u4e2d\u7edf\u4e00\u8c31\u9884\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5019\u9009\u7ed3\u6784\u751f\u6210"}}
{"id": "2602.03998", "pdf": "https://arxiv.org/pdf/2602.03998", "abs": "https://arxiv.org/abs/2602.03998", "authors": ["Ahmed Alagha", "Christopher Leclerc", "Yousef Kotp", "Omar Metwally", "Calvin Moras", "Peter Rentopoulos", "Ghodsiyeh Rostami", "Bich Ngoc Nguyen", "Jumanah Baig", "Abdelhakim Khellaf", "Vincent Quoc-Huy Trinh", "Rabeb Mizouni", "Hadi Otrok", "Jamal Bentahar", "Mahdi S. Hosseini"], "title": "AtlasPatch: An Efficient and Scalable Tool for Whole Slide Image Preprocessing in Computational Pathology", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "comment": "Under review", "summary": "Whole-slide image (WSI) preprocessing, typically comprising tissue detection followed by patch extraction, is foundational to AI-driven computational pathology workflows. This remains a major computational bottleneck as existing tools either rely on inaccurate heuristic thresholding for tissue detection, or adopt AI-based approaches trained on limited-diversity data that operate at the patch level, incurring substantial computational complexity. We present AtlasPatch, an efficient and scalable slide preprocessing framework for accurate tissue detection and high-throughput patch extraction with minimal computational overhead. AtlasPatch's tissue detection module is trained on a heterogeneous and semi-manually annotated dataset of ~30,000 WSI thumbnails, using efficient fine-tuning of the Segment-Anything model. The tool extrapolates tissue masks from thumbnails to full-resolution slides to extract patch coordinates at user-specified magnifications, with options to stream patches directly into common image encoders for embedding or store patch images, all efficiently parallelized across CPUs and GPUs. We assess AtlasPatch across segmentation precision, computational complexity, and downstream multiple-instance learning, matching state-of-the-art performance while operating at a fraction of their computational cost. AtlasPatch is open-source and available at https://github.com/AtlasAnalyticsLab/AtlasPatch.", "AI": {"tldr": "AtlasPatch\u662f\u4e00\u4e2a\u9ad8\u6548\u53ef\u6269\u5c55\u7684WSI\u9884\u5904\u7406\u6846\u67b6\uff0c\u901a\u8fc7SAM\u6a21\u578b\u5fae\u8c03\u5b9e\u73b0\u51c6\u786e\u7ec4\u7ec7\u68c0\u6d4b\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u73b0\u6709WSI\u9884\u5904\u7406\u5de5\u5177\u5b58\u5728\u4e24\u5927\u95ee\u9898\uff1a\u8981\u4e48\u4f9d\u8d56\u4e0d\u51c6\u786e\u7684\u542f\u53d1\u5f0f\u9608\u503c\u8fdb\u884c\u7ec4\u7ec7\u68c0\u6d4b\uff0c\u8981\u4e48\u91c7\u7528\u57fa\u4e8eAI\u7684\u65b9\u6cd5\u4f46\u8bad\u7ec3\u6570\u636e\u591a\u6837\u6027\u6709\u9650\u4e14\u5728\u8865\u4e01\u7ea7\u522b\u64cd\u4f5c\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8", "method": "1) \u5728\u7ea630,000\u4e2aWSI\u7f29\u7565\u56fe\u7684\u5f02\u6784\u534a\u624b\u52a8\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7ec4\u7ec7\u68c0\u6d4b\u6a21\u5757\uff0c\u4f7f\u7528Segment-Anything\u6a21\u578b\u7684\u9ad8\u6548\u5fae\u8c03\uff1b2) \u4ece\u7f29\u7565\u56fe\u5916\u63a8\u7ec4\u7ec7\u63a9\u7801\u5230\u5168\u5206\u8fa8\u7387\u5207\u7247\uff1b3) \u5728\u7528\u6237\u6307\u5b9a\u653e\u5927\u500d\u6570\u4e0b\u63d0\u53d6\u8865\u4e01\u5750\u6807\uff1b4) \u652f\u6301\u5c06\u8865\u4e01\u76f4\u63a5\u6d41\u5f0f\u4f20\u8f93\u5230\u5e38\u89c1\u56fe\u50cf\u7f16\u7801\u5668\u6216\u5b58\u50a8\u8865\u4e01\u56fe\u50cf\uff1b5) \u5728CPU\u548cGPU\u4e0a\u9ad8\u6548\u5e76\u884c\u5316", "result": "\u5728\u5206\u5272\u7cbe\u5ea6\u3001\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u4e0b\u6e38\u591a\u5b9e\u4f8b\u5b66\u4e60\u65b9\u9762\u8fdb\u884c\u8bc4\u4f30\uff0c\u5339\u914d\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u4ec5\u9700\u5176\u8ba1\u7b97\u6210\u672c\u7684\u4e00\u5c0f\u90e8\u5206", "conclusion": "AtlasPatch\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u9ad8\u6548\u53ef\u6269\u5c55WSI\u9884\u5904\u7406\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u75c5\u7406\u5b66\u4e2d\u7684\u4e3b\u8981\u8ba1\u7b97\u74f6\u9888\uff0c\u63d0\u4f9b\u51c6\u786e\u7ec4\u7ec7\u68c0\u6d4b\u548c\u9ad8\u901a\u91cf\u8865\u4e01\u63d0\u53d6"}}
{"id": "2602.04021", "pdf": "https://arxiv.org/pdf/2602.04021", "abs": "https://arxiv.org/abs/2602.04021", "authors": ["Aditya Gorla", "Hugues Van Assel", "Jan-Christian Huetter", "Heming Yao", "Kyunghyun Cho", "Aviv Regev", "Russell Littman"], "title": "Group Contrastive Learning for Weakly Paired Multimodal Data", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "comment": null, "summary": "We present GROOVE, a semi-supervised multi-modal representation learning approach for high-content perturbation data where samples across modalities are weakly paired through shared perturbation labels but lack direct correspondence. Our primary contribution is GroupCLIP, a novel group-level contrastive loss that bridges the gap between CLIP for paired cross-modal data and SupCon for uni-modal supervised contrastive learning, addressing a fundamental gap in contrastive learning for weakly-paired settings. We integrate GroupCLIP with an on-the-fly backtranslating autoencoder framework to encourage cross-modally entangled representations while maintaining group-level coherence within a shared latent space. Critically, we introduce a comprehensive combinatorial evaluation framework that systematically assesses representation learners across multiple optimal transport aligners, addressing key limitations in existing evaluation strategies. This framework includes novel simulations that systematically vary shared versus modality-specific perturbation effects enabling principled assessment of method robustness. Our combinatorial benchmarking reveals that there is not yet an aligner that uniformly dominates across settings or modality pairs. Across simulations and two real single-cell genetic perturbation datasets, GROOVE performs on par with or outperforms existing approaches for downstream cross-modal matching and imputation tasks. Our ablation studies demonstrate that GroupCLIP is the key component driving performance gains. These results highlight the importance of leveraging group-level constraints for effective multi-modal representation learning in scenarios where only weak pairing is available.", "AI": {"tldr": "GROOVE\u662f\u4e00\u79cd\u534a\u76d1\u7763\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u9488\u5bf9\u5f31\u914d\u5bf9\u7684\u9ad8\u901a\u91cf\u6270\u52a8\u6570\u636e\uff0c\u901a\u8fc7GroupCLIP\u635f\u5931\u51fd\u6570\u548c\u52a8\u6001\u53cd\u5411\u7ffb\u8bd1\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u7ea0\u7f20\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u901a\u91cf\u6270\u52a8\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u8fd9\u4e9b\u6570\u636e\u4e2d\u4e0d\u540c\u6a21\u6001\u7684\u6837\u672c\u4ec5\u901a\u8fc7\u5171\u4eab\u7684\u6270\u52a8\u6807\u7b7e\u5f31\u914d\u5bf9\uff0c\u7f3a\u4e4f\u76f4\u63a5\u5bf9\u5e94\u5173\u7cfb\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5229\u7528\u8fd9\u79cd\u5f31\u914d\u5bf9\u4fe1\u606f\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51faGroupCLIP\u635f\u5931\u51fd\u6570\uff0c\u6865\u63a5CLIP\uff08\u914d\u5bf9\u8de8\u6a21\u6001\uff09\u548cSupCon\uff08\u5355\u6a21\u6001\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\uff09\u7684\u5dee\u8ddd\uff1b\u7ed3\u5408\u52a8\u6001\u53cd\u5411\u7ffb\u8bd1\u81ea\u7f16\u7801\u5668\u6846\u67b6\uff0c\u5728\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4fdd\u6301\u7ec4\u7ea7\u4e00\u81f4\u6027\uff1b\u5f15\u5165\u7ec4\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u6700\u4f18\u4f20\u8f93\u5bf9\u9f50\u5668\u7684\u6027\u80fd\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u548c\u4e24\u4e2a\u771f\u5b9e\u5355\u7ec6\u80de\u9057\u4f20\u6270\u52a8\u6570\u636e\u96c6\u4e0a\uff0cGROOVE\u5728\u8de8\u6a21\u6001\u5339\u914d\u548c\u63d2\u8865\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff1b\u7ec4\u5408\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5c1a\u65e0\u5bf9\u9f50\u5668\u5728\u6240\u6709\u8bbe\u7f6e\u6216\u6a21\u6001\u5bf9\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff1b\u6d88\u878d\u7814\u7a76\u8868\u660eGroupCLIP\u662f\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u5728\u5f31\u914d\u5bf9\u573a\u666f\u4e2d\uff0c\u5229\u7528\u7ec4\u7ea7\u7ea6\u675f\u5bf9\u4e8e\u6709\u6548\u7684\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff1bGroupCLIP\u6210\u529f\u89e3\u51b3\u4e86\u5f31\u914d\u5bf9\u5bf9\u6bd4\u5b66\u4e60\u7684\u57fa\u672c\u5dee\u8ddd\uff1b\u63d0\u51fa\u7684\u7ec4\u5408\u8bc4\u4f30\u6846\u67b6\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u5de5\u5177\u3002"}}
{"id": "2602.04119", "pdf": "https://arxiv.org/pdf/2602.04119", "abs": "https://arxiv.org/abs/2602.04119", "authors": ["Hyeonah Kim", "Minsu Kim", "Celine Roget", "Dionessa Biton", "Louis Vaillancourt", "Yves V. Brun", "Yoshua Bengio", "Alex Hernandez-Garcia"], "title": "Synthesizable Molecular Generation via Soft-constrained GFlowNets with Rich Chemical Priors", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "The application of generative models for experimental drug discovery campaigns is severely limited by the difficulty of designing molecules de novo that can be synthesized in practice. Previous works have leveraged Generative Flow Networks (GFlowNets) to impose hard synthesizability constraints through the design of state and action spaces based on predefined reaction templates and building blocks. Despite the promising prospects of this approach, it currently lacks flexibility and scalability. As an alternative, we propose S3-GFN, which generates synthesizable SMILES molecules via simple soft regularization of a sequence-based GFlowNet. Our approach leverages rich molecular priors learned from large-scale SMILES corpora to steer molecular generation towards high-reward, synthesizable chemical spaces. The model induces constraints through off-policy replay training with a contrastive learning signal based on separate buffers of synthesizable and unsynthesizable samples. Our experiments show that S3-GFN learns to generate synthesizable molecules ($\\geq 95\\%$) with higher rewards in diverse tasks.", "AI": {"tldr": "S3-GFN\uff1a\u901a\u8fc7\u8f6f\u6b63\u5219\u5316\u5e8f\u5217GFlowNet\u751f\u6210\u53ef\u5408\u6210SMILES\u5206\u5b50\uff0c\u907f\u514d\u786c\u7ea6\u675f\u9650\u5236", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53cd\u5e94\u6a21\u677f\u548c\u6784\u5efa\u5757\u7684\u786c\u7ea6\u675f\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9650\u5236\u4e86\u751f\u6210\u6a21\u578b\u5728\u5b9e\u9a8c\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5e94\u7528", "method": "\u63d0\u51faS3-GFN\uff0c\u901a\u8fc7\u7b80\u5355\u8f6f\u6b63\u5219\u5316\u5e8f\u5217GFlowNet\u751f\u6210\u53ef\u5408\u6210SMILES\u5206\u5b50\uff0c\u5229\u7528\u5927\u89c4\u6a21SMILES\u8bed\u6599\u5e93\u5b66\u4e60\u5206\u5b50\u5148\u9a8c\uff0c\u901a\u8fc7\u57fa\u4e8e\u53ef\u5408\u6210/\u4e0d\u53ef\u5408\u6210\u6837\u672c\u7f13\u51b2\u533a\u7684\u5bf9\u6bd4\u5b66\u4e60\u4fe1\u53f7\u8fdb\u884c\u79bb\u7b56\u7565\u56de\u653e\u8bad\u7ec3", "result": "S3-GFN\u80fd\u591f\u751f\u6210\u9ad8\u6bd4\u4f8b\u53ef\u5408\u6210\u5206\u5b50\uff08\u226595%\uff09\u5e76\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u83b7\u5f97\u66f4\u9ad8\u5956\u52b1", "conclusion": "\u8f6f\u6b63\u5219\u5316\u65b9\u6cd5\u6bd4\u786c\u7ea6\u675f\u65b9\u6cd5\u66f4\u7075\u6d3b\u6709\u6548\uff0c\u80fd\u591f\u5f15\u5bfc\u5206\u5b50\u751f\u6210\u5230\u9ad8\u5956\u52b1\u3001\u53ef\u5408\u6210\u7684\u5316\u5b66\u7a7a\u95f4"}}
{"id": "2602.04270", "pdf": "https://arxiv.org/pdf/2602.04270", "abs": "https://arxiv.org/abs/2602.04270", "authors": ["Noga Mudrik", "Yuxi Chen", "Gal Mishne", "Adam S. Charles"], "title": "Multi-Integration of Labels across Categories for Component Identification (MILCCI)", "categories": ["cs.LG", "q-bio.NC", "q-bio.QM", "stat.ML"], "comment": null, "summary": "Many fields collect large-scale temporal data through repeated measurements (trials), where each trial is labeled with a set of metadata variables spanning several categories. For example, a trial in a neuroscience study may be linked to a value from category (a): task difficulty, and category (b): animal choice. A critical challenge in time-series analysis is to understand how these labels are encoded within the multi-trial observations, and disentangle the distinct effect of each label entry across categories. Here, we present MILCCI, a novel data-driven method that i) identifies the interpretable components underlying the data, ii) captures cross-trial variability, and iii) integrates label information to understand each category's representation within the data. MILCCI extends a sparse per-trial decomposition that leverages label similarities within each category to enable subtle, label-driven cross-trial adjustments in component compositions and to distinguish the contribution of each category. MILCCI also learns each component's corresponding temporal trace, which evolves over time within each trial and varies flexibly across trials. We demonstrate MILCCI's performance through both synthetic and real-world examples, including voting patterns, online page view trends, and neuronal recordings.", "AI": {"tldr": "MILCCI\u662f\u4e00\u79cd\u65b0\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u5e26\u6709\u591a\u7c7b\u522b\u5143\u6570\u636e\u6807\u7b7e\u7684\u91cd\u590d\u6d4b\u91cf\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u80fd\u591f\u8bc6\u522b\u53ef\u89e3\u91ca\u7684\u7ec4\u4ef6\u3001\u6355\u6349\u8de8\u8bd5\u9a8c\u53d8\u5f02\u6027\uff0c\u5e76\u6574\u5408\u6807\u7b7e\u4fe1\u606f\u6765\u7406\u89e3\u6bcf\u4e2a\u7c7b\u522b\u5728\u6570\u636e\u4e2d\u7684\u8868\u793a\u3002", "motivation": "\u8bb8\u591a\u9886\u57df\u901a\u8fc7\u91cd\u590d\u6d4b\u91cf\u6536\u96c6\u5927\u89c4\u6a21\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u6bcf\u4e2a\u8bd5\u9a8c\u90fd\u5e26\u6709\u8de8\u8d8a\u591a\u4e2a\u7c7b\u522b\u7684\u5143\u6570\u636e\u53d8\u91cf\u3002\u5173\u952e\u6311\u6218\u662f\u7406\u89e3\u8fd9\u4e9b\u6807\u7b7e\u5982\u4f55\u5728\u591a\u8bd5\u9a8c\u89c2\u6d4b\u4e2d\u88ab\u7f16\u7801\uff0c\u5e76\u533a\u5206\u6bcf\u4e2a\u6807\u7b7e\u6761\u76ee\u5728\u4e0d\u540c\u7c7b\u522b\u4e2d\u7684\u4e0d\u540c\u5f71\u54cd\u3002", "method": "MILCCI\u6269\u5c55\u4e86\u7a00\u758f\u7684\u6bcf\u8bd5\u9a8c\u5206\u89e3\u65b9\u6cd5\uff0c\u5229\u7528\u6bcf\u4e2a\u7c7b\u522b\u5185\u7684\u6807\u7b7e\u76f8\u4f3c\u6027\u6765\u5b9e\u73b0\u7ec6\u5fae\u7684\u3001\u6807\u7b7e\u9a71\u52a8\u7684\u8de8\u8bd5\u9a8c\u7ec4\u4ef6\u7ec4\u6210\u8c03\u6574\uff0c\u5e76\u533a\u5206\u6bcf\u4e2a\u7c7b\u522b\u7684\u8d21\u732e\u3002\u540c\u65f6\u5b66\u4e60\u6bcf\u4e2a\u7ec4\u4ef6\u5bf9\u5e94\u7684\u65f6\u95f4\u8f68\u8ff9\uff0c\u8fd9\u4e9b\u8f68\u8ff9\u5728\u6bcf\u4e2a\u8bd5\u9a8c\u5185\u968f\u65f6\u95f4\u6f14\u5316\u5e76\u5728\u4e0d\u540c\u8bd5\u9a8c\u95f4\u7075\u6d3b\u53d8\u5316\u3002", "result": "\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u793a\u4f8b\uff08\u5305\u62ec\u6295\u7968\u6a21\u5f0f\u3001\u5728\u7ebf\u9875\u9762\u6d4f\u89c8\u8d8b\u52bf\u548c\u795e\u7ecf\u5143\u8bb0\u5f55\uff09\u5c55\u793a\u4e86MILCCI\u7684\u6027\u80fd\u3002", "conclusion": "MILCCI\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5206\u6790\u5e26\u6709\u591a\u7c7b\u522b\u5143\u6570\u636e\u6807\u7b7e\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u80fd\u591f\u8bc6\u522b\u53ef\u89e3\u91ca\u7684\u7ec4\u4ef6\u5e76\u7406\u89e3\u4e0d\u540c\u7c7b\u522b\u6807\u7b7e\u5728\u6570\u636e\u4e2d\u7684\u8868\u793a\u65b9\u5f0f\u3002"}}
{"id": "2602.04883", "pdf": "https://arxiv.org/pdf/2602.04883", "abs": "https://arxiv.org/abs/2602.04883", "authors": ["Yanru Qu", "Cheng-Yen Hsieh", "Zaixiang Zheng", "Ge Liu", "Quanquan Gu"], "title": "Protein Autoregressive Modeling via Multiscale Structure Generation", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.QM"], "comment": "ByteDance Seed Tech Report; Page: https://par-protein.github.io/", "summary": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.", "AI": {"tldr": "PAR\u662f\u9996\u4e2a\u591a\u5c3a\u5ea6\u81ea\u56de\u5f52\u86cb\u767d\u8d28\u9aa8\u67b6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u7c97\u5230\u7ec6\u7684\u9010\u5c3a\u5ea6\u9884\u6d4b\u751f\u6210\u86cb\u767d\u8d28\u7ed3\u6784\uff0c\u91c7\u7528\u566a\u58f0\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u8c03\u5ea6\u91c7\u6837\u7f13\u89e3\u66b4\u9732\u504f\u5dee\uff0c\u5728\u65e0\u6761\u4ef6\u751f\u6210\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5229\u7528\u86cb\u767d\u8d28\u7684\u5c42\u6b21\u6027\u672c\u8d28\uff0c\u6a21\u4eff\u96d5\u5851\u8fc7\u7a0b\u4ece\u7c97\u5230\u7ec6\u751f\u6210\u86cb\u767d\u8d28\u7ed3\u6784\uff0c\u89e3\u51b3\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u66b4\u9732\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u86cb\u767d\u8d28\u9aa8\u67b6\u751f\u6210\u3002", "method": "1) \u591a\u5c3a\u5ea6\u4e0b\u91c7\u6837\u64cd\u4f5c\u8868\u793a\u4e0d\u540c\u5c3a\u5ea6\u7684\u86cb\u767d\u8d28\u7ed3\u6784\uff1b2) \u81ea\u56de\u5f52Transformer\u7f16\u7801\u591a\u5c3a\u5ea6\u4fe1\u606f\u5e76\u4ea7\u751f\u6761\u4ef6\u5d4c\u5165\uff1b3) \u57fa\u4e8e\u6d41\u7684\u9aa8\u67b6\u89e3\u7801\u5668\u6839\u636e\u5d4c\u5165\u751f\u6210\u9aa8\u67b6\u539f\u5b50\uff1b4) \u91c7\u7528\u566a\u58f0\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u8c03\u5ea6\u91c7\u6837\u7f13\u89e3\u66b4\u9732\u504f\u5dee\u3002", "result": "PAR\u6709\u6548\u5b66\u4e60\u86cb\u767d\u8d28\u5206\u5e03\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u9aa8\u67b6\u7ed3\u6784\uff0c\u8868\u73b0\u51fa\u826f\u597d\u7684\u7f29\u653e\u884c\u4e3a\uff0c\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u7075\u6d3b\u7684\u4eba\u7c7b\u63d0\u793a\u6761\u4ef6\u751f\u6210\u548c\u57fa\u5e8f\u652f\u67b6\uff0c\u65e0\u9700\u5fae\u8c03\u3002", "conclusion": "PAR\u4f5c\u4e3a\u4e00\u4e2a\u6709\u524d\u666f\u7684\u86cb\u767d\u8d28\u7ed3\u6784\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u81ea\u56de\u5f52\u65b9\u6cd5\u89e3\u51b3\u4e86\u66b4\u9732\u504f\u5dee\u95ee\u9898\uff0c\u5728\u65e0\u6761\u4ef6\u751f\u6210\u548c\u6761\u4ef6\u751f\u6210\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
