{"id": "2602.06282", "pdf": "https://arxiv.org/pdf/2602.06282", "abs": "https://arxiv.org/abs/2602.06282", "authors": ["Marilyn Lionts", "Arnhildur Tomasdottir", "Viktor I. Agustsson", "Yuankai Huo", "Hans T. Bjornsson", "Lotta M. Ellingsen"], "title": "An Interpretable Vision Transformer as a Fingerprint-Based Diagnostic Aid for Kabuki and Wiedemann-Steiner Syndromes", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "Kabuki syndrome (KS) and Wiedemann-Steiner syndrome (WSS) are rare but distinct developmental disorders that share overlapping clinical features, including neurodevelopmental delay, growth restriction, and persistent fetal fingertip pads. While genetic testing remains the diagnostic gold standard, many individuals with KS or WSS remain undiagnosed due to barriers in access to both genetic testing and expertise. Dermatoglyphic anomalies, despite being established hallmarks of several genetic syndromes, remain an underutilized diagnostic signal in the era of molecular testing. This study presents a vision transformer-based deep learning model that leverages fingerprint images to distinguish individuals with KS and WSS from unaffected controls and from one another. We evaluate model performance across three binary classification tasks. Across the three classification tasks, the model achieved AUC scores of 0.80 (control vs. KS), 0.73 (control vs. WSS), and 0.85 (KS vs. WSS), with corresponding F1 scores of 0.71, 0.72, and 0.83, respectively. Beyond classification, we apply attention-based visualizations to identify fingerprint regions most salient to model predictions, enhancing interpretability. Together, these findings suggest the presence of syndrome-specific fingerprint features, demonstrating the feasibility of a fingerprint-based artificial intelligence (AI) tool as a noninvasive, interpretable, and accessible future diagnostic aid for the early diagnosis of underdiagnosed genetic syndromes.", "AI": {"tldr": "\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5229\u7528\u6307\u7eb9\u56fe\u50cf\u533a\u5206Kabuki\u7efc\u5408\u5f81\u3001Wiedemann-Steiner\u7efc\u5408\u5f81\u548c\u5065\u5eb7\u5bf9\u7167\uff0c\u4e3a\u7f55\u89c1\u9057\u4f20\u75c5\u63d0\u4f9b\u975e\u4fb5\u5165\u6027AI\u8bca\u65ad\u5de5\u5177", "motivation": "Kabuki\u7efc\u5408\u5f81\u548cWiedemann-Steiner\u7efc\u5408\u5f81\u662f\u4e34\u5e8a\u8868\u73b0\u91cd\u53e0\u7684\u7f55\u89c1\u53d1\u80b2\u969c\u788d\uff0c\u7531\u4e8e\u9057\u4f20\u68c0\u6d4b\u53ef\u53ca\u6027\u548c\u4e13\u4e1a\u77e5\u8bc6\u7684\u9650\u5236\uff0c\u8bb8\u591a\u60a3\u8005\u672a\u80fd\u5f97\u5230\u8bca\u65ad\u3002\u5c3d\u7ba1\u76ae\u7eb9\u5f02\u5e38\u662f\u591a\u79cd\u9057\u4f20\u7efc\u5408\u5f81\u7684\u5df2\u77e5\u7279\u5f81\uff0c\u4f46\u5728\u5206\u5b50\u68c0\u6d4b\u65f6\u4ee3\u4ecd\u672a\u88ab\u5145\u5206\u5229\u7528\u4f5c\u4e3a\u8bca\u65ad\u4fe1\u53f7\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u6307\u7eb9\u56fe\u50cf\u8fdb\u884c\u4e09\u7c7b\u4e8c\u5143\u5206\u7c7b\uff1a\u5065\u5eb7\u5bf9\u7167vs.KS\u3001\u5065\u5eb7\u5bf9\u7167vs.WSS\u3001KS vs.WSS\u3002\u4f7f\u7528\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u6280\u672f\u8bc6\u522b\u5bf9\u6a21\u578b\u9884\u6d4b\u6700\u5173\u952e\u7684\u6307\u7eb9\u533a\u57df\uff0c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6a21\u578b\u5728\u4e09\u9879\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff1a\u5065\u5eb7\u5bf9\u7167vs.KS\u7684AUC\u4e3a0.80\uff0c\u5065\u5eb7\u5bf9\u7167vs.WSS\u7684AUC\u4e3a0.73\uff0cKS vs.WSS\u7684AUC\u4e3a0.85\u3002\u76f8\u5e94\u7684F1\u5206\u6570\u5206\u522b\u4e3a0.71\u30010.72\u548c0.83\u3002\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u63ed\u793a\u4e86\u4e0e\u7efc\u5408\u5f81\u76f8\u5173\u7684\u7279\u5f02\u6027\u6307\u7eb9\u7279\u5f81\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u7efc\u5408\u5f81\u7279\u5f02\u6027\u6307\u7eb9\u7279\u5f81\u7684\u5b58\u5728\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u6307\u7eb9\u7684AI\u5de5\u5177\u4f5c\u4e3a\u975e\u4fb5\u5165\u6027\u3001\u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u83b7\u53d6\u7684\u8bca\u65ad\u8f85\u52a9\u624b\u6bb5\u7684\u53ef\u884c\u6027\uff0c\u6709\u671b\u7528\u4e8e\u65e9\u671f\u8bca\u65ad\u672a\u786e\u8bca\u7684\u9057\u4f20\u7efc\u5408\u5f81\u3002"}}
{"id": "2602.06296", "pdf": "https://arxiv.org/pdf/2602.06296", "abs": "https://arxiv.org/abs/2602.06296", "authors": ["Takeshi Ishida"], "title": "Internalized Morphogenesis: A Self-Organizing Model for Growth, Replication, and Regeneration via Local Token Exchange in Modular Systems", "categories": ["cs.RO", "q-bio.QM"], "comment": null, "summary": "This study presents an internalized morphogenesis model for autonomous systems, such as swarm robotics and micro-nanomachines, that eliminates the need for external spatial computation. Traditional self-organizing models often require calculations across the entire coordinate space, including empty areas, which is impractical for resource-constrained physical modules. Our proposed model achieves complex morphogenesis through strictly local interactions between adjacent modules within the \"body.\" By extending the \"Ishida token model,\" modules exchange integer values using an RD-inspired discrete analogue without solving differential equations. The internal potential, derived from token accumulation and aging, guides autonomous growth, shrinkage, and replication. Simulations on a hexagonal grid demonstrated the emergence of limb-like extensions, self-division, and robust regeneration capabilities following structural amputation. A key feature is the use of the body boundary as a natural sink for information entropy (tokens) to maintain a dynamic equilibrium. These results indicate that sophisticated morphological behaviors can emerge from minimal, internal-only rules. This framework offers a computationally efficient and biologically plausible approach to developing self-repairing, adaptive, and autonomous hardware.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5185\u90e8\u5316\u5f62\u6001\u53d1\u751f\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u5757\u95f4\u7684\u5c40\u90e8\u4ea4\u4e92\u5b9e\u73b0\u590d\u6742\u5f62\u6001\u751f\u6210\uff0c\u65e0\u9700\u5916\u90e8\u7a7a\u95f4\u8ba1\u7b97\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u81ea\u7ec4\u7ec7\u6a21\u578b\u9700\u8981\u5728\u6574\u4e2a\u5750\u6807\u7a7a\u95f4\u8fdb\u884c\u8ba1\u7b97\uff0c\u5305\u62ec\u7a7a\u533a\u57df\uff0c\u8fd9\u5bf9\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7269\u7406\u6a21\u5757\uff08\u5982\u7fa4\u4f53\u673a\u5668\u4eba\u3001\u5fae\u7eb3\u673a\u5668\uff09\u4e0d\u5207\u5b9e\u9645\u3002\u9700\u8981\u4e00\u79cd\u4ec5\u4f9d\u8d56\u5c40\u90e8\u4ea4\u4e92\u7684\u5185\u90e8\u5316\u65b9\u6cd5\u3002", "method": "\u6269\u5c55\"Ishida token\u6a21\u578b\"\uff0c\u6a21\u5757\u901a\u8fc7\u76f8\u90bb\u6a21\u5757\u4ea4\u6362\u6574\u6570\u503c\uff0c\u91c7\u7528RD\u542f\u53d1\u7684\u79bb\u6563\u6a21\u62df\u800c\u4e0d\u6c42\u89e3\u5fae\u5206\u65b9\u7a0b\u3002\u5185\u90e8\u7535\u4f4d\u7531token\u79ef\u7d2f\u548c\u8001\u5316\u4ea7\u751f\uff0c\u6307\u5bfc\u81ea\u4e3b\u751f\u957f\u3001\u6536\u7f29\u548c\u590d\u5236\u3002", "result": "\u5728\u516d\u8fb9\u5f62\u7f51\u683c\u4e0a\u7684\u6a21\u62df\u5c55\u793a\u4e86\u80a2\u4f53\u6837\u5ef6\u4f38\u3001\u81ea\u6211\u5206\u88c2\u548c\u7ed3\u6784\u622a\u80a2\u540e\u7684\u9c81\u68d2\u518d\u751f\u80fd\u529b\u3002\u5229\u7528\u8eab\u4f53\u8fb9\u754c\u4f5c\u4e3a\u4fe1\u606f\u71b5\uff08token\uff09\u7684\u81ea\u7136\u6c47\u6765\u7ef4\u6301\u52a8\u6001\u5e73\u8861\u3002", "conclusion": "\u590d\u6742\u5f62\u6001\u884c\u4e3a\u53ef\u4ee5\u4ece\u6700\u5c0f\u5316\u3001\u4ec5\u5185\u90e8\u89c4\u5219\u4e2d\u6d8c\u73b0\u3002\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u81ea\u4fee\u590d\u3001\u81ea\u9002\u5e94\u548c\u81ea\u4e3b\u786c\u4ef6\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u4e14\u751f\u7269\u5b66\u5408\u7406\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.06394", "pdf": "https://arxiv.org/pdf/2602.06394", "abs": "https://arxiv.org/abs/2602.06394", "authors": ["Arvid E. Gollwitzer", "Paridhi Latawa", "David de Gruijl", "Deepak A. Subramanian", "Adri\u00e1n Noriega de la Colina"], "title": "Unlocking Noisy Real-World Corpora for Foundation Model Pre-Training via Quality-Aware Tokenization", "categories": ["cs.AI", "cs.CE", "q-bio.GN", "q-fin.CP"], "comment": null, "summary": "Current tokenization methods process sequential data without accounting for signal quality, limiting their effectiveness on noisy real-world corpora. We present QA-Token (Quality-Aware Tokenization), which incorporates data reliability directly into vocabulary construction. We make three key contributions: (i) a bilevel optimization formulation that jointly optimizes vocabulary construction and downstream performance, (ii) a reinforcement learning approach that learns merge policies through quality-aware rewards with convergence guarantees, and (iii) an adaptive parameter learning mechanism via Gumbel-Softmax relaxation for end-to-end optimization. Our experimental evaluation demonstrates consistent improvements: genomics (6.7 percentage point F1 gain in variant calling over BPE), finance (30% Sharpe ratio improvement). At foundation scale, we tokenize a pretraining corpus comprising 1.7 trillion base-pairs and achieve state-of-the-art pathogen detection (94.53 MCC) while reducing token count by 15%. We unlock noisy real-world corpora, spanning petabases of genomic sequences and terabytes of financial time series, for foundation model training with zero inference overhead.", "AI": {"tldr": "QA-Token\uff1a\u4e00\u79cd\u8d28\u91cf\u611f\u77e5\u7684\u5206\u8bcd\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u5c06\u6570\u636e\u53ef\u9760\u6027\u7eb3\u5165\u8bcd\u6c47\u8868\u6784\u5efa\uff0c\u5728\u57fa\u56e0\u7ec4\u5b66\u548c\u91d1\u878d\u9886\u57df\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u5f53\u524d\u7684\u5206\u8bcd\u65b9\u6cd5\u5728\u5904\u7406\u5e8f\u5217\u6570\u636e\u65f6\u672a\u8003\u8651\u4fe1\u53f7\u8d28\u91cf\uff0c\u9650\u5236\u4e86\u5176\u5728\u5608\u6742\u771f\u5b9e\u4e16\u754c\u8bed\u6599\u5e93\u4e0a\u7684\u6709\u6548\u6027\u3002\u9700\u8981\u4e00\u79cd\u80fd\u76f4\u63a5\u7eb3\u5165\u6570\u636e\u53ef\u9760\u6027\u7684\u5206\u8bcd\u65b9\u6cd5\u3002", "method": "\u63d0\u51faQA-Token\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u8d21\u732e\uff1a(1) \u53cc\u5c42\u4f18\u5316\u516c\u5f0f\uff0c\u8054\u5408\u4f18\u5316\u8bcd\u6c47\u8868\u6784\u5efa\u548c\u4e0b\u6e38\u6027\u80fd\uff1b(2) \u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d28\u91cf\u611f\u77e5\u5956\u52b1\u5b66\u4e60\u5408\u5e76\u7b56\u7565\u5e76\u4fdd\u8bc1\u6536\u655b\uff1b(3) \u901a\u8fc7Gumbel-Softmax\u677e\u5f1b\u7684\u81ea\u9002\u5e94\u53c2\u6570\u5b66\u4e60\u673a\u5236\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u4e00\u81f4\u6539\u8fdb\uff1a\u57fa\u56e0\u7ec4\u5b66\uff08\u53d8\u5f02\u68c0\u6d4bF1\u5206\u6570\u6bd4BPE\u63d0\u9ad86.7\u4e2a\u767e\u5206\u70b9\uff09\u3001\u91d1\u878d\uff08\u590f\u666e\u6bd4\u7387\u63d0\u9ad830%\uff09\u3002\u5728\u57fa\u7840\u6a21\u578b\u89c4\u6a21\u4e0a\uff0c\u5206\u8bcd\u4e861.7\u4e07\u4ebf\u78b1\u57fa\u5bf9\u7684\u9884\u8bad\u7ec3\u8bed\u6599\uff0c\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u75c5\u539f\u4f53\u68c0\u6d4b\uff0894.53 MCC\uff09\uff0c\u540c\u65f6\u51cf\u5c1115%\u7684token\u6570\u91cf\u3002", "conclusion": "QA-Token\u89e3\u9501\u4e86\u5608\u6742\u7684\u771f\u5b9e\u4e16\u754c\u8bed\u6599\u5e93\uff08\u5305\u62ec\u6570petabase\u7684\u57fa\u56e0\u7ec4\u5e8f\u5217\u548c\u6570TB\u7684\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\uff09\uff0c\u7528\u4e8e\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\uff0c\u4e14\u63a8\u7406\u65f6\u65e0\u989d\u5916\u5f00\u9500\u3002"}}
