{"id": "2602.11609", "pdf": "https://arxiv.org/pdf/2602.11609", "abs": "https://arxiv.org/abs/2602.11609", "authors": ["Yiming Gao", "Zhen Wang", "Jefferson Chen", "Mark Antkowiak", "Mengzhou Hu", "JungHo Kong", "Dexter Pratt", "Jieyuan Liu", "Enze Ma", "Zhiting Hu", "Eric P. Xing"], "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery", "categories": ["cs.AI", "q-bio.GN"], "comment": "Accepted at NeurIPS 2025 Main Conference", "summary": "We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.\n  To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.\n  Code, data, and package are available at https://github.com/maitrix-org/scPilot", "AI": {"tldr": "scPilot\u662f\u9996\u4e2a\u5b9e\u73b0\u7ec4\u5b66\u539f\u751f\u63a8\u7406\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u68c0\u67e5\u5355\u7ec6\u80deRNA-seq\u6570\u636e\u548c\u4f7f\u7528\u751f\u7269\u4fe1\u606f\u5b66\u5de5\u5177\uff0c\u901a\u8fc7\u9010\u6b65\u63a8\u7406\u89e3\u51b3\u5355\u7ec6\u80de\u5206\u6790\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5355\u7ec6\u80de\u5206\u6790\u9700\u8981\u4e13\u4e1a\u751f\u7269\u4fe1\u606f\u5b66\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002scPilot\u65e8\u5728\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u57fa\u4e8e\u539f\u59cb\u7ec4\u5b66\u6570\u636e\u8fdb\u884c\u63a8\u7406\uff0c\u5b9e\u73b0\u53ef\u5ba1\u8ba1\u3001\u53ef\u89e3\u91ca\u7684\u5355\u7ec6\u80de\u5206\u6790\u3002", "method": "\u5c06\u6838\u5fc3\u5355\u7ec6\u80de\u5206\u6790\uff08\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u3001\u53d1\u80b2\u8f68\u8ff9\u91cd\u5efa\u3001\u8f6c\u5f55\u56e0\u5b50\u9776\u5411\uff09\u8f6c\u5316\u4e3a\u9010\u6b65\u63a8\u7406\u95ee\u9898\uff0c\u8ba9LLM\u5728\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u4e2d\u76f4\u63a5\u68c0\u67e5\u5355\u7ec6\u80de\u6570\u636e\u548c\u5de5\u5177\uff0c\u652f\u6301\u8fed\u4ee3\u63a8\u7406\u548c\u8bc1\u636e\u4fee\u8ba2\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a\u8fed\u4ee3\u7ec4\u5b66\u539f\u751f\u63a8\u7406\u5c06\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534711%\uff1bGemini-2.5-Pro\u5c06\u8f68\u8ff9\u56fe\u7f16\u8f91\u8ddd\u79bb\u51cf\u5c1130%\uff1b\u751f\u6210\u900f\u660e\u63a8\u7406\u8f68\u8ff9\u89e3\u91ca\u6807\u8bb0\u57fa\u56e0\u6a21\u7cca\u6027\u548c\u8c03\u63a7\u903b\u8f91\u3002", "conclusion": "scPilot\u901a\u8fc7\u5c06LLM\u951a\u5b9a\u5728\u539f\u59cb\u7ec4\u5b66\u6570\u636e\u4e2d\uff0c\u5b9e\u73b0\u4e86\u53ef\u5ba1\u8ba1\u3001\u53ef\u89e3\u91ca\u4e14\u5177\u6709\u8bca\u65ad\u4fe1\u606f\u7684\u5355\u7ec6\u80de\u5206\u6790\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u8303\u5f0f\u3002"}}
{"id": "2602.11618", "pdf": "https://arxiv.org/pdf/2602.11618", "abs": "https://arxiv.org/abs/2602.11618", "authors": ["Tatsuya Sagawa", "Ryosuke Kojima"], "title": "How Well Do Large-Scale Chemical Language Models Transfer to Downstream Tasks?", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Chemical Language Models (CLMs) pre-trained on large scale molecular data are widely used for molecular property prediction. However, the common belief that increasing training resources such as model size, dataset size, and training compute improves both pretraining loss and downstream task performance has not been systematically validated in the chemical domain. In this work, we evaluate this assumption by pretraining CLMs while scaling training resources and measuring transfer performance across diverse molecular property prediction (MPP) tasks. We find that while pretraining loss consistently decreases with increased training resources, downstream task performance shows limited improvement. Moreover, alternative metrics based on the Hessian or loss landscape also fail to estimate downstream performance in CLMs. We further identify conditions under which downstream performance saturates or degrades despite continued improvements in pretraining metrics, and analyze the underlying task dependent failure modes through parameter space visualizations. These results expose a gap between pretraining based evaluation and downstream performance, and emphasize the need for model selection and evaluation strategies that explicitly account for downstream task characteristics.", "AI": {"tldr": "\u5316\u5b66\u8bed\u8a00\u6a21\u578b\uff08CLMs\uff09\u7684\u9884\u8bad\u7ec3\u635f\u5931\u968f\u8bad\u7ec3\u8d44\u6e90\u589e\u52a0\u800c\u964d\u4f4e\uff0c\u4f46\u4e0b\u6e38\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u63d0\u5347\u6709\u9650\uff0c\u9884\u8bad\u7ec3\u6307\u6807\u4e0e\u4e0b\u6e38\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "motivation": "\u9a8c\u8bc1\u5316\u5b66\u9886\u57df\u4e2d\"\u589e\u52a0\u8bad\u7ec3\u8d44\u6e90\uff08\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u96c6\u5927\u5c0f\u3001\u8bad\u7ec3\u8ba1\u7b97\u91cf\uff09\u80fd\u540c\u65f6\u6539\u5584\u9884\u8bad\u7ec3\u635f\u5931\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\"\u8fd9\u4e00\u5e38\u89c1\u5047\u8bbe\u662f\u5426\u6210\u7acb\u3002", "method": "\u901a\u8fc7\u7f29\u653e\u8bad\u7ec3\u8d44\u6e90\u9884\u8bad\u7ec3CLMs\uff0c\u6d4b\u91cf\u5728\u591a\u6837\u5316\u5206\u5b50\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u8fc1\u79fb\u6027\u80fd\uff0c\u5206\u6790Hessian\u548c\u635f\u5931\u666f\u89c2\u7b49\u66ff\u4ee3\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u7a7a\u95f4\u53ef\u89c6\u5316\u5206\u6790\u4efb\u52a1\u4f9d\u8d56\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u9884\u8bad\u7ec3\u635f\u5931\u968f\u8bad\u7ec3\u8d44\u6e90\u589e\u52a0\u800c\u6301\u7eed\u964d\u4f4e\uff0c\u4f46\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u6539\u5584\u6709\u9650\uff1b\u57fa\u4e8eHessian\u6216\u635f\u5931\u666f\u89c2\u7684\u66ff\u4ee3\u6307\u6807\u4e5f\u65e0\u6cd5\u6709\u6548\u4f30\u8ba1CLMs\u7684\u4e0b\u6e38\u6027\u80fd\uff1b\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\uff0c\u4e0b\u6e38\u6027\u80fd\u4f1a\u9971\u548c\u751a\u81f3\u4e0b\u964d\uff0c\u5c3d\u7ba1\u9884\u8bad\u7ec3\u6307\u6807\u6301\u7eed\u6539\u5584\u3002", "conclusion": "\u9884\u8bad\u7ec3\u8bc4\u4f30\u4e0e\u4e0b\u6e38\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u660e\u786e\u8003\u8651\u4e0b\u6e38\u4efb\u52a1\u7279\u6027\u7684\u6a21\u578b\u9009\u62e9\u548c\u8bc4\u4f30\u7b56\u7565\u3002"}}
{"id": "2602.12026", "pdf": "https://arxiv.org/pdf/2602.12026", "abs": "https://arxiv.org/abs/2602.12026", "authors": ["Darin Tsui", "Kunal Talreja", "Daniel Saeedi", "Amirali Aghazadeh"], "title": "Protein Circuit Tracing via Cross-layer Transcoders", "categories": ["cs.LG", "q-bio.QM"], "comment": "29 pages, 15 figures", "summary": "Protein language models (pLMs) have emerged as powerful predictors of protein structure and function. However, the computational circuits underlying their predictions remain poorly understood. Recent mechanistic interpretability methods decompose pLM representations into interpretable features, but they treat each layer independently and thus fail to capture cross-layer computation, limiting their ability to approximate the full model. We introduce ProtoMech, a framework for discovering computational circuits in pLMs using cross-layer transcoders that learn sparse latent representations jointly across layers to capture the model's full computational circuitry. Applied to the pLM ESM2, ProtoMech recovers 82-89% of the original performance on protein family classification and function prediction tasks. ProtoMech then identifies compressed circuits that use <1% of the latent space while retaining up to 79% of model accuracy, revealing correspondence with structural and functional motifs, including binding, signaling, and stability. Steering along these circuits enables high-fitness protein design, surpassing baseline methods in more than 70% of cases. These results establish ProtoMech as a principled framework for protein circuit tracing.", "AI": {"tldr": "ProtoMech\u662f\u4e00\u4e2a\u7528\u4e8e\u53d1\u73b0\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u4e2d\u8ba1\u7b97\u7535\u8def\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5c42\u8f6c\u7801\u5668\u5b66\u4e60\u7a00\u758f\u6f5c\u5728\u8868\u793a\uff0c\u80fd\u6062\u590d82-89%\u7684\u539f\u59cb\u6027\u80fd\uff0c\u8bc6\u522b\u51fa\u4ec5\u4f7f\u7528<1%\u6f5c\u5728\u7a7a\u95f4\u7684\u538b\u7f29\u7535\u8def\uff0c\u5e76\u53ef\u7528\u4e8e\u86cb\u767d\u8d28\u8bbe\u8ba1\u3002", "motivation": "\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5df2\u6210\u4e3a\u9884\u6d4b\u86cb\u767d\u8d28\u7ed3\u6784\u548c\u529f\u80fd\u7684\u6709\u529b\u5de5\u5177\uff0c\u4f46\u5176\u80cc\u540e\u7684\u8ba1\u7b97\u673a\u5236\u4ecd\u4e0d\u660e\u786e\u3002\u73b0\u6709\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u6bcf\u4e00\u5c42\uff0c\u65e0\u6cd5\u6355\u6349\u8de8\u5c42\u8ba1\u7b97\uff0c\u9650\u5236\u4e86\u8fd1\u4f3c\u5b8c\u6574\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165ProtoMech\u6846\u67b6\uff0c\u4f7f\u7528\u8de8\u5c42\u8f6c\u7801\u5668\u5b66\u4e60\u7a00\u758f\u6f5c\u5728\u8868\u793a\uff0c\u5171\u540c\u6355\u6349\u6a21\u578b\u5728\u6240\u6709\u5c42\u7684\u5b8c\u6574\u8ba1\u7b97\u7535\u8def\u3002\u5e94\u7528\u4e8eESM2\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u86cb\u767d\u8d28\u5bb6\u65cf\u5206\u7c7b\u548c\u529f\u80fd\u9884\u6d4b\u4efb\u52a1\u4e2d\u6062\u590d82-89%\u7684\u539f\u59cb\u6027\u80fd\uff1b\u8bc6\u522b\u51fa\u4ec5\u4f7f\u7528<1%\u6f5c\u5728\u7a7a\u95f4\u7684\u538b\u7f29\u7535\u8def\uff0c\u4fdd\u7559\u9ad8\u8fbe79%\u7684\u6a21\u578b\u51c6\u786e\u7387\uff1b\u63ed\u793a\u4e0e\u7ed3\u6784\u548c\u529f\u80fd\u57fa\u5e8f\u7684\u5bf9\u5e94\u5173\u7cfb\uff1b\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u4e2d\uff0c\u8d85\u8fc770%\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ProtoMech\u4e3a\u86cb\u767d\u8d28\u7535\u8def\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u80fd\u591f\u53d1\u73b0\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8ba1\u7b97\u7535\u8def\uff0c\u63ed\u793a\u5176\u4e0e\u751f\u7269\u5b66\u529f\u80fd\u7684\u8054\u7cfb\uff0c\u5e76\u5e94\u7528\u4e8e\u86cb\u767d\u8d28\u8bbe\u8ba1\u3002"}}
