{"id": "2602.16696", "pdf": "https://arxiv.org/pdf/2602.16696", "abs": "https://arxiv.org/abs/2602.16696", "authors": ["Huan Souza", "Pankaj Mehta"], "title": "Parameter-free representations outperform single-cell foundation models on downstream benchmarks", "categories": ["q-bio.GN", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7b80\u5355\u7684\u7ebf\u6027\u65b9\u6cd5\u5728\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u5206\u6790\u4e2d\u53ef\u4ee5\u8fbe\u5230\u4e0e\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u6311\u6218\u4e86\u5f53\u524d\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "motivation": "\u5f53\u524d\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u5206\u6790\u4e2d\uff0c\u5927\u578b\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u6a21\u578b\uff08\u5982TranscriptFormer\uff09\u867d\u7136\u53d6\u5f97\u4e86\u5148\u8fdb\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u66f4\u7b80\u5355\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u6cd5\u8fbe\u5230\u76f8\u4f3c\u6027\u80fd\uff0c\u4ece\u800c\u8d28\u7591\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5355\u7ec6\u80de\u6570\u636e\u5206\u6790\u4e2d\u7684\u5fc5\u8981\u6027\u3002", "method": "\u91c7\u7528\u7b80\u5355\u3001\u53ef\u89e3\u91ca\u7684\u6d41\u7a0b\uff0c\u4e3b\u8981\u4f9d\u8d56\u4ed4\u7ec6\u7684\u6807\u51c6\u5316\u5904\u7406\u548c\u7ebf\u6027\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u8ba1\u7b97\u5bc6\u96c6\u7684\u6df1\u5ea6\u5b66\u4e60\u8868\u793a\u3002\u8fd9\u4e9b\u65b9\u6cd5\u907f\u514d\u4e86\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4e13\u6ce8\u4e8e\u57fa\u7840\u7684\u7edf\u8ba1\u5904\u7406\u3002", "result": "\u5728\u591a\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7b80\u5355\u7ebf\u6027\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6216\u63a5\u8fd1\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u7279\u522b\u662f\u5728\u6d89\u53ca\u8bad\u7ec3\u6570\u636e\u4e2d\u672a\u89c1\u8fc7\u7684\u65b0\u7ec6\u80de\u7c7b\u578b\u548c\u751f\u7269\u4f53\u7684\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u57fa\u7840\u6a21\u578b\u7684\u8868\u73b0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7ec6\u80de\u8eab\u4efd\u7684\u751f\u7269\u5b66\u7279\u5f81\u53ef\u4ee5\u901a\u8fc7\u5355\u7ec6\u80de\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u7684\u7b80\u5355\u7ebf\u6027\u8868\u793a\u6765\u6355\u6349\u3002\u8fd9\u5f3a\u8c03\u4e86\u4e25\u683c\u57fa\u51c6\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6311\u6218\u4e86\u5f53\u524d\u5bf9\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u57fa\u7840\u6a21\u578b\u7684\u8fc7\u5ea6\u4f9d\u8d56\u3002"}}
{"id": "2602.16504", "pdf": "https://arxiv.org/pdf/2602.16504", "abs": "https://arxiv.org/abs/2602.16504", "authors": ["Ashley Babjac", "Adrienne Hoarfrost"], "title": "GRIMM: Genetic stRatification for Inference in Molecular Modeling", "categories": ["q-bio.QM"], "comment": "9 pages, 1 figure, 2 tables, submitted to ISMB main conference proceedings 2026", "summary": "The vast majority of biological sequences encode unknown functions and bear little resemblance to experimentally characterized proteins, limiting both our understanding of biology and our ability to harness functional potential for the bioeconomy. Predicting enzyme function from sequence remains a central challenge in computational biology, complicated by low sequence diversity and imbalanced label support in publicly available datasets. Models trained on these data can overestimate performance and fail to generalize. To address this, we introduce GRIMM (Genetic stRatification for Inference in Molecular Modeling), a benchmark for enzyme function prediction that employs genetic stratification: sequences are clustered by similarity and clusters are assigned exclusively to training, validation, or test sets. This ensures that sequences from the same cluster do not appear in multiple partitions. GRIMM produces multiple test sets: a closed-set test with the same label distribution as training (Test-1) and an open-set test containing novel labels (Test-2), serving as a realistic out-of-distribution proxy for discovering novel enzyme functions. While demonstrated on enzymes, this approach is generalizable to any sequence-based classification task where inputs can be clustered by similarity. By formalizing a splitting strategy often used implicitly, GRIMM provides a unified and reproducible framework for closed- and open-set evaluation. The method is lightweight, requiring only sequence clustering and label annotations, and can be adapted to different similarity thresholds, data scales, and biological tasks. GRIMM enables more realistic evaluation of functional prediction models on both familiar and unseen classes and establishes a benchmark that more faithfully assesses model performance and generalizability.", "AI": {"tldr": "GRIMM\u662f\u4e00\u4e2a\u7528\u4e8e\u9176\u529f\u80fd\u9884\u6d4b\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u9057\u4f20\u5206\u5c42\uff08\u57fa\u4e8e\u5e8f\u5217\u76f8\u4f3c\u6027\u805a\u7c7b\uff09\u6765\u521b\u5efa\u66f4\u771f\u5b9e\u7684\u8bad\u7ec3/\u9a8c\u8bc1/\u6d4b\u8bd5\u96c6\u5212\u5206\uff0c\u652f\u6301\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u8bc4\u4f30\u3002", "motivation": "\u5927\u591a\u6570\u751f\u7269\u5e8f\u5217\u529f\u80fd\u672a\u77e5\u4e14\u4e0e\u5df2\u77e5\u86cb\u767d\u8d28\u76f8\u4f3c\u6027\u4f4e\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5e8f\u5217\u591a\u6837\u6027\u4f4e\u3001\u6807\u7b7e\u5206\u5e03\u4e0d\u5e73\u8861\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u88ab\u9ad8\u4f30\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u8981\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "GRIMM\u91c7\u7528\u9057\u4f20\u5206\u5c42\u7b56\u7565\uff1a\u5c06\u5e8f\u5217\u6309\u76f8\u4f3c\u6027\u805a\u7c7b\uff0c\u6bcf\u4e2a\u805a\u7c7b\u88ab\u5b8c\u5168\u5206\u914d\u5230\u8bad\u7ec3\u3001\u9a8c\u8bc1\u6216\u6d4b\u8bd5\u96c6\u4e4b\u4e00\uff0c\u786e\u4fdd\u540c\u4e00\u805a\u7c7b\u7684\u5e8f\u5217\u4e0d\u4f1a\u51fa\u73b0\u5728\u591a\u4e2a\u5206\u533a\u3002\u521b\u5efa\u4e24\u79cd\u6d4b\u8bd5\u96c6\uff1a\u5c01\u95ed\u96c6\u6d4b\u8bd5\uff08Test-1\uff0c\u6807\u7b7e\u5206\u5e03\u4e0e\u8bad\u7ec3\u96c6\u76f8\u540c\uff09\u548c\u5f00\u653e\u96c6\u6d4b\u8bd5\uff08Test-2\uff0c\u5305\u542b\u65b0\u6807\u7b7e\uff09\u3002", "result": "GRIMM\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u4e14\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u80fd\u591f\u66f4\u771f\u5b9e\u5730\u8bc4\u4f30\u529f\u80fd\u9884\u6d4b\u6a21\u578b\u5728\u719f\u6089\u7c7b\u522b\u548c\u672a\u89c1\u7c7b\u522b\u4e0a\u7684\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u66f4\u80fd\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u7684\u57fa\u51c6\u3002", "conclusion": "GRIMM\u65b9\u6cd5\u8f7b\u91cf\u7ea7\uff08\u4ec5\u9700\u5e8f\u5217\u805a\u7c7b\u548c\u6807\u7b7e\u6ce8\u91ca\uff09\uff0c\u53ef\u9002\u5e94\u4e0d\u540c\u76f8\u4f3c\u6027\u9608\u503c\u3001\u6570\u636e\u89c4\u6a21\u548c\u751f\u7269\u4efb\u52a1\uff0c\u4e3a\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9176\u529f\u80fd\u9884\u6d4b\u548c\u751f\u7269\u7ecf\u6d4e\u5e94\u7528\u3002"}}
{"id": "2602.16004", "pdf": "https://arxiv.org/pdf/2602.16004", "abs": "https://arxiv.org/abs/2602.16004", "authors": ["Nan Xu", "Xiaodi Zhang", "Wen-Ju Pan", "Jeremy L. Smith", "Eric H. Schumacher", "Jason W. Allen", "Vince D. Calhoun", "Shella D. Keilholz"], "title": "Time-Varying Directed Interactions in Functional Brain Networks: Modeling and Validation", "categories": ["q-bio.NC", "q-bio.QM"], "comment": null, "summary": "Understanding the dynamic nature of brain connectivity is critical for elucidating neural processing, behavior, and brain disorders. Traditional approaches such as sliding-window correlation (SWC) characterize time-varying undirected associations but do not resolve directional interactions, limiting inference about time-resolved information flow in brain networks. We introduce sliding-window prediction correlation (SWpC), which embeds a directional linear time-invariant (LTI) model within each sliding window to estimate time-varying directed functional connectivity (FC). SWpC yields two complementary descriptors of directed interactions: a strength measure (prediction correlation) and a duration measure (window-wise duration of information transfer). Using concurrent local field potential (LFP) and fMRI BOLD recordings from rat somatosensory cortices, we demonstrate stable directionality estimates in both LFP band-limited power and BOLD. Using Human Connectome Project (HCP) motor task fMRI, SWpC detects significant task-evoked changes in directed FC strength and duration and shows higher sensitivity than SWC for identifying task-evoked connectivity differences. Finally, in post-concussion vestibular dysfunction (PCVD), SWpC reveals reproducible vestibular-multisensory brain-state shifts and improves healthy-control vs subacute patient (HC-ST) discrimination using state-derived features. Together, these results show that SWpC provides biologically interpretable, time-resolved directed connectivity patterns across multimodal validation and clinical application settings, supporting both basic and translational neuroscience.", "AI": {"tldr": "\u63d0\u51fa\u6ed1\u52a8\u7a97\u53e3\u9884\u6d4b\u76f8\u5173\uff08SWpC\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u65f6\u53d8\u6709\u5411\u529f\u80fd\u8fde\u63a5\uff0c\u76f8\u6bd4\u4f20\u7edf\u6ed1\u52a8\u7a97\u53e3\u76f8\u5173\uff08SWC\uff09\u80fd\u6355\u6349\u65b9\u5411\u6027\u4fe1\u606f\u6d41", "motivation": "\u4f20\u7edf\u6ed1\u52a8\u7a97\u53e3\u76f8\u5173\u65b9\u6cd5\u53ea\u80fd\u8868\u5f81\u65f6\u53d8\u65e0\u5411\u5173\u8054\uff0c\u65e0\u6cd5\u89e3\u6790\u65b9\u5411\u6027\u76f8\u4e92\u4f5c\u7528\uff0c\u9650\u5236\u4e86\u8111\u7f51\u7edc\u4e2d\u65f6\u95f4\u5206\u8fa8\u4fe1\u606f\u6d41\u7684\u63a8\u65ad", "method": "\u5728\u6ed1\u52a8\u7a97\u53e3\u5185\u5d4c\u5165\u65b9\u5411\u6027\u7ebf\u6027\u65f6\u4e0d\u53d8\u6a21\u578b\uff0c\u4f30\u8ba1\u65f6\u53d8\u6709\u5411\u529f\u80fd\u8fde\u63a5\uff0c\u4ea7\u751f\u4e24\u4e2a\u4e92\u8865\u63cf\u8ff0\u7b26\uff1a\u5f3a\u5ea6\u5ea6\u91cf\uff08\u9884\u6d4b\u76f8\u5173\uff09\u548c\u6301\u7eed\u65f6\u95f4\u5ea6\u91cf\uff08\u4fe1\u606f\u4f20\u9012\u7684\u7a97\u53e3\u6301\u7eed\u65f6\u95f4\uff09", "result": "1. \u5728\u5927\u9f20\u611f\u89c9\u76ae\u5c42LFP\u548cBOLD\u8bb0\u5f55\u4e2d\u663e\u793a\u7a33\u5b9a\u7684\u65b9\u5411\u6027\u4f30\u8ba1\uff1b2. \u5728HCP\u8fd0\u52a8\u4efb\u52a1fMRI\u4e2d\u68c0\u6d4b\u5230\u663e\u8457\u4efb\u52a1\u8bf1\u53d1\u6709\u5411\u8fde\u63a5\u5f3a\u5ea6\u548c\u6301\u7eed\u65f6\u95f4\u53d8\u5316\uff0c\u6bd4SWC\u66f4\u654f\u611f\uff1b3. \u5728\u8111\u9707\u8361\u540e\u524d\u5ead\u529f\u80fd\u969c\u788d\u4e2d\u63ed\u793a\u53ef\u91cd\u590d\u7684\u8111\u72b6\u6001\u8f6c\u79fb\uff0c\u5e76\u6539\u5584\u60a3\u8005\u5206\u7c7b", "conclusion": "SWpC\u63d0\u4f9b\u4e86\u751f\u7269\u5b66\u53ef\u89e3\u91ca\u3001\u65f6\u95f4\u5206\u8fa8\u7684\u6709\u5411\u8fde\u63a5\u6a21\u5f0f\uff0c\u652f\u6301\u57fa\u7840\u795e\u7ecf\u79d1\u5b66\u548c\u8f6c\u5316\u795e\u7ecf\u79d1\u5b66\u7814\u7a76"}}
{"id": "2602.16357", "pdf": "https://arxiv.org/pdf/2602.16357", "abs": "https://arxiv.org/abs/2602.16357", "authors": ["Sarkis Ter Martirosyan", "Xinyue Huang", "David Qin", "Anthony Yu", "Stanislav Emelianov"], "title": "Optical Inversion and Spectral Unmixing of Spectroscopic Photoacoustic Images with Physics-Informed Neural Networks", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Accurate estimation of the relative concentrations of chromophores in a spectroscopic photoacoustic (sPA) image can reveal immense structural, functional, and molecular information about physiological processes. However, due to nonlinearities and ill-posedness inherent to sPA imaging, concentration estimation is intractable. The Spectroscopic Photoacoustic Optical Inversion Autoencoder (SPOI-AE) aims to address the sPA optical inversion and spectral unmixing problems without assuming linearity. Herein, SPOI-AE was trained and tested on \\textit{in vivo} mouse lymph node sPA images with unknown ground truth chromophore concentrations. SPOI-AE better reconstructs input sPA pixels than conventional algorithms while providing biologically coherent estimates for optical parameters, chromophore concentrations, and the percent oxygen saturation of tissue. SPOI-AE's unmixing accuracy was validated using a simulated mouse lymph node phantom ground truth.", "AI": {"tldr": "SPOI-AE\u662f\u4e00\u79cd\u7528\u4e8e\u5149\u8c31\u5149\u58f0\u6210\u50cf\u7684\u81ea\u52a8\u7f16\u7801\u5668\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u91cd\u5efa\u8f93\u5165\u50cf\u7d20\u5e76\u63d0\u4f9b\u751f\u7269\u5b66\u4e0a\u4e00\u81f4\u7684\u5149\u5b66\u53c2\u6570\u3001\u8272\u7d20\u6d53\u5ea6\u548c\u7ec4\u7ec7\u6c27\u9971\u548c\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u5149\u8c31\u5149\u58f0\u6210\u50cf\u4e2d\u8272\u7d20\u76f8\u5bf9\u6d53\u5ea6\u7684\u51c6\u786e\u4f30\u8ba1\u80fd\u63ed\u793a\u4e30\u5bcc\u7684\u751f\u7406\u8fc7\u7a0b\u4fe1\u606f\uff0c\u4f46\u7531\u4e8e\u975e\u7ebf\u6027\u6027\u548c\u4e0d\u9002\u5b9a\u6027\uff0c\u6d53\u5ea6\u4f30\u8ba1\u96be\u4ee5\u5b9e\u73b0\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u7ebf\u6027\u5173\u7cfb\uff0c\u65e0\u6cd5\u5904\u7406\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86SPOI-AE\uff08\u5149\u8c31\u5149\u58f0\u5149\u5b66\u53cd\u6f14\u81ea\u52a8\u7f16\u7801\u5668\uff09\uff0c\u65e0\u9700\u5047\u8bbe\u7ebf\u6027\u5173\u7cfb\uff0c\u76f4\u63a5\u5904\u7406\u5149\u58f0\u6210\u50cf\u7684\u5149\u5b66\u53cd\u6f14\u548c\u5149\u8c31\u89e3\u6df7\u95ee\u9898\u3002\u4f7f\u7528\u672a\u77e5\u771f\u5b9e\u8272\u7d20\u6d53\u5ea6\u7684\u6d3b\u4f53\u5c0f\u9f20\u6dcb\u5df4\u7ed3\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "SPOI-AE\u6bd4\u4f20\u7edf\u7b97\u6cd5\u66f4\u597d\u5730\u91cd\u5efa\u8f93\u5165\u5149\u58f0\u50cf\u7d20\uff0c\u540c\u65f6\u63d0\u4f9b\u751f\u7269\u5b66\u4e0a\u4e00\u81f4\u7684\u5149\u5b66\u53c2\u6570\u3001\u8272\u7d20\u6d53\u5ea6\u548c\u7ec4\u7ec7\u6c27\u9971\u548c\u5ea6\u4f30\u8ba1\u3002\u901a\u8fc7\u6a21\u62df\u5c0f\u9f20\u6dcb\u5df4\u7ed3\u5e7b\u5f71\u7684\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u4e86\u89e3\u6df7\u51c6\u786e\u6027\u3002", "conclusion": "SPOI-AE\u6210\u529f\u89e3\u51b3\u4e86\u5149\u8c31\u5149\u58f0\u6210\u50cf\u4e2d\u7684\u975e\u7ebf\u6027\u5149\u5b66\u53cd\u6f14\u548c\u5149\u8c31\u89e3\u6df7\u95ee\u9898\uff0c\u4e3a\u6d3b\u4f53\u7ec4\u7ec7\u8272\u7d20\u6d53\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
