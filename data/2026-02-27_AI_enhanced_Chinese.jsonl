{"id": "2602.22236", "pdf": "https://arxiv.org/pdf/2602.22236", "abs": "https://arxiv.org/abs/2602.22236", "authors": ["Rabeya Tus Sadia", "Qiang Ye", "Qiang Cheng"], "title": "CrossLLM-Mamba: Multimodal State Space Fusion of LLMs for RNA Interaction Prediction", "categories": ["q-bio.GN", "cs.CV", "cs.LG"], "comment": null, "summary": "Accurate prediction of RNA-associated interactions is essential for understanding cellular regulation and advancing drug discovery. While Biological Large Language Models (BioLLMs) such as ESM-2 and RiNALMo provide powerful sequence representations, existing methods rely on static fusion strategies that fail to capture the dynamic, context-dependent nature of molecular binding. We introduce CrossLLM-Mamba, a novel framework that reformulates interaction prediction as a state-space alignment problem. By leveraging bidirectional Mamba encoders, our approach enables deep ``crosstalk'' between modality-specific embeddings through hidden state propagation, modeling interactions as dynamic sequence transitions rather than static feature overlaps. The framework maintains linear computational complexity, making it scalable to high-dimensional BioLLM embeddings. We further incorporate Gaussian noise injection and Focal Loss to enhance robustness against hard-negative samples. Comprehensive experiments across three interaction categories, RNA-protein, RNA-small molecule, and RNA-RNA demonstrate that CrossLLM-Mamba achieves state-of-the-art performance. On the RPI1460 benchmark, our model attains an MCC of 0.892, surpassing the previous best by 5.2\\%. For binding affinity prediction, we achieve Pearson correlations exceeding 0.95 on riboswitch and repeat RNA subtypes. These results establish state-space modeling as a powerful paradigm for multi-modal biological interaction prediction.", "AI": {"tldr": "CrossLLM-Mamba\uff1a\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u5bf9\u9f50\u7684RNA\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u65b0\u6846\u67b6\uff0c\u5229\u7528\u53cc\u5411Mamba\u7f16\u7801\u5668\u5b9e\u73b0\u6a21\u6001\u95f4\u6df1\u5ea6\u4ea4\u4e92\uff0c\u5728\u591a\u4e2aRNA\u76f8\u4e92\u4f5c\u7528\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eBioLLM\u7684RNA\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u878d\u5408\u7b56\u7565\uff0c\u65e0\u6cd5\u6355\u6349\u5206\u5b50\u7ed3\u5408\u7684\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7279\u6027\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u5c06\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u72b6\u6001\u7a7a\u95f4\u5bf9\u9f50\u95ee\u9898\uff0c\u4f7f\u7528\u53cc\u5411Mamba\u7f16\u7801\u5668\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u4f20\u64ad\u5b9e\u73b0\u6a21\u6001\u7279\u5b9a\u5d4c\u5165\u95f4\u7684\u6df1\u5ea6\"\u5bf9\u8bdd\"\uff0c\u5c06\u76f8\u4e92\u4f5c\u7528\u5efa\u6a21\u4e3a\u52a8\u6001\u5e8f\u5217\u8f6c\u6362\u800c\u975e\u9759\u6001\u7279\u5f81\u91cd\u53e0\u3002\u6846\u67b6\u4fdd\u6301\u7ebf\u6027\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5e76\u5f15\u5165\u9ad8\u65af\u566a\u58f0\u6ce8\u5165\u548cFocal Loss\u589e\u5f3a\u5bf9\u56f0\u96be\u8d1f\u6837\u672c\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728RNA-\u86cb\u767d\u8d28\u3001RNA-\u5c0f\u5206\u5b50\u3001RNA-RNA\u4e09\u7c7b\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97SOTA\u6027\u80fd\u3002\u5728RPI1460\u57fa\u51c6\u6d4b\u8bd5\u4e2dMCC\u8fbe\u52300.892\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u53475.2%\u3002\u5728\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u6838\u7cd6\u5f00\u5173\u548c\u91cd\u590dRNA\u4e9a\u578b\u7684Pearson\u76f8\u5173\u7cfb\u6570\u8d85\u8fc70.95\u3002", "conclusion": "\u72b6\u6001\u7a7a\u95f4\u5efa\u6a21\u662f\u591a\u6a21\u6001\u751f\u7269\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u7684\u5f3a\u5927\u8303\u5f0f\uff0cCrossLLM-Mamba\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5e8f\u5217\u8f6c\u6362\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86RNA\u76f8\u5173\u76f8\u4e92\u4f5c\u7528\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2602.22247", "pdf": "https://arxiv.org/pdf/2602.22247", "abs": "https://arxiv.org/abs/2602.22247", "authors": ["Ihor Kendiukhov"], "title": "Multi-Dimensional Spectral Geometry of Biological Knowledge in Single-Cell Transformer Representations", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "comment": null, "summary": "Single-cell foundation models such as scGPT learn high-dimensional gene representations, but what biological knowledge these representations encode remains unclear. We systematically decode the geometric structure of scGPT internal representations through 63 iterations of automated hypothesis screening (183 hypotheses tested), revealing that the model organizes genes into a structured biological coordinate system rather than an opaque feature space.\n  The dominant spectral axis separates genes by subcellular localization, with secreted proteins at one pole and cytosolic proteins at the other. Intermediate transformer layers transiently encode mitochondrial and ER compartments in a sequence that mirrors the cellular secretory pathway. Orthogonal axes encode protein-protein interaction networks with graded fidelity to experimentally measured interaction strength (Spearman rho = 1.000 across n = 5 STRING confidence quintiles, p = 0.017).\n  In a compact six-dimensional spectral subspace, the model distinguishes transcription factors from their target genes (AUROC = 0.744, all 12 layers significant). Early layers preserve which specific genes regulate which targets, while deeper layers compress this into a coarser regulator versus regulated distinction. Repression edges are geometrically more prominent than activation edges, and B-cell master regulators BATF and BACH2 show convergence toward the B-cell identity anchor PAX5 across transformer depth. Cell-type marker genes cluster with high fidelity (AUROC = 0.851). Residual-stream geometry encodes biological structure complementary to attention patterns. These results indicate that biological transformers learn an interpretable internal model of cellular organization, with implications for regulatory network inference, drug target prioritization, and model auditing.", "AI": {"tldr": "scGPT\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u5b66\u4e60\u5230\u7684\u57fa\u56e0\u8868\u5f81\u5177\u6709\u53ef\u89e3\u91ca\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u5f62\u6210\u751f\u7269\u5750\u6807\u7cfb\u7edf\u800c\u975e\u4e0d\u900f\u660e\u7279\u5f81\u7a7a\u95f4\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5185\u90e8\u5bf9\u7ec6\u80de\u7ec4\u7ec7\u7684\u7ed3\u6784\u5316\u7406\u89e3\u3002", "motivation": "\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u5982scGPT\u5b66\u4e60\u9ad8\u7ef4\u57fa\u56e0\u8868\u5f81\uff0c\u4f46\u8fd9\u4e9b\u8868\u5f81\u7f16\u7801\u4e86\u54ea\u4e9b\u751f\u7269\u5b66\u77e5\u8bc6\u5c1a\u4e0d\u6e05\u695a\u3002\u9700\u8981\u7cfb\u7edf\u89e3\u7801scGPT\u5185\u90e8\u8868\u5f81\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u7406\u89e3\u6a21\u578b\u5982\u4f55\u7ec4\u7ec7\u548c\u8868\u793a\u751f\u7269\u5b66\u4fe1\u606f\u3002", "method": "\u901a\u8fc763\u6b21\u81ea\u52a8\u5316\u5047\u8bbe\u7b5b\u9009\uff08\u6d4b\u8bd5183\u4e2a\u5047\u8bbe\uff09\uff0c\u7cfb\u7edf\u89e3\u7801scGPT\u5185\u90e8\u8868\u5f81\u7684\u51e0\u4f55\u7ed3\u6784\u3002\u5206\u6790\u6a21\u578b\u7684\u8c31\u8f74\u3001\u8f6c\u6362\u5668\u5c42\u3001\u6b63\u4ea4\u8f74\u7b49\uff0c\u7814\u7a76\u57fa\u56e0\u5728\u8868\u5f81\u7a7a\u95f4\u4e2d\u7684\u7ec4\u7ec7\u65b9\u5f0f\u3002", "result": "1. \u4e3b\u8981\u8c31\u8f74\u6309\u4e9a\u7ec6\u80de\u5b9a\u4f4d\u5206\u79bb\u57fa\u56e0\uff08\u5206\u6ccc\u86cb\u767d\u4e0e\u80de\u8d28\u86cb\u767d\u4e24\u6781\uff09\uff1b2. \u4e2d\u95f4\u8f6c\u6362\u5668\u5c42\u77ac\u65f6\u7f16\u7801\u7ebf\u7c92\u4f53\u548cER\u533a\u5ba4\uff0c\u5e8f\u5217\u53cd\u6620\u7ec6\u80de\u5206\u6ccc\u9014\u5f84\uff1b3. \u6b63\u4ea4\u8f74\u7f16\u7801\u86cb\u767d\u8d28-\u86cb\u767d\u8d28\u76f8\u4e92\u4f5c\u7528\u7f51\u7edc\uff0c\u4e0e\u5b9e\u9a8c\u6d4b\u91cf\u5f3a\u5ea6\u9ad8\u5ea6\u76f8\u5173\uff1b4. \u516d\u7ef4\u8c31\u5b50\u7a7a\u95f4\u80fd\u533a\u5206\u8f6c\u5f55\u56e0\u5b50\u4e0e\u9776\u57fa\u56e0\uff1b5. \u7ec6\u80de\u7c7b\u578b\u6807\u8bb0\u57fa\u56e0\u9ad8\u4fdd\u771f\u805a\u7c7b\uff1b6. \u6b8b\u5dee\u6d41\u51e0\u4f55\u7f16\u7801\u4e0e\u6ce8\u610f\u529b\u6a21\u5f0f\u4e92\u8865\u7684\u751f\u7269\u7ed3\u6784\u3002", "conclusion": "\u751f\u7269\u5b66\u8f6c\u6362\u5668\u5b66\u4e60\u4e86\u4e00\u4e2a\u53ef\u89e3\u91ca\u7684\u7ec6\u80de\u7ec4\u7ec7\u5185\u90e8\u6a21\u578b\uff0c\u8fd9\u5bf9\u8c03\u63a7\u7f51\u7edc\u63a8\u65ad\u3001\u836f\u7269\u9776\u70b9\u4f18\u5148\u6392\u5e8f\u548c\u6a21\u578b\u5ba1\u8ba1\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u6a21\u578b\u5185\u90e8\u8868\u5f81\u5f62\u6210\u4e86\u7ed3\u6784\u5316\u7684\u751f\u7269\u5750\u6807\u7cfb\u7edf\u800c\u975e\u4e0d\u900f\u660e\u7279\u5f81\u7a7a\u95f4\u3002"}}
{"id": "2602.22235", "pdf": "https://arxiv.org/pdf/2602.22235", "abs": "https://arxiv.org/abs/2602.22235", "authors": ["Jine Xie", "Zhicheng Zhang", "Yunwei Chen", "Yanqiu Feng", "Xinyuan Zhang"], "title": "Unsupervised Denoising of Diffusion-Weighted Images with Bias and Variance Corrected Noise Modeling", "categories": ["q-bio.QM", "cs.AI", "eess.IV"], "comment": null, "summary": "Diffusion magnetic resonance imaging (dMRI) plays a vital role in both clinical diagnostics and neuroscience research. However, its inherently low signal-to-noise ratio (SNR), especially under high diffusion weighting, significantly degrades image quality and impairs downstream analysis. Recent self-supervised and unsupervised denoising methods offer a practical solution by enhancing image quality without requiring clean references. However, most of these methods do not explicitly account for the non-Gaussian noise characteristics commonly present in dMRI magnitude data during the supervised learning process, potentially leading to systematic bias and heteroscedastic variance, particularly under low-SNR conditions. To overcome this limitation, we introduce noise-corrected training objectives that explicitly model Rician statistics. Specifically, we propose two alternative loss functions: one derived from the first-order moment to remove mean bias, and another from the second-order moment to correct squared-signal bias. Both losses include adaptive weighting to account for variance heterogeneity and can be used without changing the network architecture. These objectives are instantiated in an image-specific, unsupervised Deep Image Prior (DIP) framework. Comprehensive experiments on simulated and in-vivo dMRI show that the proposed losses effectively reduce Rician bias and suppress noise fluctuations, yielding higher image quality and more reliable diffusion metrics than state-of-the-art denoising baselines. These results underscore the importance of bias- and variance-aware noise modeling for robust dMRI analysis under low-SNR conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8eRician\u7edf\u8ba1\u7684\u566a\u58f0\u6821\u6b63\u8bad\u7ec3\u76ee\u6807\uff0c\u7528\u4e8e\u6269\u6563MRI\u53bb\u566a\uff0c\u6709\u6548\u51cf\u5c11\u7cfb\u7edf\u504f\u5dee\u548c\u5f02\u65b9\u5dee\u566a\u58f0\uff0c\u63d0\u5347\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u6269\u6563\u6307\u6807\u53ef\u9760\u6027\u3002", "motivation": "\u6269\u6563MRI\u5728\u4e34\u5e8a\u548c\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u56fa\u6709\u7684\u4f4e\u4fe1\u566a\u6bd4\uff08\u5c24\u5176\u662f\u5728\u9ad8\u6269\u6563\u52a0\u6743\u4e0b\uff09\u4f1a\u663e\u8457\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u5e76\u5f71\u54cd\u4e0b\u6e38\u5206\u6790\u3002\u73b0\u6709\u7684\u81ea\u76d1\u7763\u548c\u65e0\u76d1\u7763\u53bb\u566a\u65b9\u6cd5\u5927\u591a\u672a\u660e\u786e\u8003\u8651dMRI\u5e45\u5ea6\u6570\u636e\u4e2d\u5e38\u89c1\u7684\u975e\u9ad8\u65af\u566a\u58f0\u7279\u6027\uff0c\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u504f\u5dee\u548c\u5f02\u65b9\u5dee\u65b9\u5dee\uff0c\u7279\u522b\u662f\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u566a\u58f0\u6821\u6b63\u8bad\u7ec3\u76ee\u6807\uff0c\u660e\u786e\u5efa\u6a21Rician\u7edf\u8ba1\u7279\u6027\uff1a\u4e00\u79cd\u57fa\u4e8e\u4e00\u9636\u77e9\u53bb\u9664\u5747\u503c\u504f\u5dee\uff0c\u53e6\u4e00\u79cd\u57fa\u4e8e\u4e8c\u9636\u77e9\u6821\u6b63\u5e73\u65b9\u4fe1\u53f7\u504f\u5dee\u3002\u4e24\u79cd\u635f\u5931\u51fd\u6570\u90fd\u5305\u542b\u81ea\u9002\u5e94\u6743\u91cd\u4ee5\u8003\u8651\u65b9\u5dee\u5f02\u8d28\u6027\uff0c\u53ef\u5728\u4e0d\u6539\u53d8\u7f51\u7edc\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u3002\u8fd9\u4e9b\u76ee\u6807\u5728\u56fe\u50cf\u7279\u5b9a\u7684\u65e0\u76d1\u7763\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u6846\u67b6\u4e2d\u5b9e\u73b0\u3002", "result": "\u5728\u6a21\u62df\u548c\u4f53\u5185dMRI\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u6709\u6548\u51cf\u5c11\u4e86Rician\u504f\u5dee\u5e76\u6291\u5236\u4e86\u566a\u58f0\u6ce2\u52a8\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u53bb\u566a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u83b7\u5f97\u4e86\u66f4\u9ad8\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u66f4\u53ef\u9760\u7684\u6269\u6563\u6307\u6807\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u8c03\u4e86\u504f\u5dee\u548c\u65b9\u5dee\u611f\u77e5\u7684\u566a\u58f0\u5efa\u6a21\u5bf9\u4e8e\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u7a33\u5065\u7684dMRI\u5206\u6790\u7684\u91cd\u8981\u6027\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3adMRI\u53bb\u566a\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u51c6\u786e\u7684\u7edf\u8ba1\u5efa\u6a21\u65b9\u6848\u3002"}}
{"id": "2602.22289", "pdf": "https://arxiv.org/pdf/2602.22289", "abs": "https://arxiv.org/abs/2602.22289", "authors": ["Ihor Kendiukhov"], "title": "What Topological and Geometric Structure Do Biological Foundation Models Learn? Evidence from 141 Hypotheses", "categories": ["q-bio.QM", "cs.LG", "q-bio.GN"], "comment": null, "summary": "When biological foundation models such as scGPT and Geneformer process single-cell gene expression, what geometric and topological structure forms in their internal representations? Is that structure biologically meaningful or a training artifact, and how confident should we be in such claims? We address these questions through autonomous large-scale hypothesis screening: an AI-driven executor-brainstormer loop that proposed, tested, and refined 141 geometric and topological hypotheses across 52 iterations, covering persistent homology, manifold distances, cross-model alignment, community structure, and directed topology, all with explicit null controls and disjoint gene-pool splits.\n  Three principal findings emerge. First, the models learn genuine geometric structure. Gene embedding neighborhoods exhibit non-trivial topology, with persistent homology significant in 11 of 12 transformer layers at p < 0.05 in the weakest domain and 12 of 12 in the other two. A multi-level distance hierarchy shows that manifold-aware metrics outperform Euclidean distance for identifying regulatory gene pairs, and graph community partitions track known transcription factor target relationships. Second, this structure is shared across independently trained models. CCA alignment between scGPT and Geneformer yields canonical correlation of 0.80 and gene retrieval accuracy of 72 percent, yet none of 19 tested methods reliably recover gene-level correspondences. The models agree on the global shape of gene space but not on precise gene placement. Third, the structure is more localized than it first appears. Under stringent null controls applied across all null families, robust signal concentrates in immune tissue, while lung and external lung signals weaken substantially.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0scGPT\u548cGeneformer\u7b49\u751f\u7269\u57fa\u7840\u6a21\u578b\u5728\u5904\u7406\u5355\u7ec6\u80de\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u65f6\uff0c\u5176\u5185\u90e8\u8868\u793a\u5f62\u6210\u4e86\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u51e0\u4f55\u62d3\u6251\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7ed3\u6784\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5171\u4eab\u4f46\u57fa\u56e0\u7ea7\u5bf9\u5e94\u5173\u7cfb\u4e0d\u660e\u786e\uff0c\u4e14\u4fe1\u53f7\u4e3b\u8981\u96c6\u4e2d\u5728\u514d\u75ab\u7ec4\u7ec7\u4e2d\u3002", "motivation": "\u63a2\u7a76\u751f\u7269\u57fa\u7840\u6a21\u578b\u5904\u7406\u5355\u7ec6\u80de\u57fa\u56e0\u8868\u8fbe\u6570\u636e\u65f6\uff0c\u5176\u5185\u90e8\u8868\u793a\u5f62\u6210\u7684\u51e0\u4f55\u62d3\u6251\u7ed3\u6784\u662f\u5426\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\uff0c\u8fd8\u662f\u4ec5\u4ec5\u662f\u8bad\u7ec3\u4ea7\u7269\uff0c\u4ee5\u53ca\u5bf9\u8fd9\u4e9b\u7ed3\u8bba\u7684\u7f6e\u4fe1\u5ea6\u5982\u4f55\u3002", "method": "\u91c7\u7528\u81ea\u4e3b\u5927\u89c4\u6a21\u5047\u8bbe\u7b5b\u9009\u65b9\u6cd5\uff1aAI\u9a71\u52a8\u7684\u6267\u884c\u8005-\u5934\u8111\u98ce\u66b4\u8005\u5faa\u73af\uff0c\u63d0\u51fa\u3001\u6d4b\u8bd5\u548c\u7cbe\u70bc\u4e86141\u4e2a\u51e0\u4f55\u62d3\u6251\u5047\u8bbe\uff0c\u6db5\u76d6\u6301\u4e45\u540c\u8c03\u3001\u6d41\u5f62\u8ddd\u79bb\u3001\u8de8\u6a21\u578b\u5bf9\u9f50\u3001\u793e\u533a\u7ed3\u6784\u548c\u6709\u5411\u62d3\u6251\u7b49\uff0c\u6240\u6709\u5b9e\u9a8c\u90fd\u5305\u542b\u660e\u786e\u7684\u96f6\u5047\u8bbe\u63a7\u5236\u548c\u4e0d\u76f8\u4ea4\u57fa\u56e0\u6c60\u5206\u5272\u3002", "result": "1. \u6a21\u578b\u5b66\u4e60\u5230\u4e86\u771f\u5b9e\u7684\u51e0\u4f55\u7ed3\u6784\uff1a\u57fa\u56e0\u5d4c\u5165\u90bb\u57df\u8868\u73b0\u51fa\u975e\u5e73\u51e1\u62d3\u6251\u7ed3\u6784\uff0c\u6301\u4e45\u540c\u8c03\u572812\u4e2atransformer\u5c42\u4e2d\u670911\u5c42\u5728\u5f31\u57df\u4e2d\u663e\u8457(p<0.05)\uff0c\u5728\u5176\u4ed6\u4e24\u4e2a\u57df\u4e2d12/12\u663e\u8457\uff1b\u6d41\u5f62\u611f\u77e5\u5ea6\u91cf\u5728\u8bc6\u522b\u8c03\u63a7\u57fa\u56e0\u5bf9\u65b9\u9762\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\uff1b\u56fe\u793e\u533a\u5212\u5206\u8ffd\u8e2a\u5df2\u77e5\u7684\u8f6c\u5f55\u56e0\u5b50\u9776\u6807\u5173\u7cfb\u3002\n2. \u7ed3\u6784\u5728\u4e0d\u540c\u72ec\u7acb\u8bad\u7ec3\u6a21\u578b\u95f4\u5171\u4eab\uff1ascGPT\u548cGeneformer\u7684CCA\u5bf9\u9f50\u4ea7\u751f0.80\u7684\u5178\u578b\u76f8\u5173\u6027\u548c72%\u7684\u57fa\u56e0\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u4f4619\u79cd\u6d4b\u8bd5\u65b9\u6cd5\u4e2d\u65e0\u4e00\u80fd\u53ef\u9760\u6062\u590d\u57fa\u56e0\u7ea7\u5bf9\u5e94\u5173\u7cfb\u3002\n3. \u7ed3\u6784\u6bd4\u6700\u521d\u770b\u8d77\u6765\u66f4\u5c40\u90e8\u5316\uff1a\u5728\u6240\u6709\u96f6\u5047\u8bbe\u5bb6\u65cf\u4e2d\u5e94\u7528\u4e25\u683c\u63a7\u5236\u540e\uff0c\u7a33\u5065\u4fe1\u53f7\u96c6\u4e2d\u5728\u514d\u75ab\u7ec4\u7ec7\u4e2d\uff0c\u800c\u80ba\u548c\u5916\u90e8\u80ba\u4fe1\u53f7\u663e\u8457\u51cf\u5f31\u3002", "conclusion": "\u751f\u7269\u57fa\u7840\u6a21\u578b\u786e\u5b9e\u5b66\u4e60\u5230\u4e86\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u51e0\u4f55\u62d3\u6251\u7ed3\u6784\uff0c\u8fd9\u4e9b\u7ed3\u6784\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5171\u4eab\u4f46\u57fa\u56e0\u7ea7\u5bf9\u5e94\u5173\u7cfb\u4e0d\u660e\u786e\uff0c\u4e14\u4fe1\u53f7\u5206\u5e03\u5177\u6709\u7ec4\u7ec7\u7279\u5f02\u6027\uff0c\u4e3b\u8981\u96c6\u4e2d\u5728\u514d\u75ab\u7ec4\u7ec7\u4e2d\u3002"}}
{"id": "2602.23269", "pdf": "https://arxiv.org/pdf/2602.23269", "abs": "https://arxiv.org/abs/2602.23269", "authors": ["Ashley Babjac", "Adrienne Hoarfrost"], "title": "An Active Learning Framework for Data-Efficient, Human-in-the-Loop Enzyme Function Prediction", "categories": ["q-bio.QM"], "comment": "10 pages, 4 figures, 1 table, submitted to ACM BCB 2026", "summary": "Generalizable protein function prediction is increasingly constrained by the growing mismatch between exponentially expanding sequences of environmental proteins and the comparatively slow accumulation of experimentally verified functional data. Active learning offers a promising path forward for accelerating biological function prediction, by selecting the most informative proteins to experimentally annotate for data-efficient training, yet its potential remains largely unexplored. We introduce HATTER (Human-in-the-loop Adaptive Toolkit for Transferable Enzyme Representations), a modular framework that integrates multiple active learning strategies with human-in-the-loop experimental annotation to efficiently fine tune function prediction models. We compare active learning training to standard supervised training for biological enzyme function prediction, demonstrating that active learning achieves performance comparable to standard training across diverse protein sequence evaluation datasets while requiring fewer model updates, processing less data, and substantially reducing computational cost. Interestingly, point-based uncertainty sampling methods like entropy or margin sampling perform as well or better than more complex acquisition functions such as bayesian sampling or BALD, highlighting the relative importance of sequence diversity in training datasets and model architecture design. These results demonstrate that human-in-the-loop active learning can efficiently accelerate enzyme discovery, providing a flexible platform for adaptive, scalable, and expert-guided protein function prediction.", "AI": {"tldr": "HATTER\u6846\u67b6\u901a\u8fc7\u4eba\u673a\u534f\u540c\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u548c\u6570\u636e\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e0e\u6807\u51c6\u76d1\u7763\u8bad\u7ec3\u76f8\u5f53\u7684\u9176\u529f\u80fd\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73af\u5883\u86cb\u767d\u8d28\u5e8f\u5217\u5448\u6307\u6570\u589e\u957f\uff0c\u800c\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u529f\u80fd\u6570\u636e\u79ef\u7d2f\u7f13\u6162\uff0c\u8fd9\u79cd\u4e0d\u5339\u914d\u9650\u5236\u4e86\u901a\u7528\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u7684\u53d1\u5c55\u3002\u4e3b\u52a8\u5b66\u4e60\u901a\u8fc7\u9009\u62e9\u6700\u5177\u4fe1\u606f\u91cf\u7684\u86cb\u767d\u8d28\u8fdb\u884c\u5b9e\u9a8c\u6ce8\u91ca\uff0c\u6709\u671b\u52a0\u901f\u751f\u7269\u529f\u80fd\u9884\u6d4b\uff0c\u4f46\u5176\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faHATTER\uff08\u4eba\u673a\u534f\u540c\u81ea\u9002\u5e94\u53ef\u8f6c\u79fb\u9176\u8868\u5f81\u5de5\u5177\u5305\uff09\u6846\u67b6\uff0c\u6574\u5408\u591a\u79cd\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u4e0e\u4eba\u673a\u534f\u540c\u5b9e\u9a8c\u6ce8\u91ca\uff0c\u7528\u4e8e\u9ad8\u6548\u5fae\u8c03\u529f\u80fd\u9884\u6d4b\u6a21\u578b\u3002\u6bd4\u8f83\u4e3b\u52a8\u5b66\u4e60\u8bad\u7ec3\u4e0e\u6807\u51c6\u76d1\u7763\u8bad\u7ec3\u5728\u751f\u7269\u9176\u529f\u80fd\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u4e3b\u52a8\u5b66\u4e60\u5728\u591a\u79cd\u86cb\u767d\u8d28\u5e8f\u5217\u8bc4\u4f30\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e0e\u6807\u51c6\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u6a21\u578b\u66f4\u65b0\u3001\u5904\u7406\u66f4\u5c11\u7684\u6570\u636e\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002\u57fa\u4e8e\u70b9\u7684\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u65b9\u6cd5\uff08\u5982\u71b5\u6216\u8fb9\u7f18\u91c7\u6837\uff09\u8868\u73b0\u4e0e\u66f4\u590d\u6742\u7684\u83b7\u53d6\u51fd\u6570\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u4eba\u673a\u534f\u540c\u4e3b\u52a8\u5b66\u4e60\u80fd\u6709\u6548\u52a0\u901f\u9176\u53d1\u73b0\uff0c\u4e3a\u81ea\u9002\u5e94\u3001\u53ef\u6269\u5c55\u3001\u4e13\u5bb6\u6307\u5bfc\u7684\u86cb\u767d\u8d28\u529f\u80fd\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u5e73\u53f0\u3002\u5e8f\u5217\u591a\u6837\u6027\u548c\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\u6bd4\u590d\u6742\u7684\u83b7\u53d6\u51fd\u6570\u66f4\u91cd\u8981\u3002"}}
{"id": "2602.22263", "pdf": "https://arxiv.org/pdf/2602.22263", "abs": "https://arxiv.org/abs/2602.22263", "authors": ["Fuyao Huang", "Xiaozhu Yu", "Kui Xu", "Qiangfeng Cliff Zhang"], "title": "CryoNet.Refine: A One-step Diffusion Model for Rapid Refinement of Structural Models with Cryo-EM Density Map Restraints", "categories": ["q-bio.BM", "cs.AI", "eess.IV", "q-bio.QM"], "comment": "Published as a conference paper at ICLR 2026", "summary": "High-resolution structure determination by cryo-electron microscopy (cryo-EM) requires the accurate fitting of an atomic model into an experimental density map. Traditional refinement pipelines such as Phenix.real_space_refine and Rosetta are computationally expensive, demand extensive manual tuning, and present a significant bottleneck for researchers. We present CryoNet.Refine, an end-to-end deep learning framework that automates and accelerates molecular structure refinement. Our approach utilizes a one-step diffusion model that integrates a density-aware loss function with robust stereochemical restraints, enabling rapid optimization of a structure against experimental data. CryoNet.Refine provides a unified and versatile solution capable of refining protein complexes as well as DNA/RNA-protein complexes. In benchmarks against Phenix.real_space_refine, CryoNet.Refine consistently achieves substantial improvements in both model-map correlation and overall geometric quality metrics. By offering a scalable, automated, and powerful alternative, CryoNet.Refine aims to serve as an essential tool for next-generation cryo-EM structure refinement. Web server: https://cryonet.ai/refine; Source code: https://github.com/kuixu/cryonet.refine.", "AI": {"tldr": "CryoNet.Refine\uff1a\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e00\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u52a0\u901f\u51b7\u51bb\u7535\u955c\u7ed3\u6784\u7cbe\u4fee\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b-\u5bc6\u5ea6\u56fe\u76f8\u5173\u6027\u548c\u51e0\u4f55\u8d28\u91cf", "motivation": "\u4f20\u7edf\u51b7\u51bb\u7535\u955c\u7ed3\u6784\u7cbe\u4fee\u65b9\u6cd5\uff08\u5982Phenix.real_space_refine\u548cRosetta\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9700\u8981\u5927\u91cf\u4eba\u5de5\u8c03\u6574\uff0c\u6210\u4e3a\u7814\u7a76\u74f6\u9888\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u4e00\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u5bc6\u5ea6\u611f\u77e5\u635f\u5931\u51fd\u6570\u548c\u7a33\u5065\u7684\u7acb\u4f53\u5316\u5b66\u7ea6\u675f\uff0c\u5feb\u901f\u4f18\u5316\u7ed3\u6784\u4ee5\u9002\u5e94\u5b9e\u9a8c\u6570\u636e", "result": "\u4e0ePhenix.real_space_refine\u76f8\u6bd4\uff0cCryoNet.Refine\u5728\u6a21\u578b-\u5bc6\u5ea6\u56fe\u76f8\u5173\u6027\u548c\u6574\u4f53\u51e0\u4f55\u8d28\u91cf\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u6539\u8fdb", "conclusion": "CryoNet.Refine\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u81ea\u52a8\u5316\u4e14\u5f3a\u5927\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u6709\u671b\u6210\u4e3a\u4e0b\u4e00\u4ee3\u51b7\u51bb\u7535\u955c\u7ed3\u6784\u7cbe\u4fee\u7684\u91cd\u8981\u5de5\u5177"}}
{"id": "2602.22673", "pdf": "https://arxiv.org/pdf/2602.22673", "abs": "https://arxiv.org/abs/2602.22673", "authors": ["Md Tanvir Hasan Turja"], "title": "Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support", "categories": ["cs.LG", "q-bio.QM"], "comment": "18 pages, 4 figures, code and data available at https://github.com/TanvirTurja", "summary": "Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u7ec4\u4ef6\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6297\u83cc\u7d20\u8010\u836f\u6027\u8d8b\u52bf\u5e76\u63d0\u4f9b\u57fa\u4e8e\u8bc1\u636e\u7684\u653f\u7b56\u51b3\u7b56\u652f\u6301\uff0cXGBoost\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u5b9e\u73b0\u4e86\u7ed3\u5408WHO\u653f\u7b56\u6587\u6863\u7684RAG\u7cfb\u7edf\u3002", "motivation": "\u6297\u83cc\u7d20\u8010\u836f\u6027\uff08AMR\uff09\u662f\u5168\u7403\u6027\u5371\u673a\uff0c\u9884\u8ba1\u5230205\u5e74\u6bcf\u5e74\u5bfc\u81f41000\u4e07\u4eba\u6b7b\u4ea1\u3002\u867d\u7136WHO GLASS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u76d1\u6d4b\u6570\u636e\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u5e94\u7528\u673a\u5668\u5b66\u4e60\u6765\u9884\u6d4b\u4eba\u53e3\u5c42\u9762\u7684\u8010\u836f\u8d8b\u52bf\u3002\u9700\u8981\u5f00\u53d1\u9884\u6d4b\u6a21\u578b\u548c\u653f\u7b56\u51b3\u7b56\u652f\u6301\u5de5\u5177\u6765\u5e94\u5bf9\u8fd9\u4e00\u5371\u673a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u7ec4\u4ef6\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u516d\u79cd\u6a21\u578b\uff08\u6734\u7d20\u6a21\u578b\u3001\u7ebf\u6027\u56de\u5f52\u3001\u5cad\u56de\u5f52\u3001XGBoost\u3001LightGBM\u3001LSTM\uff09\u5bf9WHO GLASS\u76845,909\u4e2a\u89c2\u6d4b\u503c\u8fdb\u884cAMR\u8d8b\u52bf\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\uff1b2\uff09\u5b9e\u73b0\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7ba1\u9053\uff0c\u7ed3\u5408ChromaDB\u5411\u91cf\u5b58\u50a8\u7684WHO\u653f\u7b56\u6587\u6863\u548c\u672c\u5730\u90e8\u7f72\u7684Phi-3 Mini\u8bed\u8a00\u6a21\u578b\u3002", "result": "XGBoost\u8868\u73b0\u6700\u4f73\uff0c\u6d4b\u8bd5MAE\u4e3a7.07%\uff0cR\u5e73\u65b9\u4e3a0.854\uff0c\u6bd4\u6734\u7d20\u57fa\u7ebf\u63d0\u9ad8\u4e8683.1%\u3002\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u663e\u793a\u524d\u4e00\u5e74\u8010\u836f\u7387\u662f\u6700\u91cd\u8981\u7684\u9884\u6d4b\u56e0\u5b50\uff0850.5%\u91cd\u8981\u6027\uff09\u3002\u533a\u57dfMAE\u4ece\u6b27\u6d32\u533a\u57df\u76844.16%\u5230\u4e1c\u5357\u4e9a\u533a\u57df\u768410.14%\u4e0d\u7b49\u3002RAG\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u6709\u6765\u6e90\u5f15\u7528\u3001\u5e7b\u89c9\u53d7\u9650\u7684\u653f\u7b56\u7b54\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u673a\u5668\u5b66\u4e60\u5728\u9884\u6d4bAMR\u8d8b\u52bf\u65b9\u9762\u7684\u6709\u6548\u6027\uff0cXGBoost\u662f\u6700\u4f73\u6a21\u578b\u3002\u7ed3\u5408RAG\u7684\u653f\u7b56\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e3a\u516c\u5171\u536b\u751f\u51b3\u7b56\u8005\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u5de5\u5177\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\uff0c\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2602.23324", "pdf": "https://arxiv.org/pdf/2602.23324", "abs": "https://arxiv.org/abs/2602.23324", "authors": ["Jose M. Betancourt", "Matthew P. Leighton", "Thierry Emonet", "Benjamin B. Machta", "Michael C. Abbott"], "title": "Discrete turn strategies emerge in information-limited navigation", "categories": ["physics.bio-ph", "cond-mat.stat-mech", "q-bio.QM"], "comment": "6 pages, 4 figures, plus appendices", "summary": "Navigation up a sensory gradient is one of the simplest behaviours, and the simplest strategy is run and tumble. But some organisms use other strategies, such as reversing direction or turning by some angle. Here we ask what drives the choice of strategy, which we frame as maximising up-gradient speed using a given amount of sensory information per unit time. We find that, without directional information on which way to turn, behavioural strategies which make sudden turns perform better than gradual steering. We see various transitions where a different strategy becomes optimal, such as a switch from reversing direction to fully re-orienting tumbles as more information becomes available. And, among more complex re-orientation strategies, we show that discrete turn angles are best, and see transitions in how many such angles the optimal strategy employs.", "AI": {"tldr": "\u7814\u7a76\u751f\u7269\u5728\u611f\u5b98\u68af\u5ea6\u4e2d\u5bfc\u822a\u7684\u6700\u4f73\u7b56\u7565\u9009\u62e9\uff0c\u53d1\u73b0\u7a81\u7136\u8f6c\u5411\u6bd4\u6e10\u8fdb\u8f6c\u5411\u66f4\u6709\u6548\uff0c\u5e76\u89c2\u5bdf\u5230\u4e0d\u540c\u4fe1\u606f\u91cf\u4e0b\u6700\u4f18\u7b56\u7565\u7684\u8f6c\u53d8\u3002", "motivation": "\u63a2\u7d22\u751f\u7269\u5728\u611f\u5b98\u68af\u5ea6\u5bfc\u822a\u4e2d\u9009\u62e9\u4e0d\u540c\u884c\u4e3a\u7b56\u7565\uff08\u5982\u53cd\u8f6c\u65b9\u5411\u3001\u7279\u5b9a\u89d2\u5ea6\u8f6c\u5411\uff09\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u65e8\u5728\u7406\u89e3\u5982\u4f55\u7528\u7ed9\u5b9a\u5355\u4f4d\u65f6\u95f4\u5185\u7684\u611f\u5b98\u4fe1\u606f\u6700\u5927\u5316\u4e0a\u68af\u5ea6\u901f\u5ea6\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u5206\u6790\u4e0d\u540c\u884c\u4e3a\u7b56\u7565\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u8fd0\u884c-\u7ffb\u6eda\u3001\u65b9\u5411\u53cd\u8f6c\u3001\u89d2\u5ea6\u8f6c\u5411\u7b49\u7b56\u7565\uff0c\u8003\u5bdf\u4fe1\u606f\u91cf\u53d8\u5316\u5bf9\u6700\u4f18\u7b56\u7565\u9009\u62e9\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u65e0\u65b9\u5411\u4fe1\u606f\u65f6\uff0c\u7a81\u7136\u8f6c\u5411\u7b56\u7565\u4f18\u4e8e\u6e10\u8fdb\u8f6c\u5411\uff1b\u89c2\u5bdf\u5230\u7b56\u7565\u8f6c\u53d8\uff1a\u4fe1\u606f\u91cf\u589e\u52a0\u65f6\u4ece\u65b9\u5411\u53cd\u8f6c\u5230\u5b8c\u5168\u91cd\u5b9a\u5411\u7ffb\u6eda\uff1b\u5728\u590d\u6742\u91cd\u5b9a\u5411\u7b56\u7565\u4e2d\uff0c\u79bb\u6563\u8f6c\u5411\u89d2\u5ea6\u6700\u4f18\uff0c\u4e14\u6700\u4f18\u7b56\u7565\u4f7f\u7528\u7684\u89d2\u5ea6\u6570\u91cf\u968f\u6761\u4ef6\u53d8\u5316\u3002", "conclusion": "\u751f\u7269\u5728\u611f\u5b98\u68af\u5ea6\u5bfc\u822a\u4e2d\u7684\u7b56\u7565\u9009\u62e9\u53d7\u53ef\u7528\u4fe1\u606f\u91cf\u5f71\u54cd\uff0c\u7a81\u7136\u8f6c\u5411\u548c\u79bb\u6563\u89d2\u5ea6\u8f6c\u5411\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u6700\u4f18\uff0c\u8fd9\u89e3\u91ca\u4e86\u81ea\u7136\u754c\u4e2d\u89c2\u5bdf\u5230\u7684\u4e0d\u540c\u884c\u4e3a\u7b56\u7565\u7684\u9002\u5e94\u6027\u4f18\u52bf\u3002"}}
