<div id=toc></div>

# Table of Contents

- [q-bio.GN](#q-bio.GN) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [1] [CrossLLM-Mamba: Multimodal State Space Fusion of LLMs for RNA Interaction Prediction](https://arxiv.org/abs/2602.22236)
*Rabeya Tus Sadia,Qiang Ye,Qiang Cheng*

Main category: q-bio.GN

TL;DR: CrossLLM-Mamba：一种基于状态空间对齐的新型RNA相互作用预测框架，通过双向Mamba编码器实现模态间深度"对话"，在多个RNA相互作用类别上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有基于BioLLM的方法采用静态融合策略，无法捕捉分子结合的动态、上下文依赖特性，需要更有效的多模态交互建模方法

Method: 将相互作用预测重新定义为状态空间对齐问题，使用双向Mamba编码器通过隐藏状态传播实现模态特定嵌入间的深度"对话"，将相互作用建模为动态序列转换而非静态特征重叠，并引入高斯噪声注入和Focal Loss增强鲁棒性

Result: 在RNA-蛋白质、RNA-小分子和RNA-RNA三类相互作用预测中均取得SOTA性能：RPI1460基准测试MCC达0.892（提升5.2%），结合亲和力预测在核糖开关和重复RNA亚型上Pearson相关系数超过0.95

Conclusion: 状态空间建模是多模态生物相互作用预测的强大范式，CrossLLM-Mamba框架通过动态序列转换建模实现了更准确的RNA相关相互作用预测

Abstract: Accurate prediction of RNA-associated interactions is essential for understanding cellular regulation and advancing drug discovery. While Biological Large Language Models (BioLLMs) such as ESM-2 and RiNALMo provide powerful sequence representations, existing methods rely on static fusion strategies that fail to capture the dynamic, context-dependent nature of molecular binding. We introduce CrossLLM-Mamba, a novel framework that reformulates interaction prediction as a state-space alignment problem. By leveraging bidirectional Mamba encoders, our approach enables deep ``crosstalk'' between modality-specific embeddings through hidden state propagation, modeling interactions as dynamic sequence transitions rather than static feature overlaps. The framework maintains linear computational complexity, making it scalable to high-dimensional BioLLM embeddings. We further incorporate Gaussian noise injection and Focal Loss to enhance robustness against hard-negative samples. Comprehensive experiments across three interaction categories, RNA-protein, RNA-small molecule, and RNA-RNA demonstrate that CrossLLM-Mamba achieves state-of-the-art performance. On the RPI1460 benchmark, our model attains an MCC of 0.892, surpassing the previous best by 5.2\%. For binding affinity prediction, we achieve Pearson correlations exceeding 0.95 on riboswitch and repeat RNA subtypes. These results establish state-space modeling as a powerful paradigm for multi-modal biological interaction prediction.

</details>


### [2] [Multi-Dimensional Spectral Geometry of Biological Knowledge in Single-Cell Transformer Representations](https://arxiv.org/abs/2602.22247)
*Ihor Kendiukhov*

Main category: q-bio.GN

TL;DR: scGPT单细胞基础模型学习基因表示，研究发现这些表示编码了可解释的生物知识：模型将基因组织成结构化的生物坐标系，而非不透明的特征空间


<details>
  <summary>Details</summary>
Motivation: 单细胞基础模型如scGPT学习高维基因表示，但这些表示编码了哪些生物知识尚不清楚。需要系统解码scGPT内部表示的几何结构，揭示模型如何组织基因信息

Method: 通过63次自动化假设筛选（测试183个假设），系统解码scGPT内部表示的几何结构。分析模型的光谱轴、中间层表示、正交轴等，评估转录因子与靶基因区分、细胞类型标记基因聚类等

Result: 模型将基因组织成结构化生物坐标系：主要光谱轴按亚细胞定位分离基因（分泌蛋白与胞质蛋白）；中间层瞬时编码线粒体和ER区室，反映细胞分泌途径；正交轴编码蛋白质相互作用网络（Spearman rho = 1.000）。在六维光谱子空间中，模型能区分转录因子与靶基因（AUROC = 0.744），细胞类型标记基因聚类效果好（AUROC = 0.851）

Conclusion: 生物transformer学习了一个可解释的细胞组织内部模型，这对调控网络推断、药物靶点优先排序和模型审计具有重要意义。残差流几何编码了与注意力模式互补的生物结构

Abstract: Single-cell foundation models such as scGPT learn high-dimensional gene representations, but what biological knowledge these representations encode remains unclear. We systematically decode the geometric structure of scGPT internal representations through 63 iterations of automated hypothesis screening (183 hypotheses tested), revealing that the model organizes genes into a structured biological coordinate system rather than an opaque feature space.
  The dominant spectral axis separates genes by subcellular localization, with secreted proteins at one pole and cytosolic proteins at the other. Intermediate transformer layers transiently encode mitochondrial and ER compartments in a sequence that mirrors the cellular secretory pathway. Orthogonal axes encode protein-protein interaction networks with graded fidelity to experimentally measured interaction strength (Spearman rho = 1.000 across n = 5 STRING confidence quintiles, p = 0.017).
  In a compact six-dimensional spectral subspace, the model distinguishes transcription factors from their target genes (AUROC = 0.744, all 12 layers significant). Early layers preserve which specific genes regulate which targets, while deeper layers compress this into a coarser regulator versus regulated distinction. Repression edges are geometrically more prominent than activation edges, and B-cell master regulators BATF and BACH2 show convergence toward the B-cell identity anchor PAX5 across transformer depth. Cell-type marker genes cluster with high fidelity (AUROC = 0.851). Residual-stream geometry encodes biological structure complementary to attention patterns. These results indicate that biological transformers learn an interpretable internal model of cellular organization, with implications for regulatory network inference, drug target prioritization, and model auditing.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [3] [Unsupervised Denoising of Diffusion-Weighted Images with Bias and Variance Corrected Noise Modeling](https://arxiv.org/abs/2602.22235)
*Jine Xie,Zhicheng Zhang,Yunwei Chen,Yanqiu Feng,Xinyuan Zhang*

Main category: q-bio.QM

TL;DR: 提出两种基于Rician噪声统计的校正训练目标，在无监督Deep Image Prior框架中有效降低dMRI图像中的Rician偏差和噪声波动，提升图像质量和扩散指标可靠性。


<details>
  <summary>Details</summary>
Motivation: dMRI在临床诊断和神经科学研究中至关重要，但其固有的低信噪比（尤其在强扩散加权下）会显著降低图像质量并影响下游分析。现有自监督和无监督去噪方法大多未明确考虑dMRI幅度数据中常见的非高斯噪声特性，可能导致系统偏差和异方差性，特别是在低SNR条件下。

Method: 提出两种噪声校正训练目标：基于一阶矩去除均值偏差的损失函数，以及基于二阶矩校正平方信号偏差的损失函数。两种损失都包含自适应加权以考虑方差异质性，可在不改变网络架构的情况下使用。这些目标在图像特定的无监督Deep Image Prior框架中实现。

Result: 在模拟和体内dMRI上的综合实验表明，所提出的损失函数能有效减少Rician偏差并抑制噪声波动，相比最先进的去噪基线方法，能获得更高的图像质量和更可靠的扩散指标。

Conclusion: 这些结果强调了偏差和方差感知的噪声建模对于低SNR条件下稳健dMRI分析的重要性。提出的方法为dMRI去噪提供了一种更准确的统计建模方案。

Abstract: Diffusion magnetic resonance imaging (dMRI) plays a vital role in both clinical diagnostics and neuroscience research. However, its inherently low signal-to-noise ratio (SNR), especially under high diffusion weighting, significantly degrades image quality and impairs downstream analysis. Recent self-supervised and unsupervised denoising methods offer a practical solution by enhancing image quality without requiring clean references. However, most of these methods do not explicitly account for the non-Gaussian noise characteristics commonly present in dMRI magnitude data during the supervised learning process, potentially leading to systematic bias and heteroscedastic variance, particularly under low-SNR conditions. To overcome this limitation, we introduce noise-corrected training objectives that explicitly model Rician statistics. Specifically, we propose two alternative loss functions: one derived from the first-order moment to remove mean bias, and another from the second-order moment to correct squared-signal bias. Both losses include adaptive weighting to account for variance heterogeneity and can be used without changing the network architecture. These objectives are instantiated in an image-specific, unsupervised Deep Image Prior (DIP) framework. Comprehensive experiments on simulated and in-vivo dMRI show that the proposed losses effectively reduce Rician bias and suppress noise fluctuations, yielding higher image quality and more reliable diffusion metrics than state-of-the-art denoising baselines. These results underscore the importance of bias- and variance-aware noise modeling for robust dMRI analysis under low-SNR conditions.

</details>


### [4] [What Topological and Geometric Structure Do Biological Foundation Models Learn? Evidence from 141 Hypotheses](https://arxiv.org/abs/2602.22289)
*Ihor Kendiukhov*

Main category: q-bio.QM

TL;DR: 单细胞基因表达基础模型（如scGPT和Geneformer）的内部表征形成具有生物学意义的几何拓扑结构，该结构在独立训练模型间共享但基因级对应关系不明确，且信号主要集中于免疫组织。


<details>
  <summary>Details</summary>
Motivation: 探究单细胞基因表达基础模型内部表征的几何拓扑结构本质：这些结构是真实的生物学特征还是训练伪影？如何评估此类声明的可信度？

Method: 采用自主大规模假设筛选框架：AI驱动的执行器-头脑风暴循环，提出、测试和精炼141个几何拓扑假设（覆盖持续同调、流形距离、跨模型对齐、社区结构和有向拓扑），使用明确的零假设控制和不相交基因池分割。

Result: 1. 模型学习到真实的几何结构：基因嵌入邻域展现非平凡拓扑，持续同调在12个transformer层中11层显著；流形感知度量优于欧氏距离识别调控基因对；图社区划分追踪已知转录因子-靶标关系。
2. 结构在独立训练模型间共享：scGPT和Geneformer的CCA对齐显示0.80典型相关和72%基因检索准确率，但19种方法均无法可靠恢复基因级对应关系。
3. 结构比表面更局部化：在严格零假设控制下，稳健信号集中于免疫组织，肺和外部肺组织信号显著减弱。

Conclusion: 单细胞基础模型确实学习到具有生物学意义的几何拓扑结构，这些结构在模型间共享但存在基因级对应不确定性，且信号分布具有组织特异性，强调需要严格的零假设控制和跨组织验证。

Abstract: When biological foundation models such as scGPT and Geneformer process single-cell gene expression, what geometric and topological structure forms in their internal representations? Is that structure biologically meaningful or a training artifact, and how confident should we be in such claims? We address these questions through autonomous large-scale hypothesis screening: an AI-driven executor-brainstormer loop that proposed, tested, and refined 141 geometric and topological hypotheses across 52 iterations, covering persistent homology, manifold distances, cross-model alignment, community structure, and directed topology, all with explicit null controls and disjoint gene-pool splits.
  Three principal findings emerge. First, the models learn genuine geometric structure. Gene embedding neighborhoods exhibit non-trivial topology, with persistent homology significant in 11 of 12 transformer layers at p < 0.05 in the weakest domain and 12 of 12 in the other two. A multi-level distance hierarchy shows that manifold-aware metrics outperform Euclidean distance for identifying regulatory gene pairs, and graph community partitions track known transcription factor target relationships. Second, this structure is shared across independently trained models. CCA alignment between scGPT and Geneformer yields canonical correlation of 0.80 and gene retrieval accuracy of 72 percent, yet none of 19 tested methods reliably recover gene-level correspondences. The models agree on the global shape of gene space but not on precise gene placement. Third, the structure is more localized than it first appears. Under stringent null controls applied across all null families, robust signal concentrates in immune tissue, while lung and external lung signals weaken substantially.

</details>


### [5] [An Active Learning Framework for Data-Efficient, Human-in-the-Loop Enzyme Function Prediction](https://arxiv.org/abs/2602.23269)
*Ashley Babjac,Adrienne Hoarfrost*

Main category: q-bio.QM

TL;DR: HATTER框架通过主动学习结合人工标注，显著减少酶功能预测的计算成本和数据需求，达到与传统监督学习相当的性能。


<details>
  <summary>Details</summary>
Motivation: 环境蛋白质序列呈指数增长，而实验验证的功能数据积累缓慢，导致蛋白质功能预测面临数据不匹配的挑战，需要更高效的数据利用方法。

Method: 开发HATTER模块化框架，整合多种主动学习策略与人工标注循环，通过选择信息量最大的蛋白质进行实验注释来高效微调功能预测模型。

Result: 主动学习在多种蛋白质序列评估数据集上达到与传统监督训练相当的性能，同时需要更少的模型更新、处理更少数据，并大幅降低计算成本。基于不确定性的简单采样方法表现优于复杂采集函数。

Conclusion: 人工循环主动学习能有效加速酶发现，为自适应、可扩展、专家指导的蛋白质功能预测提供灵活平台。

Abstract: Generalizable protein function prediction is increasingly constrained by the growing mismatch between exponentially expanding sequences of environmental proteins and the comparatively slow accumulation of experimentally verified functional data. Active learning offers a promising path forward for accelerating biological function prediction, by selecting the most informative proteins to experimentally annotate for data-efficient training, yet its potential remains largely unexplored. We introduce HATTER (Human-in-the-loop Adaptive Toolkit for Transferable Enzyme Representations), a modular framework that integrates multiple active learning strategies with human-in-the-loop experimental annotation to efficiently fine tune function prediction models. We compare active learning training to standard supervised training for biological enzyme function prediction, demonstrating that active learning achieves performance comparable to standard training across diverse protein sequence evaluation datasets while requiring fewer model updates, processing less data, and substantially reducing computational cost. Interestingly, point-based uncertainty sampling methods like entropy or margin sampling perform as well or better than more complex acquisition functions such as bayesian sampling or BALD, highlighting the relative importance of sequence diversity in training datasets and model architecture design. These results demonstrate that human-in-the-loop active learning can efficiently accelerate enzyme discovery, providing a flexible platform for adaptive, scalable, and expert-guided protein function prediction.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [6] [CryoNet.Refine: A One-step Diffusion Model for Rapid Refinement of Structural Models with Cryo-EM Density Map Restraints](https://arxiv.org/abs/2602.22263)
*Fuyao Huang,Xiaozhu Yu,Kui Xu,Qiangfeng Cliff Zhang*

Main category: q-bio.BM

TL;DR: CryoNet.Refine是一个端到端的深度学习框架，用于自动化加速冷冻电镜结构精修，通过扩散模型结合密度感知损失和立体化学约束，显著提升模型质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统冷冻电镜结构精修方法（如Phenix.real_space_refine和Rosetta）存在计算成本高、需要大量人工调整的问题，成为研究瓶颈，需要更高效自动化的解决方案。

Method: 采用端到端深度学习框架，使用一步扩散模型，结合密度感知损失函数和稳健的立体化学约束，能够快速优化结构以适应实验数据。

Result: 在基准测试中，与Phenix.real_space_refine相比，CryoNet.Refine在模型-密度图相关性和整体几何质量指标上均取得显著改进。

Conclusion: CryoNet.Refine提供了一个可扩展、自动化且功能强大的替代方案，有望成为下一代冷冻电镜结构精修的重要工具。

Abstract: High-resolution structure determination by cryo-electron microscopy (cryo-EM) requires the accurate fitting of an atomic model into an experimental density map. Traditional refinement pipelines such as Phenix.real_space_refine and Rosetta are computationally expensive, demand extensive manual tuning, and present a significant bottleneck for researchers. We present CryoNet.Refine, an end-to-end deep learning framework that automates and accelerates molecular structure refinement. Our approach utilizes a one-step diffusion model that integrates a density-aware loss function with robust stereochemical restraints, enabling rapid optimization of a structure against experimental data. CryoNet.Refine provides a unified and versatile solution capable of refining protein complexes as well as DNA/RNA-protein complexes. In benchmarks against Phenix.real_space_refine, CryoNet.Refine consistently achieves substantial improvements in both model-map correlation and overall geometric quality metrics. By offering a scalable, automated, and powerful alternative, CryoNet.Refine aims to serve as an essential tool for next-generation cryo-EM structure refinement. Web server: https://cryonet.ai/refine; Source code: https://github.com/kuixu/cryonet.refine.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Forecasting Antimicrobial Resistance Trends Using Machine Learning on WHO GLASS Surveillance Data: A Retrieval-Augmented Generation Approach for Policy Decision Support](https://arxiv.org/abs/2602.22673)
*Md Tanvir Hasan Turja*

Main category: cs.LG

TL;DR: 开发了一个两组件框架，用于预测抗生素耐药性趋势并提供基于证据的政策决策支持，XGBoost在WHO GLASS数据上表现最佳，并实现了RAG政策问答系统。


<details>
  <summary>Details</summary>
Motivation: 抗生素耐药性（AMR）是全球性危机，预计到205年每年导致1000万人死亡。虽然WHO GLASS系统提供了标准化监测数据，但很少有研究应用机器学习从这些数据中预测人群水平的耐药趋势。

Method: 提出了一个两组件框架：1）使用六种模型（Naive、线性回归、岭回归、XGBoost、LightGBM、LSTM）在5,909个WHO GLASS观测数据上进行AMR趋势预测；2）实现检索增强生成（RAG）管道，结合WHO政策文档的ChromaDB向量存储和本地部署的Phi-3 Mini语言模型，生成有来源归属、幻觉受限的政策答案。

Result: XGBoost表现最佳，测试MAE为7.07%，R平方为0.854，比朴素基线提升83.1%。特征重要性分析显示前一年耐药率是最重要的预测因子（50.5%重要性）。区域MAE范围从4.16%（欧洲区域）到10.14%（东南亚区域）。RAG系统能够生成基于证据的政策建议。

Conclusion: 该研究展示了机器学习在AMR趋势预测中的有效性，XGBoost是最佳模型，同时RAG系统为政策制定者提供了基于证据的决策支持工具，有助于应对全球抗生素耐药性危机。

Abstract: Antimicrobial resistance (AMR) is a growing global crisis projected to cause 10 million deaths per year by 2050. While the WHO Global Antimicrobial Resistance and Use Surveillance System (GLASS) provides standardized surveillance data across 44 countries, few studies have applied machine learning to forecast population-level resistance trends from this data. This paper presents a two-component framework for AMR trend forecasting and evidence-grounded policy decision support. We benchmark six models -- Naive, Linear Regression, Ridge Regression, XGBoost, LightGBM, and LSTM -- on 5,909 WHO GLASS observations across six WHO regions (2021-2023). XGBoost achieved the best performance with a test MAE of 7.07% and R-squared of 0.854, outperforming the naive baseline by 83.1%. Feature importance analysis identified the prior-year resistance rate as the dominant predictor (50.5% importance), while regional MAE ranged from 4.16% (European Region) to 10.14% (South-East Asia Region). We additionally implemented a Retrieval-Augmented Generation (RAG) pipeline combining a ChromaDB vector store of WHO policy documents with a locally deployed Phi-3 Mini language model, producing source-attributed, hallucination-constrained policy answers. Code and data are available at https://github.com/TanvirTurja

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [8] [Discrete turn strategies emerge in information-limited navigation](https://arxiv.org/abs/2602.23324)
*Jose M. Betancourt,Matthew P. Leighton,Thierry Emonet,Benjamin B. Machta,Michael C. Abbott*

Main category: physics.bio-ph

TL;DR: 研究探索了在有限感官信息下最大化上梯度速度的最佳导航策略，发现无方向信息时突然转向优于渐进转向，并观察到策略转变和离散转向角度的优势。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解生物体在梯度导航中选择不同策略（如反转方向、特定角度转向）的驱动因素，特别是如何利用有限感官信息最大化上梯度速度。

Method: 将策略选择问题框架化为在给定单位时间感官信息量下最大化上梯度速度，分析不同转向策略（反转、完全重定向、离散角度转向）的性能。

Result: 发现无方向信息时突然转向策略优于渐进转向；观察到策略转变（如信息量增加时从方向反转到完全重定向）；在复杂重定向策略中，离散转向角度最佳，且最优策略使用的离散角度数量存在转变。

Conclusion: 梯度导航策略的选择受可用感官信息量的影响，无方向信息时突然转向更有效，离散转向角度策略在复杂重定向中表现最优，信息量的变化会导致最优策略的转变。

Abstract: Navigation up a sensory gradient is one of the simplest behaviours, and the simplest strategy is run and tumble. But some organisms use other strategies, such as reversing direction or turning by some angle. Here we ask what drives the choice of strategy, which we frame as maximising up-gradient speed using a given amount of sensory information per unit time. We find that, without directional information on which way to turn, behavioural strategies which make sudden turns perform better than gradual steering. We see various transitions where a different strategy becomes optimal, such as a switch from reversing direction to fully re-orienting tumbles as more information becomes available. And, among more complex re-orientation strategies, we show that discrete turn angles are best, and see transitions in how many such angles the optimal strategy employs.

</details>
